{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9789891d-4847-4447-b9a7-139c56cf7e89",
   "metadata": {},
   "source": [
    "# importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20d93b80-785a-4c6f-afc2-7b9b27053759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import argparse\n",
    "import copy\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc962190-682f-436f-8070-f43e404d532e",
   "metadata": {},
   "source": [
    "# def"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edfd9a5-63f9-4ea2-bd55-45d9b0a9c5f8",
   "metadata": {},
   "source": [
    "## get_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c4fe2b-c336-4a7d-8e7d-18568bd2d289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(dataset, data_path):\n",
    "    if dataset == 'MNIST':\n",
    "        channel = 1\n",
    "        im_size = (28, 28)\n",
    "        num_classes = 10\n",
    "        mean = [0.1307]\n",
    "        std = [0.3081]\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
    "        dst_train = datasets.MNIST(data_path, train=True, download=True, transform=transform) # no augmentation\n",
    "        dst_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
    "        class_names = [str(c) for c in range(num_classes)]\n",
    "\n",
    "    elif dataset == 'FashionMNIST':\n",
    "        channel = 1\n",
    "        im_size = (28, 28)\n",
    "        num_classes = 10\n",
    "        mean = [0.2861]\n",
    "        std = [0.3530]\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
    "        dst_train = datasets.FashionMNIST(data_path, train=True, download=True, transform=transform) # no augmentation\n",
    "        dst_test = datasets.FashionMNIST(data_path, train=False, download=True, transform=transform)\n",
    "        class_names = dst_train.classes\n",
    "\n",
    "    elif dataset == 'SVHN':\n",
    "        channel = 3\n",
    "        im_size = (32, 32)\n",
    "        num_classes = 10\n",
    "        mean = [0.4377, 0.4438, 0.4728]\n",
    "        std = [0.1980, 0.2010, 0.1970]\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
    "        dst_train = datasets.SVHN(data_path, split='train', download=True, transform=transform)  # no augmentation\n",
    "        dst_test = datasets.SVHN(data_path, split='test', download=True, transform=transform)\n",
    "        class_names = [str(c) for c in range(num_classes)]\n",
    "\n",
    "    elif dataset == 'CIFAR10':\n",
    "        channel = 3\n",
    "        im_size = (32, 32)\n",
    "        num_classes = 10\n",
    "        mean = [0.4914, 0.4822, 0.4465]\n",
    "        std = [0.2023, 0.1994, 0.2010]\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
    "        dst_train = datasets.CIFAR10(data_path, train=True, download=True, transform=transform) # no augmentation\n",
    "        dst_test = datasets.CIFAR10(data_path, train=False, download=True, transform=transform)\n",
    "        class_names = dst_train.classes\n",
    "\n",
    "    elif dataset == 'CIFAR100':\n",
    "        channel = 3\n",
    "        im_size = (32, 32)\n",
    "        num_classes = 100\n",
    "        mean = [0.5071, 0.4866, 0.4409]\n",
    "        std = [0.2673, 0.2564, 0.2762]\n",
    "        transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
    "        dst_train = datasets.CIFAR100(data_path, train=True, download=True, transform=transform) # no augmentation\n",
    "        dst_test = datasets.CIFAR100(data_path, train=False, download=True, transform=transform)\n",
    "        class_names = dst_train.classes\n",
    "\n",
    "    elif dataset == 'TinyImageNet':\n",
    "        channel = 3\n",
    "        im_size = (64, 64)\n",
    "        num_classes = 200\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        data = torch.load(os.path.join(data_path, 'tinyimagenet.pt'), map_location='cpu')\n",
    "\n",
    "        class_names = data['classes']\n",
    "\n",
    "        images_train = data['images_train']\n",
    "        labels_train = data['labels_train']\n",
    "        images_train = images_train.detach().float() / 255.0\n",
    "        labels_train = labels_train.detach()\n",
    "        for c in range(channel):\n",
    "            images_train[:,c] = (images_train[:,c] - mean[c])/std[c]\n",
    "        dst_train = TensorDataset(images_train, labels_train)  # no augmentation\n",
    "\n",
    "        images_val = data['images_val']\n",
    "        labels_val = data['labels_val']\n",
    "        images_val = images_val.detach().float() / 255.0\n",
    "        labels_val = labels_val.detach()\n",
    "\n",
    "        for c in range(channel):\n",
    "            images_val[:, c] = (images_val[:, c] - mean[c]) / std[c]\n",
    "\n",
    "        dst_test = TensorDataset(images_val, labels_val)  # no augmentation\n",
    "\n",
    "    else:\n",
    "        exit('unknown dataset: %s'%dataset)\n",
    "\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(dst_test, batch_size=256, shuffle=False, num_workers=0)\n",
    "    return channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf1295a-fd79-4f8c-8fd7-cebe1221fa10",
   "metadata": {},
   "source": [
    "## get_loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ff32f71-8d4a-407e-8250-877ffad756f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loops(ipc):\n",
    "    # Get the two hyper-parameters of outer-loop and inner-loop.\n",
    "    # The following values are empirically good.\n",
    "    if ipc == 1:\n",
    "        outer_loop, inner_loop = 1, 1\n",
    "    elif ipc == 10:\n",
    "        outer_loop, inner_loop = 10, 50\n",
    "    elif ipc == 20:\n",
    "        outer_loop, inner_loop = 20, 25\n",
    "    elif ipc == 30:\n",
    "        outer_loop, inner_loop = 30, 20\n",
    "    elif ipc == 40:\n",
    "        outer_loop, inner_loop = 40, 15\n",
    "    elif ipc == 50:\n",
    "        outer_loop, inner_loop = 50, 10\n",
    "    else:\n",
    "        outer_loop, inner_loop = 0, 0\n",
    "        exit('loop hyper-parameters are not defined for %d ipc'%ipc)\n",
    "    return outer_loop, inner_loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebc4a8f-ee98-4807-b8a4-2d3c2463c455",
   "metadata": {},
   "source": [
    "## get_daparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0627c926-84a1-429e-bd9f-c7025de30985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_daparam(dataset, model, model_eval, ipc):\n",
    "    # We find that augmentation doesn't always benefit the performance.\n",
    "    # So we do augmentation for some of the settings.\n",
    "\n",
    "    dc_aug_param = dict()\n",
    "    dc_aug_param['crop'] = 4\n",
    "    dc_aug_param['scale'] = 0.2\n",
    "    dc_aug_param['rotate'] = 45\n",
    "    dc_aug_param['noise'] = 0.001\n",
    "    dc_aug_param['strategy'] = 'none'\n",
    "\n",
    "    if dataset == 'MNIST':\n",
    "        dc_aug_param['strategy'] = 'crop_scale_rotate'\n",
    "\n",
    "    if model_eval in ['ConvNetBN']: # Data augmentation makes model training with Batch Norm layer easier.\n",
    "        dc_aug_param['strategy'] = 'crop_noise'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efe2b66-4bf7-4e8b-8967-aef6290a80c9",
   "metadata": {},
   "source": [
    "## distance_wb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b23d2dd-7804-4691-bf01-77eb9eab8c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_wb(gwr, gws):\n",
    "    shape = gwr.shape\n",
    "    if len(shape) == 4: # conv, out*in*h*w\n",
    "        gwr = gwr.reshape(shape[0], shape[1] * shape[2] * shape[3])\n",
    "        gws = gws.reshape(shape[0], shape[1] * shape[2] * shape[3])\n",
    "    elif len(shape) == 3:  # layernorm, C*h*w\n",
    "        gwr = gwr.reshape(shape[0], shape[1] * shape[2])\n",
    "        gws = gws.reshape(shape[0], shape[1] * shape[2])\n",
    "    elif len(shape) == 2: # linear, out*in\n",
    "        tmp = 'do nothing'\n",
    "    elif len(shape) == 1: # batchnorm/instancenorm, C; groupnorm x, bias\n",
    "        gwr = gwr.reshape(1, shape[0])\n",
    "        gws = gws.reshape(1, shape[0])\n",
    "        return torch.tensor(0, dtype=torch.float, device=gwr.device)\n",
    "\n",
    "    dis_weight = torch.sum(1 - torch.sum(gwr * gws, dim=-1) / (torch.norm(gwr, dim=-1) * torch.norm(gws, dim=-1) + 0.000001))\n",
    "    dis = dis_weight\n",
    "    return dis\n",
    "\n",
    "\n",
    "\n",
    "def match_loss(gw_syn, gw_real, args):\n",
    "    dis = torch.tensor(0.0).to(args.device)\n",
    "\n",
    "    if args.dis_metric == 'ours':\n",
    "        for ig in range(len(gw_real)):\n",
    "            gwr = gw_real[ig]\n",
    "            gws = gw_syn[ig]\n",
    "            dis += distance_wb(gwr, gws)\n",
    "\n",
    "    elif args.dis_metric == 'mse':\n",
    "        gw_real_vec = []\n",
    "        gw_syn_vec = []\n",
    "        for ig in range(len(gw_real)):\n",
    "            gw_real_vec.append(gw_real[ig].reshape((-1)))\n",
    "            gw_syn_vec.append(gw_syn[ig].reshape((-1)))\n",
    "        gw_real_vec = torch.cat(gw_real_vec, dim=0)\n",
    "        gw_syn_vec = torch.cat(gw_syn_vec, dim=0)\n",
    "        dis = torch.sum((gw_syn_vec - gw_real_vec)**2)\n",
    "\n",
    "    elif args.dis_metric == 'cos':\n",
    "        gw_real_vec = []\n",
    "        gw_syn_vec = []\n",
    "        for ig in range(len(gw_real)):\n",
    "            gw_real_vec.append(gw_real[ig].reshape((-1)))\n",
    "            gw_syn_vec.append(gw_syn[ig].reshape((-1)))\n",
    "        gw_real_vec = torch.cat(gw_real_vec, dim=0)\n",
    "        gw_syn_vec = torch.cat(gw_syn_vec, dim=0)\n",
    "        dis = 1 - torch.sum(gw_real_vec * gw_syn_vec, dim=-1) / (torch.norm(gw_real_vec, dim=-1) * torch.norm(gw_syn_vec, dim=-1) + 0.000001)\n",
    "\n",
    "    else:\n",
    "        exit('unknown distance function: %s'%args.dis_metric)\n",
    "\n",
    "    return dis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a748af96-d314-40bf-8a6f-1c18fb9b1f59",
   "metadata": {},
   "source": [
    "# get_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c0c0dc9-e8f2-4854-bcd4-9e304ea7f12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time():\n",
    "    return str(time.strftime(\"[%Y-%m-%d %H:%M:%S]\", time.localtime()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3183cf4-5999-4415-a3f1-470cc7c11f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, images, labels): # images: n x c x h x w tensor\n",
    "        self.images = images.detach().float()\n",
    "        self.labels = labels.detach()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.images[index], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af15b6a4-6bb1-46ca-ae2b-e15bb1aaf00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DiffAugment(x, strategy='', seed = -1, param = None):\n",
    "    if strategy == 'None' or strategy == 'none' or strategy == '':\n",
    "        return x\n",
    "\n",
    "    if seed == -1:\n",
    "        param.Siamese = False\n",
    "    else:\n",
    "        param.Siamese = True\n",
    "\n",
    "    param.latestseed = seed\n",
    "\n",
    "    if strategy:\n",
    "        if param.aug_mode == 'M': # original\n",
    "            for p in strategy.split('_'):\n",
    "                for f in AUGMENT_FNS[p]:\n",
    "                    x = f(x, param)\n",
    "        elif param.aug_mode == 'S':\n",
    "            pbties = strategy.split('_')\n",
    "            set_seed_DiffAug(param)\n",
    "            p = pbties[torch.randint(0, len(pbties), size=(1,)).item()]\n",
    "            for f in AUGMENT_FNS[p]:\n",
    "                x = f(x, param)\n",
    "        else:\n",
    "            exit('unknown augmentation mode: %s'%param.aug_mode)\n",
    "        x = x.contiguous()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55d6e580-461e-41ef-b3c7-949ed25b6ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamDiffAug():\n",
    "    def __init__(self):\n",
    "        self.aug_mode = 'S' #'multiple or single'\n",
    "        self.prob_flip = 0.5\n",
    "        self.ratio_scale = 1.2\n",
    "        self.ratio_rotate = 15.0\n",
    "        self.ratio_crop_pad = 0.125\n",
    "        self.ratio_cutout = 0.5 # the size would be 0.5x0.5\n",
    "        self.brightness = 1.0\n",
    "        self.saturation = 2.0\n",
    "        self.contrast = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25cd8345-b683-4d3c-ae09-5483cee45b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch(mode, dataloader, net, optimizer, criterion, args, aug):\n",
    "    loss_avg, acc_avg, num_exp = 0, 0, 0\n",
    "    net = net.to(args.device)\n",
    "    criterion = criterion.to(args.device)\n",
    "\n",
    "    if mode == 'train':\n",
    "        net.train()\n",
    "    else:\n",
    "        net.eval()\n",
    "\n",
    "    for i_batch, datum in enumerate(dataloader):\n",
    "        img = datum[0].float().to(args.device)\n",
    "        if aug:\n",
    "            if args.dsa:\n",
    "                img = DiffAugment(img, args.dsa_strategy, param=args.dsa_param)\n",
    "            else:\n",
    "                img = augment(img, args.dc_aug_param, device=args.device)\n",
    "        lab = datum[1].long().to(args.device)\n",
    "        n_b = lab.shape[0]\n",
    "\n",
    "        output = net(img)\n",
    "        loss = criterion(output, lab)\n",
    "        acc = np.sum(np.equal(np.argmax(output.cpu().data.numpy(), axis=-1), lab.cpu().data.numpy()))\n",
    "\n",
    "        loss_avg += loss.item()*n_b\n",
    "        acc_avg += acc\n",
    "        num_exp += n_b\n",
    "\n",
    "        if mode == 'train':\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    loss_avg /= num_exp\n",
    "    acc_avg /= num_exp\n",
    "\n",
    "    return loss_avg, acc_avg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb01d171-fe9d-4485-904b-143499f6a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' ConvNet '''\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, channel, num_classes, net_width, net_depth, net_act, net_norm, net_pooling, im_size = (32,32)):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        self.features, shape_feat = self._make_layers(channel, net_width, net_depth, net_norm, net_act, net_pooling, im_size)\n",
    "        num_feat = shape_feat[0]*shape_feat[1]*shape_feat[2]\n",
    "        self.classifier = nn.Linear(num_feat, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "    def embed(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        return out\n",
    "\n",
    "    def _get_activation(self, net_act):\n",
    "        if net_act == 'sigmoid':\n",
    "            return nn.Sigmoid()\n",
    "        elif net_act == 'relu':\n",
    "            return nn.ReLU(inplace=True)\n",
    "        elif net_act == 'leakyrelu':\n",
    "            return nn.LeakyReLU(negative_slope=0.01)\n",
    "        elif net_act == 'swish':\n",
    "            return Swish()\n",
    "        else:\n",
    "            exit('unknown activation function: %s'%net_act)\n",
    "\n",
    "    def _get_pooling(self, net_pooling):\n",
    "        if net_pooling == 'maxpooling':\n",
    "            return nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        elif net_pooling == 'avgpooling':\n",
    "            return nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        elif net_pooling == 'none':\n",
    "            return None\n",
    "        else:\n",
    "            exit('unknown net_pooling: %s'%net_pooling)\n",
    "\n",
    "    def _get_normlayer(self, net_norm, shape_feat):\n",
    "        # shape_feat = (c*h*w)\n",
    "        if net_norm == 'batchnorm':\n",
    "            return nn.BatchNorm2d(shape_feat[0], affine=True)\n",
    "        elif net_norm == 'layernorm':\n",
    "            return nn.LayerNorm(shape_feat, elementwise_affine=True)\n",
    "        elif net_norm == 'instancenorm':\n",
    "            return nn.GroupNorm(shape_feat[0], shape_feat[0], affine=True)\n",
    "        elif net_norm == 'groupnorm':\n",
    "            return nn.GroupNorm(4, shape_feat[0], affine=True)\n",
    "        elif net_norm == 'none':\n",
    "            return None\n",
    "        else:\n",
    "            exit('unknown net_norm: %s'%net_norm)\n",
    "\n",
    "    def _make_layers(self, channel, net_width, net_depth, net_norm, net_act, net_pooling, im_size):\n",
    "        layers = []\n",
    "        in_channels = channel\n",
    "        if im_size[0] == 28:\n",
    "            im_size = (32, 32)\n",
    "        shape_feat = [in_channels, im_size[0], im_size[1]]\n",
    "        for d in range(net_depth):\n",
    "            layers += [nn.Conv2d(in_channels, net_width, kernel_size=3, padding=3 if channel == 1 and d == 0 else 1)]\n",
    "            shape_feat[0] = net_width\n",
    "            if net_norm != 'none':\n",
    "                layers += [self._get_normlayer(net_norm, shape_feat)]\n",
    "            layers += [self._get_activation(net_act)]\n",
    "            in_channels = net_width\n",
    "            if net_pooling != 'none':\n",
    "                layers += [self._get_pooling(net_pooling)]\n",
    "                shape_feat[1] //= 2\n",
    "                shape_feat[2] //= 2\n",
    "\n",
    "        return nn.Sequential(*layers), shape_feat\n",
    "\n",
    "\n",
    "\n",
    "''' LeNet '''\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, channel, num_classes):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(channel, 6, kernel_size=5, padding=2 if channel==1 else 0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc_1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc_2 = nn.Linear(120, 84)\n",
    "        self.fc_3 = nn.Linear(84, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc_1(x))\n",
    "        x = F.relu(self.fc_2(x))\n",
    "        x = self.fc_3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "''' AlexNet '''\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, channel, num_classes):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(channel, 128, kernel_size=5, stride=1, padding=4 if channel==1 else 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(192, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 192, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(192, 192, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc = nn.Linear(192 * 4 * 4, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def embed(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "''' AlexNetBN '''\n",
    "class AlexNetBN(nn.Module):\n",
    "    def __init__(self, channel, num_classes):\n",
    "        super(AlexNetBN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(channel, 128, kernel_size=5, stride=1, padding=4 if channel==1 else 2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128, 192, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(192, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 192, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(192, 192, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.fc = nn.Linear(192 * 4 * 4, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def embed(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "''' VGG '''\n",
    "cfg_vgg = {\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name, channel, num_classes, norm='instancenorm'):\n",
    "        super(VGG, self).__init__()\n",
    "        self.channel = channel\n",
    "        self.features = self._make_layers(cfg_vgg[vgg_name], norm)\n",
    "        self.classifier = nn.Linear(512 if vgg_name != 'VGGS' else 128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def embed(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "    def _make_layers(self, cfg, norm):\n",
    "        layers = []\n",
    "        in_channels = self.channel\n",
    "        for ic, x in enumerate(cfg):\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=3 if self.channel==1 and ic==0 else 1),\n",
    "                           nn.GroupNorm(x, x, affine=True) if norm=='instancenorm' else nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def VGG11(channel, num_classes):\n",
    "    return VGG('VGG11', channel, num_classes)\n",
    "def VGG11BN(channel, num_classes):\n",
    "    return VGG('VGG11', channel, num_classes, norm='batchnorm')\n",
    "def VGG13(channel, num_classes):\n",
    "    return VGG('VGG13', channel, num_classes)\n",
    "def VGG16(channel, num_classes):\n",
    "    return VGG('VGG16', channel, num_classes)\n",
    "def VGG19(channel, num_classes):\n",
    "    return VGG('VGG19', channel, num_classes)\n",
    "\n",
    "''' MLP '''\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, channel, num_classes):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc_1 = nn.Linear(28*28*1 if channel==1 else 32*32*3, 128)\n",
    "        self.fc_2 = nn.Linear(128, 128)\n",
    "        self.fc_3 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.view(x.size(0), -1)\n",
    "        out = F.relu(self.fc_1(out))\n",
    "        out = F.relu(self.fc_2(out))\n",
    "        out = self.fc_3(out)\n",
    "        return out\n",
    "\n",
    "def ResNet18(channel, num_classes):\n",
    "    return ResNet(BasicBlock, [2,2,2,2], channel=channel, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44f9b64a-97ec-4206-8bc0-a903da921c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_convnet_setting():\n",
    "    net_width, net_depth, net_act, net_norm, net_pooling = 128, 3, 'relu', 'instancenorm', 'avgpooling'\n",
    "    return net_width, net_depth, net_act, net_norm, net_pooling\n",
    "def get_network(model, channel, num_classes, im_size=(32, 32)):\n",
    "    torch.random.manual_seed(int(time.time() * 1000) % 100000)\n",
    "    net_width, net_depth, net_act, net_norm, net_pooling = get_default_convnet_setting()\n",
    "\n",
    "    if model == 'MLP':\n",
    "        net = MLP(channel=channel, num_classes=num_classes)\n",
    "    elif model == 'ConvNet':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'LeNet':\n",
    "        net = LeNet(channel=channel, num_classes=num_classes)\n",
    "    elif model == 'AlexNet':\n",
    "        net = AlexNet(channel=channel, num_classes=num_classes)\n",
    "    elif model == 'AlexNetBN':\n",
    "        net = AlexNetBN(channel=channel, num_classes=num_classes)\n",
    "    elif model == 'VGG11':\n",
    "        net = VGG11( channel=channel, num_classes=num_classes)\n",
    "   \n",
    "    elif model == 'ConvNetD1':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=1, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetD2':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=2, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetD3':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=3, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetD4':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=4, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "\n",
    "    elif model == 'ConvNetW32':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=32, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetW64':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=64, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetW128':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=128, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetW256':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=256, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "\n",
    "    elif model == 'ConvNetAS':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='sigmoid', net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetAR':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='relu', net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetAL':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='leakyrelu', net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetASwish':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='swish', net_norm=net_norm, net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetASwishBN':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act='swish', net_norm='batchnorm', net_pooling=net_pooling, im_size=im_size)\n",
    "\n",
    "    elif model == 'ConvNetNN':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='none', net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetBN':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='batchnorm', net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetLN':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='layernorm', net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetIN':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='instancenorm', net_pooling=net_pooling, im_size=im_size)\n",
    "    elif model == 'ConvNetGN':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm='groupnorm', net_pooling=net_pooling, im_size=im_size)\n",
    "\n",
    "    elif model == 'ConvNetNP':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling='none', im_size=im_size)\n",
    "    elif model == 'ConvNetMP':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling='maxpooling', im_size=im_size)\n",
    "    elif model == 'ConvNetAP':\n",
    "        net = ConvNet(channel=channel, num_classes=num_classes, net_width=net_width, net_depth=net_depth, net_act=net_act, net_norm=net_norm, net_pooling='avgpooling', im_size=im_size)\n",
    "\n",
    "    else:\n",
    "        net = None\n",
    "        exit('unknown model: %s'%model)\n",
    "\n",
    "    gpu_num = torch.cuda.device_count()\n",
    "    if gpu_num>0:\n",
    "        device = 'cuda'\n",
    "        if gpu_num>1:\n",
    "            net = nn.DataParallel(net)\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    net = net.to(device)\n",
    "\n",
    "    return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "989f7c9c-e323-4bd6-a433-f0666b71c059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_eval_pool(eval_mode, model, model_eval):\n",
    "    if eval_mode == 'M': # multiple architectures\n",
    "        model_eval_pool = ['MLP', 'ConvNet', 'LeNet', 'AlexNet', 'VGG11', 'ResNet18']\n",
    "    elif eval_mode == 'B':  # multiple architectures with BatchNorm for DM experiments\n",
    "        model_eval_pool = ['ConvNetBN', 'ConvNetASwishBN', 'AlexNetBN', 'VGG11BN', 'ResNet18BN']\n",
    "    elif eval_mode == 'W': # ablation study on network width\n",
    "        model_eval_pool = ['ConvNetW32', 'ConvNetW64', 'ConvNetW128', 'ConvNetW256']\n",
    "    elif eval_mode == 'D': # ablation study on network depth\n",
    "        model_eval_pool = ['ConvNetD1', 'ConvNetD2', 'ConvNetD3', 'ConvNetD4']\n",
    "    elif eval_mode == 'A': # ablation study on network activation function\n",
    "        model_eval_pool = ['ConvNetAS', 'ConvNetAR', 'ConvNetAL', 'ConvNetASwish']\n",
    "    elif eval_mode == 'P': # ablation study on network pooling layer\n",
    "        model_eval_pool = ['ConvNetNP', 'ConvNetMP', 'ConvNetAP']\n",
    "    elif eval_mode == 'N': # ablation study on network normalization layer\n",
    "        model_eval_pool = ['ConvNetNN', 'ConvNetBN', 'ConvNetLN', 'ConvNetIN', 'ConvNetGN']\n",
    "    elif eval_mode == 'S': # itself\n",
    "        if 'BN' in model:\n",
    "            print('Attention: Here I will replace BN with IN in evaluation, as the synthetic set is too small to measure BN hyper-parameters.')\n",
    "        model_eval_pool = [model[:model.index('BN')]] if 'BN' in model else [model]\n",
    "    elif eval_mode == 'SS':  # itself\n",
    "        model_eval_pool = [model]\n",
    "    else:\n",
    "        model_eval_pool = [model_eval]\n",
    "    return model_eval_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "773b03e5-9121-4e49-a4a8-9c176ca7838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_synset(it_eval, net, images_train, labels_train, testloader, args):\n",
    "    net = net.to(args.device)\n",
    "    images_train = images_train.to(args.device)\n",
    "    labels_train = labels_train.to(args.device)\n",
    "    lr = float(args.lr_net)\n",
    "    Epoch = int(args.epoch_eval_train)\n",
    "    lr_schedule = [Epoch//2+1]\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n",
    "    criterion = nn.CrossEntropyLoss().to(args.device)\n",
    "\n",
    "    dst_train = TensorDataset(images_train, labels_train)\n",
    "    trainloader = torch.utils.data.DataLoader(dst_train, batch_size=args.batch_train, shuffle=True, num_workers=0)\n",
    "\n",
    "    start = time.time()\n",
    "    for ep in range(Epoch+1):\n",
    "        loss_train, acc_train = epoch('train', trainloader, net, optimizer, criterion, args, aug = True)\n",
    "        if ep in lr_schedule:\n",
    "            lr *= 0.1\n",
    "            optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    time_train = time.time() - start\n",
    "    loss_test, acc_test = epoch('test', testloader, net, optimizer, criterion, args, aug = False)\n",
    "    print('%s Evaluate_%02d: epoch = %04d train time = %d s train loss = %.6f train acc = %.4f, test acc = %.4f' % (get_time(), it_eval, Epoch, int(time_train), loss_train, acc_train, acc_test))\n",
    "\n",
    "    return net, acc_train, acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dac351b9-f8d9-40df-8cf8-6069d7d97cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamDiffAug():\n",
    "    def __init__(self):\n",
    "        self.aug_mode = 'S' #'multiple or single'\n",
    "        self.prob_flip = 0.5\n",
    "        self.ratio_scale = 1.2\n",
    "        self.ratio_rotate = 15.0\n",
    "        self.ratio_crop_pad = 0.125\n",
    "        self.ratio_cutout = 0.5 # the size would be 0.5x0.5\n",
    "        self.brightness = 1.0\n",
    "        self.saturation = 2.0\n",
    "        self.contrast = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef92ef84-98fa-408d-a77a-15a01cffb10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, images, labels): # images: n x c x h x w tensor\n",
    "        self.images = images.detach().float()\n",
    "        self.labels = labels.detach()\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.images[index], self.labels[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.images.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b0d490-af30-41a1-ab52-6771be5b93de",
   "metadata": {},
   "source": [
    "# distillation avec la methode DSA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "132863d1-2c56-4109-bb7c-b8871abc4199",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unrecognized arguments: ['-f', 'C:\\\\Users\\\\ahmed\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-fbd20088-27d9-4d3a-a8ba-06626b58e328.json']\n",
      "eval_it_pool:  [0, 500, 1000]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "================== Exp 0 ==================\n",
      " \n",
      "Hyper-parameters: \n",
      " {'method': 'DSA', 'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 1, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 300, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 1, 'inner_loop': 1, 'device': 'cuda', 'dsa_param': <__main__.ParamDiffAug object at 0x000001DC10AC6C10>, 'dsa': True}\n",
      "Evaluation model pool:  ['ConvNet']\n",
      "class c = 0: 5000 real images\n",
      "class c = 1: 5000 real images\n",
      "class c = 2: 5000 real images\n",
      "class c = 3: 5000 real images\n",
      "class c = 4: 5000 real images\n",
      "class c = 5: 5000 real images\n",
      "class c = 6: 5000 real images\n",
      "class c = 7: 5000 real images\n",
      "class c = 8: 5000 real images\n",
      "class c = 9: 5000 real images\n",
      "real images channel 0, mean = -0.0000, std = 1.2211\n",
      "real images channel 1, mean = -0.0002, std = 1.2211\n",
      "real images channel 2, mean = 0.0002, std = 1.3014\n",
      "initialize synthetic data from random noise\n",
      "[2024-08-08 10:15:56] training begins\n",
      "-------------------------\n",
      "Evaluation\n",
      "model_train = ConvNet, model_eval = ConvNet, iteration = 0\n",
      "DSA augmentation strategy: \n",
      " None\n",
      "DSA augmentation parameters: \n",
      " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\AppData\\Local\\Temp\\ipykernel_28112\\2115322540.py:85: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-08 10:16:02] Evaluate_00: epoch = 1000 train time = 3 s train loss = 0.000236 train acc = 1.0000, test acc = 0.1265\n",
      "[2024-08-08 10:16:07] Evaluate_01: epoch = 1000 train time = 3 s train loss = 0.000236 train acc = 1.0000, test acc = 0.1312\n",
      "[2024-08-08 10:16:12] Evaluate_02: epoch = 1000 train time = 3 s train loss = 0.000229 train acc = 1.0000, test acc = 0.1177\n",
      "[2024-08-08 10:16:18] Evaluate_03: epoch = 1000 train time = 3 s train loss = 0.000227 train acc = 1.0000, test acc = 0.1163\n",
      "[2024-08-08 10:16:23] Evaluate_04: epoch = 1000 train time = 3 s train loss = 0.000230 train acc = 1.0000, test acc = 0.1338\n",
      "[2024-08-08 10:16:28] Evaluate_05: epoch = 1000 train time = 2 s train loss = 0.000234 train acc = 1.0000, test acc = 0.1229\n",
      "[2024-08-08 10:16:33] Evaluate_06: epoch = 1000 train time = 3 s train loss = 0.000231 train acc = 1.0000, test acc = 0.1133\n",
      "[2024-08-08 10:16:39] Evaluate_07: epoch = 1000 train time = 3 s train loss = 0.000239 train acc = 1.0000, test acc = 0.1140\n",
      "[2024-08-08 10:16:44] Evaluate_08: epoch = 1000 train time = 3 s train loss = 0.000225 train acc = 1.0000, test acc = 0.1270\n",
      "[2024-08-08 10:16:49] Evaluate_09: epoch = 1000 train time = 3 s train loss = 0.000225 train acc = 1.0000, test acc = 0.1235\n",
      "[2024-08-08 10:16:54] Evaluate_10: epoch = 1000 train time = 3 s train loss = 0.000235 train acc = 1.0000, test acc = 0.1238\n",
      "[2024-08-08 10:16:59] Evaluate_11: epoch = 1000 train time = 3 s train loss = 0.000225 train acc = 1.0000, test acc = 0.1362\n",
      "[2024-08-08 10:17:04] Evaluate_12: epoch = 1000 train time = 3 s train loss = 0.000234 train acc = 1.0000, test acc = 0.1204\n",
      "[2024-08-08 10:17:10] Evaluate_13: epoch = 1000 train time = 3 s train loss = 0.000237 train acc = 1.0000, test acc = 0.1335\n",
      "[2024-08-08 10:17:15] Evaluate_14: epoch = 1000 train time = 3 s train loss = 0.000234 train acc = 1.0000, test acc = 0.1286\n",
      "[2024-08-08 10:17:20] Evaluate_15: epoch = 1000 train time = 3 s train loss = 0.000225 train acc = 1.0000, test acc = 0.1251\n",
      "[2024-08-08 10:17:25] Evaluate_16: epoch = 1000 train time = 3 s train loss = 0.000235 train acc = 1.0000, test acc = 0.1234\n",
      "[2024-08-08 10:17:31] Evaluate_17: epoch = 1000 train time = 3 s train loss = 0.000235 train acc = 1.0000, test acc = 0.1417\n",
      "[2024-08-08 10:17:36] Evaluate_18: epoch = 1000 train time = 3 s train loss = 0.000235 train acc = 1.0000, test acc = 0.1291\n",
      "[2024-08-08 10:17:42] Evaluate_19: epoch = 1000 train time = 3 s train loss = 0.000231 train acc = 1.0000, test acc = 0.1233\n",
      "Evaluate 20 random ConvNet, mean = 0.1256 std = 0.0072\n",
      "-------------------------\n",
      "[2024-08-08 10:17:42] iter = 0000, loss = 315.3588\n",
      "[2024-08-08 10:17:48] iter = 0010, loss = 249.3091\n",
      "[2024-08-08 10:17:53] iter = 0020, loss = 232.6897\n",
      "[2024-08-08 10:17:59] iter = 0030, loss = 225.6343\n",
      "[2024-08-08 10:18:04] iter = 0040, loss = 214.5161\n",
      "[2024-08-08 10:18:09] iter = 0050, loss = 214.7438\n",
      "[2024-08-08 10:18:15] iter = 0060, loss = 218.3138\n",
      "[2024-08-08 10:18:20] iter = 0070, loss = 207.7157\n",
      "[2024-08-08 10:18:26] iter = 0080, loss = 211.2250\n",
      "[2024-08-08 10:18:31] iter = 0090, loss = 207.5499\n",
      "[2024-08-08 10:18:36] iter = 0100, loss = 203.8657\n",
      "[2024-08-08 10:18:41] iter = 0110, loss = 205.8285\n",
      "[2024-08-08 10:18:47] iter = 0120, loss = 199.9277\n",
      "[2024-08-08 10:18:52] iter = 0130, loss = 204.6949\n",
      "[2024-08-08 10:18:58] iter = 0140, loss = 200.5481\n",
      "[2024-08-08 10:19:03] iter = 0150, loss = 198.7567\n",
      "[2024-08-08 10:19:09] iter = 0160, loss = 199.9981\n",
      "[2024-08-08 10:19:14] iter = 0170, loss = 193.3430\n",
      "[2024-08-08 10:19:20] iter = 0180, loss = 199.4716\n",
      "[2024-08-08 10:19:25] iter = 0190, loss = 194.5930\n",
      "[2024-08-08 10:19:30] iter = 0200, loss = 196.7342\n",
      "[2024-08-08 10:19:36] iter = 0210, loss = 197.8928\n",
      "[2024-08-08 10:19:41] iter = 0220, loss = 194.9409\n",
      "[2024-08-08 10:19:47] iter = 0230, loss = 195.5327\n",
      "[2024-08-08 10:19:52] iter = 0240, loss = 194.7865\n",
      "[2024-08-08 10:19:58] iter = 0250, loss = 190.2709\n",
      "[2024-08-08 10:20:03] iter = 0260, loss = 190.8117\n",
      "[2024-08-08 10:20:09] iter = 0270, loss = 193.6703\n",
      "[2024-08-08 10:20:14] iter = 0280, loss = 191.6310\n",
      "[2024-08-08 10:20:20] iter = 0290, loss = 194.6375\n",
      "[2024-08-08 10:20:25] iter = 0300, loss = 191.6592\n",
      "[2024-08-08 10:20:31] iter = 0310, loss = 193.2092\n",
      "[2024-08-08 10:20:36] iter = 0320, loss = 192.5884\n",
      "[2024-08-08 10:20:42] iter = 0330, loss = 189.5369\n",
      "[2024-08-08 10:20:48] iter = 0340, loss = 191.4218\n",
      "[2024-08-08 10:20:53] iter = 0350, loss = 193.7018\n",
      "[2024-08-08 10:20:59] iter = 0360, loss = 186.8659\n",
      "[2024-08-08 10:21:04] iter = 0370, loss = 191.6125\n",
      "[2024-08-08 10:21:10] iter = 0380, loss = 188.1973\n",
      "[2024-08-08 10:21:15] iter = 0390, loss = 193.4835\n",
      "[2024-08-08 10:21:21] iter = 0400, loss = 193.4287\n",
      "[2024-08-08 10:21:26] iter = 0410, loss = 189.5452\n",
      "[2024-08-08 10:21:32] iter = 0420, loss = 189.3415\n",
      "[2024-08-08 10:21:37] iter = 0430, loss = 185.6868\n",
      "[2024-08-08 10:21:43] iter = 0440, loss = 183.7274\n",
      "[2024-08-08 10:21:48] iter = 0450, loss = 186.6779\n",
      "[2024-08-08 10:21:54] iter = 0460, loss = 185.4951\n",
      "[2024-08-08 10:21:59] iter = 0470, loss = 188.0251\n",
      "[2024-08-08 10:22:05] iter = 0480, loss = 181.9918\n",
      "[2024-08-08 10:22:10] iter = 0490, loss = 186.7289\n",
      "-------------------------\n",
      "Evaluation\n",
      "model_train = ConvNet, model_eval = ConvNet, iteration = 500\n",
      "DSA augmentation strategy: \n",
      " None\n",
      "DSA augmentation parameters: \n",
      " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n",
      "[2024-08-08 10:22:21] Evaluate_00: epoch = 1000 train time = 3 s train loss = 0.000285 train acc = 1.0000, test acc = 0.2910\n",
      "[2024-08-08 10:22:26] Evaluate_01: epoch = 1000 train time = 3 s train loss = 0.000282 train acc = 1.0000, test acc = 0.2789\n",
      "[2024-08-08 10:22:32] Evaluate_02: epoch = 1000 train time = 3 s train loss = 0.000289 train acc = 1.0000, test acc = 0.2905\n",
      "[2024-08-08 10:22:37] Evaluate_03: epoch = 1000 train time = 3 s train loss = 0.000283 train acc = 1.0000, test acc = 0.2774\n",
      "[2024-08-08 10:22:42] Evaluate_04: epoch = 1000 train time = 3 s train loss = 0.000286 train acc = 1.0000, test acc = 0.2767\n",
      "[2024-08-08 10:22:47] Evaluate_05: epoch = 1000 train time = 3 s train loss = 0.000296 train acc = 1.0000, test acc = 0.2797\n",
      "[2024-08-08 10:22:53] Evaluate_06: epoch = 1000 train time = 3 s train loss = 0.000292 train acc = 1.0000, test acc = 0.2788\n",
      "[2024-08-08 10:22:58] Evaluate_07: epoch = 1000 train time = 3 s train loss = 0.000289 train acc = 1.0000, test acc = 0.2862\n",
      "[2024-08-08 10:23:03] Evaluate_08: epoch = 1000 train time = 3 s train loss = 0.000282 train acc = 1.0000, test acc = 0.2793\n",
      "[2024-08-08 10:23:08] Evaluate_09: epoch = 1000 train time = 3 s train loss = 0.000285 train acc = 1.0000, test acc = 0.2896\n",
      "[2024-08-08 10:23:13] Evaluate_10: epoch = 1000 train time = 3 s train loss = 0.000295 train acc = 1.0000, test acc = 0.2790\n",
      "[2024-08-08 10:23:18] Evaluate_11: epoch = 1000 train time = 3 s train loss = 0.000283 train acc = 1.0000, test acc = 0.3013\n",
      "[2024-08-08 10:23:24] Evaluate_12: epoch = 1000 train time = 3 s train loss = 0.000278 train acc = 1.0000, test acc = 0.2936\n",
      "[2024-08-08 10:23:29] Evaluate_13: epoch = 1000 train time = 3 s train loss = 0.000279 train acc = 1.0000, test acc = 0.2970\n",
      "[2024-08-08 10:23:34] Evaluate_14: epoch = 1000 train time = 3 s train loss = 0.000290 train acc = 1.0000, test acc = 0.2784\n",
      "[2024-08-08 10:23:39] Evaluate_15: epoch = 1000 train time = 3 s train loss = 0.000290 train acc = 1.0000, test acc = 0.2730\n",
      "[2024-08-08 10:23:44] Evaluate_16: epoch = 1000 train time = 3 s train loss = 0.000289 train acc = 1.0000, test acc = 0.2759\n",
      "[2024-08-08 10:23:49] Evaluate_17: epoch = 1000 train time = 3 s train loss = 0.000278 train acc = 1.0000, test acc = 0.2833\n",
      "[2024-08-08 10:23:55] Evaluate_18: epoch = 1000 train time = 3 s train loss = 0.000287 train acc = 1.0000, test acc = 0.2841\n",
      "[2024-08-08 10:24:00] Evaluate_19: epoch = 1000 train time = 3 s train loss = 0.000286 train acc = 1.0000, test acc = 0.2928\n",
      "Evaluate 20 random ConvNet, mean = 0.2843 std = 0.0077\n",
      "-------------------------\n",
      "[2024-08-08 10:24:00] iter = 0500, loss = 184.3280\n",
      "[2024-08-08 10:24:06] iter = 0510, loss = 189.4590\n",
      "[2024-08-08 10:24:12] iter = 0520, loss = 185.5951\n",
      "[2024-08-08 10:24:17] iter = 0530, loss = 184.2067\n",
      "[2024-08-08 10:24:23] iter = 0540, loss = 185.8775\n",
      "[2024-08-08 10:24:28] iter = 0550, loss = 188.7177\n",
      "[2024-08-08 10:24:34] iter = 0560, loss = 185.3447\n",
      "[2024-08-08 10:24:39] iter = 0570, loss = 190.2599\n",
      "[2024-08-08 10:24:45] iter = 0580, loss = 186.2844\n",
      "[2024-08-08 10:24:50] iter = 0590, loss = 180.5482\n",
      "[2024-08-08 10:24:55] iter = 0600, loss = 181.3228\n",
      "[2024-08-08 10:25:01] iter = 0610, loss = 184.1386\n",
      "[2024-08-08 10:25:07] iter = 0620, loss = 183.9304\n",
      "[2024-08-08 10:25:12] iter = 0630, loss = 187.3899\n",
      "[2024-08-08 10:25:18] iter = 0640, loss = 175.1952\n",
      "[2024-08-08 10:25:23] iter = 0650, loss = 184.8862\n",
      "[2024-08-08 10:25:28] iter = 0660, loss = 188.2151\n",
      "[2024-08-08 10:25:34] iter = 0670, loss = 178.8806\n",
      "[2024-08-08 10:25:39] iter = 0680, loss = 182.8975\n",
      "[2024-08-08 10:25:45] iter = 0690, loss = 186.0144\n",
      "[2024-08-08 10:25:50] iter = 0700, loss = 177.6556\n",
      "[2024-08-08 10:25:55] iter = 0710, loss = 184.9636\n",
      "[2024-08-08 10:26:01] iter = 0720, loss = 183.4544\n",
      "[2024-08-08 10:26:06] iter = 0730, loss = 180.4723\n",
      "[2024-08-08 10:26:12] iter = 0740, loss = 181.8303\n",
      "[2024-08-08 10:26:17] iter = 0750, loss = 179.7436\n",
      "[2024-08-08 10:26:22] iter = 0760, loss = 181.9355\n",
      "[2024-08-08 10:26:28] iter = 0770, loss = 181.4547\n",
      "[2024-08-08 10:26:33] iter = 0780, loss = 187.5349\n",
      "[2024-08-08 10:26:38] iter = 0790, loss = 179.7307\n",
      "[2024-08-08 10:26:44] iter = 0800, loss = 180.7874\n",
      "[2024-08-08 10:26:49] iter = 0810, loss = 181.6386\n",
      "[2024-08-08 10:26:55] iter = 0820, loss = 187.2083\n",
      "[2024-08-08 10:27:00] iter = 0830, loss = 189.0740\n",
      "[2024-08-08 10:27:05] iter = 0840, loss = 181.8886\n",
      "[2024-08-08 10:27:11] iter = 0850, loss = 182.6709\n",
      "[2024-08-08 10:27:16] iter = 0860, loss = 178.3753\n",
      "[2024-08-08 10:27:22] iter = 0870, loss = 186.8177\n",
      "[2024-08-08 10:27:27] iter = 0880, loss = 178.9450\n",
      "[2024-08-08 10:27:33] iter = 0890, loss = 184.1984\n",
      "[2024-08-08 10:27:38] iter = 0900, loss = 185.9947\n",
      "[2024-08-08 10:27:44] iter = 0910, loss = 189.2386\n",
      "[2024-08-08 10:27:49] iter = 0920, loss = 184.8413\n",
      "[2024-08-08 10:27:55] iter = 0930, loss = 183.9553\n",
      "[2024-08-08 10:28:00] iter = 0940, loss = 180.8492\n",
      "[2024-08-08 10:28:06] iter = 0950, loss = 182.3497\n",
      "[2024-08-08 10:28:11] iter = 0960, loss = 182.6261\n",
      "[2024-08-08 10:28:16] iter = 0970, loss = 185.8807\n",
      "[2024-08-08 10:28:22] iter = 0980, loss = 180.0623\n",
      "[2024-08-08 10:28:27] iter = 0990, loss = 179.5113\n",
      "-------------------------\n",
      "Evaluation\n",
      "model_train = ConvNet, model_eval = ConvNet, iteration = 1000\n",
      "DSA augmentation strategy: \n",
      " None\n",
      "DSA augmentation parameters: \n",
      " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n",
      "[2024-08-08 10:28:37] Evaluate_00: epoch = 1000 train time = 3 s train loss = 0.000300 train acc = 1.0000, test acc = 0.2936\n",
      "[2024-08-08 10:28:43] Evaluate_01: epoch = 1000 train time = 3 s train loss = 0.000294 train acc = 1.0000, test acc = 0.2829\n",
      "[2024-08-08 10:28:48] Evaluate_02: epoch = 1000 train time = 3 s train loss = 0.000283 train acc = 1.0000, test acc = 0.2820\n",
      "[2024-08-08 10:28:53] Evaluate_03: epoch = 1000 train time = 3 s train loss = 0.000291 train acc = 1.0000, test acc = 0.2865\n",
      "[2024-08-08 10:28:58] Evaluate_04: epoch = 1000 train time = 3 s train loss = 0.000297 train acc = 1.0000, test acc = 0.2831\n",
      "[2024-08-08 10:29:03] Evaluate_05: epoch = 1000 train time = 3 s train loss = 0.000297 train acc = 1.0000, test acc = 0.2863\n",
      "[2024-08-08 10:29:08] Evaluate_06: epoch = 1000 train time = 3 s train loss = 0.000288 train acc = 1.0000, test acc = 0.2767\n",
      "[2024-08-08 10:29:14] Evaluate_07: epoch = 1000 train time = 3 s train loss = 0.000300 train acc = 1.0000, test acc = 0.2806\n",
      "[2024-08-08 10:29:19] Evaluate_08: epoch = 1000 train time = 3 s train loss = 0.000289 train acc = 1.0000, test acc = 0.2872\n",
      "[2024-08-08 10:29:24] Evaluate_09: epoch = 1000 train time = 3 s train loss = 0.000286 train acc = 1.0000, test acc = 0.2777\n",
      "[2024-08-08 10:29:29] Evaluate_10: epoch = 1000 train time = 3 s train loss = 0.000268 train acc = 1.0000, test acc = 0.2814\n",
      "[2024-08-08 10:29:34] Evaluate_11: epoch = 1000 train time = 3 s train loss = 0.000282 train acc = 1.0000, test acc = 0.2836\n",
      "[2024-08-08 10:29:40] Evaluate_12: epoch = 1000 train time = 3 s train loss = 0.000295 train acc = 1.0000, test acc = 0.2740\n",
      "[2024-08-08 10:29:45] Evaluate_13: epoch = 1000 train time = 3 s train loss = 0.000299 train acc = 1.0000, test acc = 0.2747\n",
      "[2024-08-08 10:29:50] Evaluate_14: epoch = 1000 train time = 3 s train loss = 0.000290 train acc = 1.0000, test acc = 0.2757\n",
      "[2024-08-08 10:29:55] Evaluate_15: epoch = 1000 train time = 3 s train loss = 0.000296 train acc = 1.0000, test acc = 0.2840\n",
      "[2024-08-08 10:30:01] Evaluate_16: epoch = 1000 train time = 3 s train loss = 0.000294 train acc = 1.0000, test acc = 0.2801\n",
      "[2024-08-08 10:30:06] Evaluate_17: epoch = 1000 train time = 3 s train loss = 0.000297 train acc = 1.0000, test acc = 0.2861\n",
      "[2024-08-08 10:30:11] Evaluate_18: epoch = 1000 train time = 3 s train loss = 0.000293 train acc = 1.0000, test acc = 0.2863\n",
      "[2024-08-08 10:30:16] Evaluate_19: epoch = 1000 train time = 3 s train loss = 0.000305 train acc = 1.0000, test acc = 0.2792\n",
      "Evaluate 20 random ConvNet, mean = 0.2821 std = 0.0048\n",
      "-------------------------\n",
      "[2024-08-08 10:30:17] iter = 1000, loss = 182.0896\n",
      "\n",
      "================== Exp 1 ==================\n",
      " \n",
      "Hyper-parameters: \n",
      " {'method': 'DSA', 'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 1, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 1000, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 1, 'inner_loop': 1, 'device': 'cuda', 'dsa_param': <__main__.ParamDiffAug object at 0x000001DC10AC6C10>, 'dsa': True, 'dc_aug_param': None}\n",
      "Evaluation model pool:  ['ConvNet']\n",
      "class c = 0: 5000 real images\n",
      "class c = 1: 5000 real images\n",
      "class c = 2: 5000 real images\n",
      "class c = 3: 5000 real images\n",
      "class c = 4: 5000 real images\n",
      "class c = 5: 5000 real images\n",
      "class c = 6: 5000 real images\n",
      "class c = 7: 5000 real images\n",
      "class c = 8: 5000 real images\n",
      "class c = 9: 5000 real images\n",
      "real images channel 0, mean = -0.0000, std = 1.2211\n",
      "real images channel 1, mean = -0.0002, std = 1.2211\n",
      "real images channel 2, mean = 0.0002, std = 1.3014\n",
      "initialize synthetic data from random noise\n",
      "[2024-08-08 10:30:32] training begins\n",
      "-------------------------\n",
      "Evaluation\n",
      "model_train = ConvNet, model_eval = ConvNet, iteration = 0\n",
      "DSA augmentation strategy: \n",
      " None\n",
      "DSA augmentation parameters: \n",
      " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n",
      "[2024-08-08 10:30:37] Evaluate_00: epoch = 1000 train time = 3 s train loss = 0.000220 train acc = 1.0000, test acc = 0.1229\n",
      "[2024-08-08 10:30:42] Evaluate_01: epoch = 1000 train time = 3 s train loss = 0.000221 train acc = 1.0000, test acc = 0.1025\n",
      "[2024-08-08 10:30:48] Evaluate_02: epoch = 1000 train time = 3 s train loss = 0.000232 train acc = 1.0000, test acc = 0.1262\n",
      "[2024-08-08 10:30:53] Evaluate_03: epoch = 1000 train time = 3 s train loss = 0.000224 train acc = 1.0000, test acc = 0.1342\n",
      "[2024-08-08 10:30:58] Evaluate_04: epoch = 1000 train time = 3 s train loss = 0.000231 train acc = 1.0000, test acc = 0.1165\n",
      "[2024-08-08 10:31:03] Evaluate_05: epoch = 1000 train time = 3 s train loss = 0.000231 train acc = 1.0000, test acc = 0.1100\n",
      "[2024-08-08 10:31:08] Evaluate_06: epoch = 1000 train time = 3 s train loss = 0.000228 train acc = 1.0000, test acc = 0.1142\n",
      "[2024-08-08 10:31:13] Evaluate_07: epoch = 1000 train time = 3 s train loss = 0.000220 train acc = 1.0000, test acc = 0.1244\n",
      "[2024-08-08 10:31:18] Evaluate_08: epoch = 1000 train time = 3 s train loss = 0.000224 train acc = 1.0000, test acc = 0.1535\n",
      "[2024-08-08 10:31:23] Evaluate_09: epoch = 1000 train time = 3 s train loss = 0.000233 train acc = 1.0000, test acc = 0.1376\n",
      "[2024-08-08 10:31:28] Evaluate_10: epoch = 1000 train time = 3 s train loss = 0.000229 train acc = 1.0000, test acc = 0.1299\n",
      "[2024-08-08 10:31:33] Evaluate_11: epoch = 1000 train time = 3 s train loss = 0.000226 train acc = 1.0000, test acc = 0.1214\n",
      "[2024-08-08 10:31:38] Evaluate_12: epoch = 1000 train time = 3 s train loss = 0.000226 train acc = 1.0000, test acc = 0.1373\n",
      "[2024-08-08 10:31:44] Evaluate_13: epoch = 1000 train time = 3 s train loss = 0.000235 train acc = 1.0000, test acc = 0.1025\n",
      "[2024-08-08 10:31:49] Evaluate_14: epoch = 1000 train time = 3 s train loss = 0.000236 train acc = 1.0000, test acc = 0.1385\n",
      "[2024-08-08 10:31:54] Evaluate_15: epoch = 1000 train time = 3 s train loss = 0.000227 train acc = 1.0000, test acc = 0.1352\n",
      "[2024-08-08 10:31:59] Evaluate_16: epoch = 1000 train time = 3 s train loss = 0.000227 train acc = 1.0000, test acc = 0.1328\n",
      "[2024-08-08 10:32:04] Evaluate_17: epoch = 1000 train time = 3 s train loss = 0.000230 train acc = 1.0000, test acc = 0.1279\n",
      "[2024-08-08 10:32:09] Evaluate_18: epoch = 1000 train time = 3 s train loss = 0.000227 train acc = 1.0000, test acc = 0.1198\n",
      "[2024-08-08 10:32:14] Evaluate_19: epoch = 1000 train time = 3 s train loss = 0.000229 train acc = 1.0000, test acc = 0.1252\n",
      "Evaluate 20 random ConvNet, mean = 0.1256 std = 0.0124\n",
      "-------------------------\n",
      "[2024-08-08 10:32:15] iter = 0000, loss = 318.3199\n",
      "[2024-08-08 10:32:20] iter = 0010, loss = 248.7874\n",
      "[2024-08-08 10:32:26] iter = 0020, loss = 229.0619\n",
      "[2024-08-08 10:32:31] iter = 0030, loss = 219.5843\n",
      "[2024-08-08 10:32:37] iter = 0040, loss = 216.9256\n",
      "[2024-08-08 10:32:42] iter = 0050, loss = 214.8189\n",
      "[2024-08-08 10:32:48] iter = 0060, loss = 212.8019\n",
      "[2024-08-08 10:32:53] iter = 0070, loss = 209.8761\n",
      "[2024-08-08 10:32:58] iter = 0080, loss = 209.1344\n",
      "[2024-08-08 10:33:04] iter = 0090, loss = 214.2126\n",
      "[2024-08-08 10:33:09] iter = 0100, loss = 208.7462\n",
      "[2024-08-08 10:33:15] iter = 0110, loss = 203.3451\n",
      "[2024-08-08 10:33:20] iter = 0120, loss = 203.0942\n",
      "[2024-08-08 10:33:26] iter = 0130, loss = 198.7958\n",
      "[2024-08-08 10:33:31] iter = 0140, loss = 195.7972\n",
      "[2024-08-08 10:33:37] iter = 0150, loss = 200.0482\n",
      "[2024-08-08 10:33:42] iter = 0160, loss = 201.7313\n",
      "[2024-08-08 10:33:47] iter = 0170, loss = 197.4630\n",
      "[2024-08-08 10:33:53] iter = 0180, loss = 196.8215\n",
      "[2024-08-08 10:33:58] iter = 0190, loss = 192.4508\n",
      "[2024-08-08 10:34:04] iter = 0200, loss = 194.2326\n",
      "[2024-08-08 10:34:09] iter = 0210, loss = 199.2223\n",
      "[2024-08-08 10:34:15] iter = 0220, loss = 192.5813\n",
      "[2024-08-08 10:34:20] iter = 0230, loss = 194.9239\n",
      "[2024-08-08 10:34:25] iter = 0240, loss = 188.4282\n",
      "[2024-08-08 10:34:31] iter = 0250, loss = 187.9935\n",
      "[2024-08-08 10:34:36] iter = 0260, loss = 191.4127\n",
      "[2024-08-08 10:34:42] iter = 0270, loss = 194.4707\n",
      "[2024-08-08 10:34:47] iter = 0280, loss = 192.4488\n",
      "[2024-08-08 10:34:52] iter = 0290, loss = 191.4656\n",
      "[2024-08-08 10:34:58] iter = 0300, loss = 190.3495\n",
      "[2024-08-08 10:35:03] iter = 0310, loss = 192.5392\n",
      "[2024-08-08 10:35:09] iter = 0320, loss = 191.4618\n",
      "[2024-08-08 10:35:14] iter = 0330, loss = 185.3748\n",
      "[2024-08-08 10:35:19] iter = 0340, loss = 188.2705\n",
      "[2024-08-08 10:35:25] iter = 0350, loss = 191.8956\n",
      "[2024-08-08 10:35:30] iter = 0360, loss = 190.5753\n",
      "[2024-08-08 10:35:36] iter = 0370, loss = 190.7600\n",
      "[2024-08-08 10:35:41] iter = 0380, loss = 184.2861\n",
      "[2024-08-08 10:35:47] iter = 0390, loss = 188.4337\n",
      "[2024-08-08 10:35:52] iter = 0400, loss = 187.3039\n",
      "[2024-08-08 10:35:57] iter = 0410, loss = 192.8552\n",
      "[2024-08-08 10:36:03] iter = 0420, loss = 186.7248\n",
      "[2024-08-08 10:36:08] iter = 0430, loss = 187.1107\n",
      "[2024-08-08 10:36:14] iter = 0440, loss = 186.3234\n",
      "[2024-08-08 10:36:19] iter = 0450, loss = 184.9173\n",
      "[2024-08-08 10:36:25] iter = 0460, loss = 188.0523\n",
      "[2024-08-08 10:36:30] iter = 0470, loss = 187.4339\n",
      "[2024-08-08 10:36:36] iter = 0480, loss = 193.9550\n",
      "[2024-08-08 10:36:41] iter = 0490, loss = 186.2463\n",
      "-------------------------\n",
      "Evaluation\n",
      "model_train = ConvNet, model_eval = ConvNet, iteration = 500\n",
      "DSA augmentation strategy: \n",
      " None\n",
      "DSA augmentation parameters: \n",
      " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n",
      "[2024-08-08 10:36:51] Evaluate_00: epoch = 1000 train time = 2 s train loss = 0.000278 train acc = 1.0000, test acc = 0.2910\n",
      "[2024-08-08 10:36:56] Evaluate_01: epoch = 1000 train time = 3 s train loss = 0.000279 train acc = 1.0000, test acc = 0.2702\n",
      "[2024-08-08 10:37:02] Evaluate_02: epoch = 1000 train time = 3 s train loss = 0.000306 train acc = 1.0000, test acc = 0.2744\n",
      "[2024-08-08 10:37:07] Evaluate_03: epoch = 1000 train time = 3 s train loss = 0.000272 train acc = 1.0000, test acc = 0.2797\n",
      "[2024-08-08 10:37:12] Evaluate_04: epoch = 1000 train time = 3 s train loss = 0.000280 train acc = 1.0000, test acc = 0.2800\n",
      "[2024-08-08 10:37:17] Evaluate_05: epoch = 1000 train time = 3 s train loss = 0.000276 train acc = 1.0000, test acc = 0.2847\n",
      "[2024-08-08 10:37:23] Evaluate_06: epoch = 1000 train time = 3 s train loss = 0.000278 train acc = 1.0000, test acc = 0.2733\n",
      "[2024-08-08 10:37:28] Evaluate_07: epoch = 1000 train time = 3 s train loss = 0.000290 train acc = 1.0000, test acc = 0.2808\n",
      "[2024-08-08 10:37:34] Evaluate_08: epoch = 1000 train time = 3 s train loss = 0.000282 train acc = 1.0000, test acc = 0.2823\n",
      "[2024-08-08 10:37:39] Evaluate_09: epoch = 1000 train time = 3 s train loss = 0.000298 train acc = 1.0000, test acc = 0.2664\n",
      "[2024-08-08 10:37:44] Evaluate_10: epoch = 1000 train time = 3 s train loss = 0.000290 train acc = 1.0000, test acc = 0.2777\n",
      "[2024-08-08 10:37:50] Evaluate_11: epoch = 1000 train time = 3 s train loss = 0.000281 train acc = 1.0000, test acc = 0.2777\n",
      "[2024-08-08 10:37:55] Evaluate_12: epoch = 1000 train time = 3 s train loss = 0.000287 train acc = 1.0000, test acc = 0.2777\n",
      "[2024-08-08 10:38:01] Evaluate_13: epoch = 1000 train time = 3 s train loss = 0.000268 train acc = 1.0000, test acc = 0.2907\n",
      "[2024-08-08 10:38:06] Evaluate_14: epoch = 1000 train time = 3 s train loss = 0.000285 train acc = 1.0000, test acc = 0.2867\n",
      "[2024-08-08 10:38:12] Evaluate_15: epoch = 1000 train time = 3 s train loss = 0.000304 train acc = 1.0000, test acc = 0.2717\n",
      "[2024-08-08 10:38:17] Evaluate_16: epoch = 1000 train time = 3 s train loss = 0.000274 train acc = 1.0000, test acc = 0.2780\n",
      "[2024-08-08 10:38:23] Evaluate_17: epoch = 1000 train time = 3 s train loss = 0.000288 train acc = 1.0000, test acc = 0.2825\n",
      "[2024-08-08 10:38:28] Evaluate_18: epoch = 1000 train time = 3 s train loss = 0.000286 train acc = 1.0000, test acc = 0.2804\n",
      "[2024-08-08 10:38:33] Evaluate_19: epoch = 1000 train time = 3 s train loss = 0.000289 train acc = 1.0000, test acc = 0.2922\n",
      "Evaluate 20 random ConvNet, mean = 0.2799 std = 0.0067\n",
      "-------------------------\n",
      "[2024-08-08 10:38:34] iter = 0500, loss = 183.7246\n",
      "[2024-08-08 10:38:39] iter = 0510, loss = 186.9940\n",
      "[2024-08-08 10:38:44] iter = 0520, loss = 185.0919\n",
      "[2024-08-08 10:38:49] iter = 0530, loss = 183.1861\n",
      "[2024-08-08 10:38:55] iter = 0540, loss = 187.7880\n",
      "[2024-08-08 10:39:00] iter = 0550, loss = 189.3314\n",
      "[2024-08-08 10:39:05] iter = 0560, loss = 186.1372\n",
      "[2024-08-08 10:39:11] iter = 0570, loss = 182.1452\n",
      "[2024-08-08 10:39:16] iter = 0580, loss = 184.3385\n",
      "[2024-08-08 10:39:22] iter = 0590, loss = 186.1442\n",
      "[2024-08-08 10:39:27] iter = 0600, loss = 187.3061\n",
      "[2024-08-08 10:39:33] iter = 0610, loss = 183.4553\n",
      "[2024-08-08 10:39:38] iter = 0620, loss = 184.2799\n",
      "[2024-08-08 10:39:43] iter = 0630, loss = 185.7209\n",
      "[2024-08-08 10:39:49] iter = 0640, loss = 181.5544\n",
      "[2024-08-08 10:39:54] iter = 0650, loss = 187.9887\n",
      "[2024-08-08 10:40:00] iter = 0660, loss = 183.0372\n",
      "[2024-08-08 10:40:05] iter = 0670, loss = 183.4547\n",
      "[2024-08-08 10:40:10] iter = 0680, loss = 183.0164\n",
      "[2024-08-08 10:40:16] iter = 0690, loss = 182.9829\n",
      "[2024-08-08 10:40:21] iter = 0700, loss = 183.8371\n",
      "[2024-08-08 10:40:27] iter = 0710, loss = 183.1207\n",
      "[2024-08-08 10:40:32] iter = 0720, loss = 184.1064\n",
      "[2024-08-08 10:40:38] iter = 0730, loss = 184.8161\n",
      "[2024-08-08 10:40:43] iter = 0740, loss = 180.4080\n",
      "[2024-08-08 10:40:49] iter = 0750, loss = 179.5875\n",
      "[2024-08-08 10:40:54] iter = 0760, loss = 181.9042\n",
      "[2024-08-08 10:40:59] iter = 0770, loss = 183.1672\n",
      "[2024-08-08 10:41:05] iter = 0780, loss = 179.7801\n",
      "[2024-08-08 10:41:10] iter = 0790, loss = 181.5428\n",
      "[2024-08-08 10:41:16] iter = 0800, loss = 187.5796\n",
      "[2024-08-08 10:41:21] iter = 0810, loss = 181.6472\n",
      "[2024-08-08 10:41:27] iter = 0820, loss = 183.8540\n",
      "[2024-08-08 10:41:32] iter = 0830, loss = 175.4781\n",
      "[2024-08-08 10:41:38] iter = 0840, loss = 184.6975\n",
      "[2024-08-08 10:41:43] iter = 0850, loss = 184.2259\n",
      "[2024-08-08 10:41:48] iter = 0860, loss = 179.5806\n",
      "[2024-08-08 10:41:54] iter = 0870, loss = 183.7918\n",
      "[2024-08-08 10:41:59] iter = 0880, loss = 186.6724\n",
      "[2024-08-08 10:42:05] iter = 0890, loss = 184.1505\n",
      "[2024-08-08 10:42:10] iter = 0900, loss = 185.5153\n",
      "[2024-08-08 10:42:15] iter = 0910, loss = 183.3402\n",
      "[2024-08-08 10:42:20] iter = 0920, loss = 180.2676\n",
      "[2024-08-08 10:42:26] iter = 0930, loss = 180.8426\n",
      "[2024-08-08 10:42:31] iter = 0940, loss = 182.8974\n",
      "[2024-08-08 10:42:37] iter = 0950, loss = 178.2324\n",
      "[2024-08-08 10:42:42] iter = 0960, loss = 184.2703\n",
      "[2024-08-08 10:42:47] iter = 0970, loss = 182.8204\n",
      "[2024-08-08 10:42:53] iter = 0980, loss = 188.9733\n",
      "[2024-08-08 10:42:58] iter = 0990, loss = 183.8681\n",
      "-------------------------\n",
      "Evaluation\n",
      "model_train = ConvNet, model_eval = ConvNet, iteration = 1000\n",
      "DSA augmentation strategy: \n",
      " None\n",
      "DSA augmentation parameters: \n",
      " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n",
      "[2024-08-08 10:43:08] Evaluate_00: epoch = 1000 train time = 3 s train loss = 0.000288 train acc = 1.0000, test acc = 0.2898\n",
      "[2024-08-08 10:43:14] Evaluate_01: epoch = 1000 train time = 3 s train loss = 0.000294 train acc = 1.0000, test acc = 0.2828\n",
      "[2024-08-08 10:43:19] Evaluate_02: epoch = 1000 train time = 3 s train loss = 0.000292 train acc = 1.0000, test acc = 0.2801\n",
      "[2024-08-08 10:43:24] Evaluate_03: epoch = 1000 train time = 3 s train loss = 0.000279 train acc = 1.0000, test acc = 0.2721\n",
      "[2024-08-08 10:43:29] Evaluate_04: epoch = 1000 train time = 3 s train loss = 0.000295 train acc = 1.0000, test acc = 0.2817\n",
      "[2024-08-08 10:43:34] Evaluate_05: epoch = 1000 train time = 3 s train loss = 0.000293 train acc = 1.0000, test acc = 0.2828\n",
      "[2024-08-08 10:43:39] Evaluate_06: epoch = 1000 train time = 3 s train loss = 0.000303 train acc = 1.0000, test acc = 0.2861\n",
      "[2024-08-08 10:43:45] Evaluate_07: epoch = 1000 train time = 3 s train loss = 0.000283 train acc = 1.0000, test acc = 0.2880\n",
      "[2024-08-08 10:43:50] Evaluate_08: epoch = 1000 train time = 3 s train loss = 0.000277 train acc = 1.0000, test acc = 0.2897\n",
      "[2024-08-08 10:43:55] Evaluate_09: epoch = 1000 train time = 3 s train loss = 0.000298 train acc = 1.0000, test acc = 0.2778\n",
      "[2024-08-08 10:44:00] Evaluate_10: epoch = 1000 train time = 3 s train loss = 0.000279 train acc = 1.0000, test acc = 0.2889\n",
      "[2024-08-08 10:44:05] Evaluate_11: epoch = 1000 train time = 3 s train loss = 0.000277 train acc = 1.0000, test acc = 0.2750\n",
      "[2024-08-08 10:44:10] Evaluate_12: epoch = 1000 train time = 3 s train loss = 0.000287 train acc = 1.0000, test acc = 0.2910\n",
      "[2024-08-08 10:44:15] Evaluate_13: epoch = 1000 train time = 3 s train loss = 0.000318 train acc = 1.0000, test acc = 0.2689\n",
      "[2024-08-08 10:44:20] Evaluate_14: epoch = 1000 train time = 3 s train loss = 0.000294 train acc = 1.0000, test acc = 0.2891\n",
      "[2024-08-08 10:44:26] Evaluate_15: epoch = 1000 train time = 3 s train loss = 0.000272 train acc = 1.0000, test acc = 0.2830\n",
      "[2024-08-08 10:44:31] Evaluate_16: epoch = 1000 train time = 3 s train loss = 0.000278 train acc = 1.0000, test acc = 0.2813\n",
      "[2024-08-08 10:44:36] Evaluate_17: epoch = 1000 train time = 3 s train loss = 0.000282 train acc = 1.0000, test acc = 0.2806\n",
      "[2024-08-08 10:44:41] Evaluate_18: epoch = 1000 train time = 3 s train loss = 0.000307 train acc = 1.0000, test acc = 0.2810\n",
      "[2024-08-08 10:44:47] Evaluate_19: epoch = 1000 train time = 3 s train loss = 0.000287 train acc = 1.0000, test acc = 0.2788\n",
      "Evaluate 20 random ConvNet, mean = 0.2824 std = 0.0059\n",
      "-------------------------\n",
      "[2024-08-08 10:44:47] iter = 1000, loss = 176.5027\n",
      "\n",
      "================== Exp 2 ==================\n",
      " \n",
      "Hyper-parameters: \n",
      " {'method': 'DSA', 'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 1, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 1000, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 1, 'inner_loop': 1, 'device': 'cuda', 'dsa_param': <__main__.ParamDiffAug object at 0x000001DC10AC6C10>, 'dsa': True, 'dc_aug_param': None}\n",
      "Evaluation model pool:  ['ConvNet']\n",
      "class c = 0: 5000 real images\n",
      "class c = 1: 5000 real images\n",
      "class c = 2: 5000 real images\n",
      "class c = 3: 5000 real images\n",
      "class c = 4: 5000 real images\n",
      "class c = 5: 5000 real images\n",
      "class c = 6: 5000 real images\n",
      "class c = 7: 5000 real images\n",
      "class c = 8: 5000 real images\n",
      "class c = 9: 5000 real images\n",
      "real images channel 0, mean = -0.0000, std = 1.2211\n",
      "real images channel 1, mean = -0.0002, std = 1.2211\n",
      "real images channel 2, mean = 0.0002, std = 1.3014\n",
      "initialize synthetic data from random noise\n",
      "[2024-08-08 10:45:06] training begins\n",
      "-------------------------\n",
      "Evaluation\n",
      "model_train = ConvNet, model_eval = ConvNet, iteration = 0\n",
      "DSA augmentation strategy: \n",
      " None\n",
      "DSA augmentation parameters: \n",
      " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n",
      "[2024-08-08 10:45:11] Evaluate_00: epoch = 1000 train time = 3 s train loss = 0.000236 train acc = 1.0000, test acc = 0.1045\n",
      "[2024-08-08 10:45:17] Evaluate_01: epoch = 1000 train time = 3 s train loss = 0.000230 train acc = 1.0000, test acc = 0.1018\n",
      "[2024-08-08 10:45:23] Evaluate_02: epoch = 1000 train time = 3 s train loss = 0.000241 train acc = 1.0000, test acc = 0.0955\n",
      "[2024-08-08 10:45:28] Evaluate_03: epoch = 1000 train time = 3 s train loss = 0.000226 train acc = 1.0000, test acc = 0.1041\n",
      "[2024-08-08 10:45:34] Evaluate_04: epoch = 1000 train time = 3 s train loss = 0.000229 train acc = 1.0000, test acc = 0.0817\n",
      "[2024-08-08 10:45:40] Evaluate_05: epoch = 1000 train time = 3 s train loss = 0.000234 train acc = 1.0000, test acc = 0.1007\n",
      "[2024-08-08 10:45:45] Evaluate_06: epoch = 1000 train time = 3 s train loss = 0.000224 train acc = 1.0000, test acc = 0.0942\n",
      "[2024-08-08 10:45:51] Evaluate_07: epoch = 1000 train time = 3 s train loss = 0.000230 train acc = 1.0000, test acc = 0.0978\n",
      "[2024-08-08 10:45:56] Evaluate_08: epoch = 1000 train time = 3 s train loss = 0.000233 train acc = 1.0000, test acc = 0.0813\n",
      "[2024-08-08 10:46:01] Evaluate_09: epoch = 1000 train time = 3 s train loss = 0.000231 train acc = 1.0000, test acc = 0.0994\n",
      "[2024-08-08 10:46:06] Evaluate_10: epoch = 1000 train time = 3 s train loss = 0.000235 train acc = 1.0000, test acc = 0.0795\n",
      "[2024-08-08 10:46:12] Evaluate_11: epoch = 1000 train time = 3 s train loss = 0.000222 train acc = 1.0000, test acc = 0.0862\n",
      "[2024-08-08 10:46:17] Evaluate_12: epoch = 1000 train time = 3 s train loss = 0.000224 train acc = 1.0000, test acc = 0.0897\n",
      "[2024-08-08 10:46:22] Evaluate_13: epoch = 1000 train time = 3 s train loss = 0.000226 train acc = 1.0000, test acc = 0.0828\n",
      "[2024-08-08 10:46:28] Evaluate_14: epoch = 1000 train time = 3 s train loss = 0.000226 train acc = 1.0000, test acc = 0.0970\n",
      "[2024-08-08 10:46:33] Evaluate_15: epoch = 1000 train time = 3 s train loss = 0.000234 train acc = 1.0000, test acc = 0.0962\n",
      "[2024-08-08 10:46:39] Evaluate_16: epoch = 1000 train time = 3 s train loss = 0.000221 train acc = 1.0000, test acc = 0.0938\n",
      "[2024-08-08 10:46:44] Evaluate_17: epoch = 1000 train time = 3 s train loss = 0.000233 train acc = 1.0000, test acc = 0.1034\n",
      "[2024-08-08 10:46:49] Evaluate_18: epoch = 1000 train time = 3 s train loss = 0.000224 train acc = 1.0000, test acc = 0.0908\n",
      "[2024-08-08 10:46:55] Evaluate_19: epoch = 1000 train time = 3 s train loss = 0.000237 train acc = 1.0000, test acc = 0.0877\n",
      "Evaluate 20 random ConvNet, mean = 0.0934 std = 0.0079\n",
      "-------------------------\n",
      "[2024-08-08 10:46:56] iter = 0000, loss = 319.3598\n",
      "[2024-08-08 10:47:01] iter = 0010, loss = 246.5121\n",
      "[2024-08-08 10:47:07] iter = 0020, loss = 226.4933\n",
      "[2024-08-08 10:47:12] iter = 0030, loss = 217.0530\n",
      "[2024-08-08 10:47:18] iter = 0040, loss = 224.4603\n",
      "[2024-08-08 10:47:23] iter = 0050, loss = 220.6358\n",
      "[2024-08-08 10:47:29] iter = 0060, loss = 213.2435\n",
      "[2024-08-08 10:47:34] iter = 0070, loss = 207.8974\n",
      "[2024-08-08 10:47:40] iter = 0080, loss = 213.8301\n",
      "[2024-08-08 10:47:46] iter = 0090, loss = 205.3541\n",
      "[2024-08-08 10:47:51] iter = 0100, loss = 203.0853\n",
      "[2024-08-08 10:47:57] iter = 0110, loss = 204.8697\n",
      "[2024-08-08 10:48:02] iter = 0120, loss = 208.0288\n",
      "[2024-08-08 10:48:08] iter = 0130, loss = 198.5621\n",
      "[2024-08-08 10:48:14] iter = 0140, loss = 202.0298\n",
      "[2024-08-08 10:48:19] iter = 0150, loss = 200.4363\n",
      "[2024-08-08 10:48:25] iter = 0160, loss = 200.7223\n",
      "[2024-08-08 10:48:30] iter = 0170, loss = 200.8686\n",
      "[2024-08-08 10:48:36] iter = 0180, loss = 192.5255\n",
      "[2024-08-08 10:48:41] iter = 0190, loss = 194.1816\n",
      "[2024-08-08 10:48:47] iter = 0200, loss = 197.1876\n",
      "[2024-08-08 10:48:52] iter = 0210, loss = 192.5146\n",
      "[2024-08-08 10:48:58] iter = 0220, loss = 190.7740\n",
      "[2024-08-08 10:49:04] iter = 0230, loss = 189.9914\n",
      "[2024-08-08 10:49:09] iter = 0240, loss = 195.4001\n",
      "[2024-08-08 10:49:15] iter = 0250, loss = 193.0073\n",
      "[2024-08-08 10:49:20] iter = 0260, loss = 197.9606\n",
      "[2024-08-08 10:49:26] iter = 0270, loss = 195.4841\n",
      "[2024-08-08 10:49:31] iter = 0280, loss = 192.2742\n",
      "[2024-08-08 10:49:37] iter = 0290, loss = 200.4620\n",
      "[2024-08-08 10:49:42] iter = 0300, loss = 199.5912\n",
      "[2024-08-08 10:49:48] iter = 0310, loss = 189.0304\n",
      "[2024-08-08 10:49:53] iter = 0320, loss = 193.7086\n",
      "[2024-08-08 10:49:59] iter = 0330, loss = 193.4995\n",
      "[2024-08-08 10:50:04] iter = 0340, loss = 185.2540\n",
      "[2024-08-08 10:50:10] iter = 0350, loss = 194.9001\n",
      "[2024-08-08 10:50:15] iter = 0360, loss = 196.5373\n",
      "[2024-08-08 10:50:20] iter = 0370, loss = 191.1112\n",
      "[2024-08-08 10:50:26] iter = 0380, loss = 186.9082\n",
      "[2024-08-08 10:50:31] iter = 0390, loss = 193.9089\n",
      "[2024-08-08 10:50:37] iter = 0400, loss = 185.4042\n",
      "[2024-08-08 10:50:42] iter = 0410, loss = 187.1137\n",
      "[2024-08-08 10:50:48] iter = 0420, loss = 182.3641\n",
      "[2024-08-08 10:50:53] iter = 0430, loss = 189.9955\n",
      "[2024-08-08 10:50:59] iter = 0440, loss = 193.8799\n",
      "[2024-08-08 10:51:04] iter = 0450, loss = 186.6083\n",
      "[2024-08-08 10:51:10] iter = 0460, loss = 192.8465\n",
      "[2024-08-08 10:51:15] iter = 0470, loss = 191.8911\n",
      "[2024-08-08 10:51:21] iter = 0480, loss = 188.5132\n",
      "[2024-08-08 10:51:27] iter = 0490, loss = 189.9460\n",
      "-------------------------\n",
      "Evaluation\n",
      "model_train = ConvNet, model_eval = ConvNet, iteration = 500\n",
      "DSA augmentation strategy: \n",
      " None\n",
      "DSA augmentation parameters: \n",
      " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n",
      "[2024-08-08 10:51:37] Evaluate_00: epoch = 1000 train time = 3 s train loss = 0.000274 train acc = 1.0000, test acc = 0.2840\n",
      "[2024-08-08 10:51:42] Evaluate_01: epoch = 1000 train time = 3 s train loss = 0.000286 train acc = 1.0000, test acc = 0.2758\n",
      "[2024-08-08 10:51:48] Evaluate_02: epoch = 1000 train time = 3 s train loss = 0.000287 train acc = 1.0000, test acc = 0.2909\n",
      "[2024-08-08 10:51:53] Evaluate_03: epoch = 1000 train time = 3 s train loss = 0.000281 train acc = 1.0000, test acc = 0.2874\n",
      "[2024-08-08 10:51:58] Evaluate_04: epoch = 1000 train time = 3 s train loss = 0.000295 train acc = 1.0000, test acc = 0.2900\n",
      "[2024-08-08 10:52:03] Evaluate_05: epoch = 1000 train time = 3 s train loss = 0.000291 train acc = 1.0000, test acc = 0.2882\n",
      "[2024-08-08 10:52:08] Evaluate_06: epoch = 1000 train time = 3 s train loss = 0.000307 train acc = 1.0000, test acc = 0.2708\n",
      "[2024-08-08 10:52:13] Evaluate_07: epoch = 1000 train time = 3 s train loss = 0.000293 train acc = 1.0000, test acc = 0.2785\n",
      "[2024-08-08 10:52:19] Evaluate_08: epoch = 1000 train time = 3 s train loss = 0.000301 train acc = 1.0000, test acc = 0.2779\n",
      "[2024-08-08 10:52:24] Evaluate_09: epoch = 1000 train time = 3 s train loss = 0.000284 train acc = 1.0000, test acc = 0.2898\n",
      "[2024-08-08 10:52:29] Evaluate_10: epoch = 1000 train time = 3 s train loss = 0.000281 train acc = 1.0000, test acc = 0.2809\n",
      "[2024-08-08 10:52:35] Evaluate_11: epoch = 1000 train time = 3 s train loss = 0.000293 train acc = 1.0000, test acc = 0.2797\n",
      "[2024-08-08 10:52:40] Evaluate_12: epoch = 1000 train time = 3 s train loss = 0.000298 train acc = 1.0000, test acc = 0.2780\n",
      "[2024-08-08 10:52:46] Evaluate_13: epoch = 1000 train time = 3 s train loss = 0.000284 train acc = 1.0000, test acc = 0.2782\n",
      "[2024-08-08 10:52:51] Evaluate_14: epoch = 1000 train time = 3 s train loss = 0.000286 train acc = 1.0000, test acc = 0.2844\n",
      "[2024-08-08 10:52:57] Evaluate_15: epoch = 1000 train time = 3 s train loss = 0.000281 train acc = 1.0000, test acc = 0.2793\n",
      "[2024-08-08 10:53:03] Evaluate_16: epoch = 1000 train time = 3 s train loss = 0.000285 train acc = 1.0000, test acc = 0.2789\n",
      "[2024-08-08 10:53:09] Evaluate_17: epoch = 1000 train time = 3 s train loss = 0.000299 train acc = 1.0000, test acc = 0.2776\n",
      "[2024-08-08 10:53:14] Evaluate_18: epoch = 1000 train time = 3 s train loss = 0.000276 train acc = 1.0000, test acc = 0.2928\n",
      "[2024-08-08 10:53:19] Evaluate_19: epoch = 1000 train time = 3 s train loss = 0.000302 train acc = 1.0000, test acc = 0.2865\n",
      "Evaluate 20 random ConvNet, mean = 0.2825 std = 0.0058\n",
      "-------------------------\n",
      "[2024-08-08 10:53:20] iter = 0500, loss = 183.3717\n",
      "[2024-08-08 10:53:26] iter = 0510, loss = 188.1293\n",
      "[2024-08-08 10:53:31] iter = 0520, loss = 193.3495\n",
      "[2024-08-08 10:53:37] iter = 0530, loss = 189.5343\n",
      "[2024-08-08 10:53:42] iter = 0540, loss = 187.4376\n",
      "[2024-08-08 10:53:48] iter = 0550, loss = 183.2055\n",
      "[2024-08-08 10:53:53] iter = 0560, loss = 188.7954\n",
      "[2024-08-08 10:53:59] iter = 0570, loss = 187.8102\n",
      "[2024-08-08 10:54:05] iter = 0580, loss = 188.7843\n",
      "[2024-08-08 10:54:10] iter = 0590, loss = 186.0784\n",
      "[2024-08-08 10:54:16] iter = 0600, loss = 186.5547\n",
      "[2024-08-08 10:54:21] iter = 0610, loss = 189.9242\n",
      "[2024-08-08 10:54:27] iter = 0620, loss = 186.7695\n",
      "[2024-08-08 10:54:32] iter = 0630, loss = 178.8671\n",
      "[2024-08-08 10:54:38] iter = 0640, loss = 184.9422\n",
      "[2024-08-08 10:54:43] iter = 0650, loss = 184.4383\n",
      "[2024-08-08 10:54:49] iter = 0660, loss = 183.8533\n",
      "[2024-08-08 10:54:55] iter = 0670, loss = 183.4075\n",
      "[2024-08-08 10:55:00] iter = 0680, loss = 185.0381\n",
      "[2024-08-08 10:55:06] iter = 0690, loss = 180.8645\n",
      "[2024-08-08 10:55:11] iter = 0700, loss = 190.3027\n",
      "[2024-08-08 10:55:17] iter = 0710, loss = 184.1081\n",
      "[2024-08-08 10:55:22] iter = 0720, loss = 186.4102\n",
      "[2024-08-08 10:55:28] iter = 0730, loss = 184.5065\n",
      "[2024-08-08 10:55:33] iter = 0740, loss = 180.8932\n",
      "[2024-08-08 10:55:39] iter = 0750, loss = 177.7412\n",
      "[2024-08-08 10:55:44] iter = 0760, loss = 189.9393\n",
      "[2024-08-08 10:55:50] iter = 0770, loss = 187.2817\n",
      "[2024-08-08 10:55:55] iter = 0780, loss = 182.6924\n",
      "[2024-08-08 10:56:01] iter = 0790, loss = 181.7861\n",
      "[2024-08-08 10:56:06] iter = 0800, loss = 188.3945\n",
      "[2024-08-08 10:56:12] iter = 0810, loss = 175.7611\n",
      "[2024-08-08 10:56:17] iter = 0820, loss = 180.6012\n",
      "[2024-08-08 10:56:23] iter = 0830, loss = 188.7159\n",
      "[2024-08-08 10:56:28] iter = 0840, loss = 181.0581\n",
      "[2024-08-08 10:56:34] iter = 0850, loss = 178.6063\n",
      "[2024-08-08 10:56:39] iter = 0860, loss = 182.7101\n",
      "[2024-08-08 10:56:45] iter = 0870, loss = 189.6908\n",
      "[2024-08-08 10:56:50] iter = 0880, loss = 180.7762\n",
      "[2024-08-08 10:56:56] iter = 0890, loss = 179.1242\n",
      "[2024-08-08 10:57:01] iter = 0900, loss = 181.1822\n",
      "[2024-08-08 10:57:06] iter = 0910, loss = 184.2790\n",
      "[2024-08-08 10:57:12] iter = 0920, loss = 183.8257\n",
      "[2024-08-08 10:57:17] iter = 0930, loss = 183.1845\n",
      "[2024-08-08 10:57:23] iter = 0940, loss = 191.4639\n",
      "[2024-08-08 10:57:28] iter = 0950, loss = 179.3942\n",
      "[2024-08-08 10:57:34] iter = 0960, loss = 186.1861\n",
      "[2024-08-08 10:57:39] iter = 0970, loss = 183.7038\n",
      "[2024-08-08 10:57:45] iter = 0980, loss = 189.3354\n",
      "[2024-08-08 10:57:50] iter = 0990, loss = 183.8686\n",
      "-------------------------\n",
      "Evaluation\n",
      "model_train = ConvNet, model_eval = ConvNet, iteration = 1000\n",
      "DSA augmentation strategy: \n",
      " None\n",
      "DSA augmentation parameters: \n",
      " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n",
      "[2024-08-08 10:58:00] Evaluate_00: epoch = 1000 train time = 3 s train loss = 0.000281 train acc = 1.0000, test acc = 0.3001\n",
      "[2024-08-08 10:58:06] Evaluate_01: epoch = 1000 train time = 3 s train loss = 0.000298 train acc = 1.0000, test acc = 0.2858\n",
      "[2024-08-08 10:58:13] Evaluate_02: epoch = 1000 train time = 3 s train loss = 0.000297 train acc = 1.0000, test acc = 0.2812\n",
      "[2024-08-08 10:58:18] Evaluate_03: epoch = 1000 train time = 3 s train loss = 0.000302 train acc = 1.0000, test acc = 0.2900\n",
      "[2024-08-08 10:58:23] Evaluate_04: epoch = 1000 train time = 3 s train loss = 0.000302 train acc = 1.0000, test acc = 0.2930\n",
      "[2024-08-08 10:58:29] Evaluate_05: epoch = 1000 train time = 3 s train loss = 0.000278 train acc = 1.0000, test acc = 0.2888\n",
      "[2024-08-08 10:58:34] Evaluate_06: epoch = 1000 train time = 3 s train loss = 0.000301 train acc = 1.0000, test acc = 0.2778\n",
      "[2024-08-08 10:58:40] Evaluate_07: epoch = 1000 train time = 3 s train loss = 0.000290 train acc = 1.0000, test acc = 0.2845\n",
      "[2024-08-08 10:58:45] Evaluate_08: epoch = 1000 train time = 3 s train loss = 0.000308 train acc = 1.0000, test acc = 0.2831\n",
      "[2024-08-08 10:58:50] Evaluate_09: epoch = 1000 train time = 3 s train loss = 0.000300 train acc = 1.0000, test acc = 0.2790\n",
      "[2024-08-08 10:58:56] Evaluate_10: epoch = 1000 train time = 3 s train loss = 0.000285 train acc = 1.0000, test acc = 0.2944\n",
      "[2024-08-08 10:59:01] Evaluate_11: epoch = 1000 train time = 3 s train loss = 0.000299 train acc = 1.0000, test acc = 0.2816\n",
      "[2024-08-08 10:59:06] Evaluate_12: epoch = 1000 train time = 3 s train loss = 0.000298 train acc = 1.0000, test acc = 0.2795\n",
      "[2024-08-08 10:59:12] Evaluate_13: epoch = 1000 train time = 3 s train loss = 0.000291 train acc = 1.0000, test acc = 0.2782\n",
      "[2024-08-08 10:59:17] Evaluate_14: epoch = 1000 train time = 3 s train loss = 0.000295 train acc = 1.0000, test acc = 0.2914\n",
      "[2024-08-08 10:59:22] Evaluate_15: epoch = 1000 train time = 3 s train loss = 0.000302 train acc = 1.0000, test acc = 0.2740\n",
      "[2024-08-08 10:59:28] Evaluate_16: epoch = 1000 train time = 3 s train loss = 0.000282 train acc = 1.0000, test acc = 0.2841\n",
      "[2024-08-08 10:59:33] Evaluate_17: epoch = 1000 train time = 3 s train loss = 0.000292 train acc = 1.0000, test acc = 0.2863\n",
      "[2024-08-08 10:59:39] Evaluate_18: epoch = 1000 train time = 3 s train loss = 0.000284 train acc = 1.0000, test acc = 0.2841\n",
      "[2024-08-08 10:59:44] Evaluate_19: epoch = 1000 train time = 3 s train loss = 0.000293 train acc = 1.0000, test acc = 0.2894\n",
      "Evaluate 20 random ConvNet, mean = 0.2853 std = 0.0063\n",
      "-------------------------\n",
      "[2024-08-08 10:59:45] iter = 1000, loss = 180.2064\n",
      "\n",
      "================== Exp 3 ==================\n",
      " \n",
      "Hyper-parameters: \n",
      " {'method': 'DSA', 'dataset': 'CIFAR10', 'model': 'ConvNet', 'ipc': 1, 'eval_mode': 'S', 'num_exp': 5, 'num_eval': 20, 'epoch_eval_train': 1000, 'Iteration': 1000, 'lr_img': 0.1, 'lr_net': 0.01, 'batch_real': 256, 'batch_train': 256, 'init': 'noise', 'dsa_strategy': 'None', 'data_path': 'data', 'save_path': 'result', 'dis_metric': 'ours', 'outer_loop': 1, 'inner_loop': 1, 'device': 'cuda', 'dsa_param': <__main__.ParamDiffAug object at 0x000001DC10AC6C10>, 'dsa': True, 'dc_aug_param': None}\n",
      "Evaluation model pool:  ['ConvNet']\n",
      "class c = 0: 5000 real images\n",
      "class c = 1: 5000 real images\n",
      "class c = 2: 5000 real images\n",
      "class c = 3: 5000 real images\n",
      "class c = 4: 5000 real images\n",
      "class c = 5: 5000 real images\n",
      "class c = 6: 5000 real images\n",
      "class c = 7: 5000 real images\n",
      "class c = 8: 5000 real images\n",
      "class c = 9: 5000 real images\n",
      "real images channel 0, mean = -0.0000, std = 1.2211\n",
      "real images channel 1, mean = -0.0002, std = 1.2211\n",
      "real images channel 2, mean = 0.0002, std = 1.3014\n",
      "initialize synthetic data from random noise\n",
      "[2024-08-08 11:00:03] training begins\n",
      "-------------------------\n",
      "Evaluation\n",
      "model_train = ConvNet, model_eval = ConvNet, iteration = 0\n",
      "DSA augmentation strategy: \n",
      " None\n",
      "DSA augmentation parameters: \n",
      " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n",
      "[2024-08-08 11:00:08] Evaluate_00: epoch = 1000 train time = 3 s train loss = 0.000224 train acc = 1.0000, test acc = 0.0874\n",
      "[2024-08-08 11:00:14] Evaluate_01: epoch = 1000 train time = 3 s train loss = 0.000228 train acc = 1.0000, test acc = 0.1013\n",
      "[2024-08-08 11:00:19] Evaluate_02: epoch = 1000 train time = 3 s train loss = 0.000233 train acc = 1.0000, test acc = 0.0997\n",
      "[2024-08-08 11:00:25] Evaluate_03: epoch = 1000 train time = 3 s train loss = 0.000230 train acc = 1.0000, test acc = 0.0927\n",
      "[2024-08-08 11:00:30] Evaluate_04: epoch = 1000 train time = 3 s train loss = 0.000225 train acc = 1.0000, test acc = 0.0861\n",
      "[2024-08-08 11:00:36] Evaluate_05: epoch = 1000 train time = 3 s train loss = 0.000232 train acc = 1.0000, test acc = 0.0891\n",
      "[2024-08-08 11:00:41] Evaluate_06: epoch = 1000 train time = 3 s train loss = 0.000232 train acc = 1.0000, test acc = 0.0858\n",
      "[2024-08-08 11:00:47] Evaluate_07: epoch = 1000 train time = 3 s train loss = 0.000232 train acc = 1.0000, test acc = 0.0861\n",
      "[2024-08-08 11:00:52] Evaluate_08: epoch = 1000 train time = 3 s train loss = 0.000226 train acc = 1.0000, test acc = 0.0774\n",
      "[2024-08-08 11:00:58] Evaluate_09: epoch = 1000 train time = 3 s train loss = 0.000228 train acc = 1.0000, test acc = 0.0901\n",
      "[2024-08-08 11:01:03] Evaluate_10: epoch = 1000 train time = 3 s train loss = 0.000226 train acc = 1.0000, test acc = 0.0865\n",
      "[2024-08-08 11:01:09] Evaluate_11: epoch = 1000 train time = 3 s train loss = 0.000232 train acc = 1.0000, test acc = 0.0853\n",
      "[2024-08-08 11:01:14] Evaluate_12: epoch = 1000 train time = 3 s train loss = 0.000231 train acc = 1.0000, test acc = 0.0859\n",
      "[2024-08-08 11:01:20] Evaluate_13: epoch = 1000 train time = 3 s train loss = 0.000234 train acc = 1.0000, test acc = 0.0875\n",
      "[2024-08-08 11:01:25] Evaluate_14: epoch = 1000 train time = 3 s train loss = 0.000223 train acc = 1.0000, test acc = 0.0928\n",
      "[2024-08-08 11:01:30] Evaluate_15: epoch = 1000 train time = 3 s train loss = 0.000227 train acc = 1.0000, test acc = 0.1004\n",
      "[2024-08-08 11:01:35] Evaluate_16: epoch = 1000 train time = 3 s train loss = 0.000227 train acc = 1.0000, test acc = 0.0954\n",
      "[2024-08-08 11:01:41] Evaluate_17: epoch = 1000 train time = 3 s train loss = 0.000236 train acc = 1.0000, test acc = 0.1017\n",
      "[2024-08-08 11:01:47] Evaluate_18: epoch = 1000 train time = 3 s train loss = 0.000232 train acc = 1.0000, test acc = 0.0974\n",
      "[2024-08-08 11:01:52] Evaluate_19: epoch = 1000 train time = 3 s train loss = 0.000231 train acc = 1.0000, test acc = 0.1062\n",
      "Evaluate 20 random ConvNet, mean = 0.0917 std = 0.0072\n",
      "-------------------------\n",
      "[2024-08-08 11:01:53] iter = 0000, loss = 316.7773\n",
      "[2024-08-08 11:01:58] iter = 0010, loss = 251.3792\n",
      "[2024-08-08 11:02:04] iter = 0020, loss = 234.3186\n",
      "[2024-08-08 11:02:09] iter = 0030, loss = 227.1804\n",
      "[2024-08-08 11:02:15] iter = 0040, loss = 223.3296\n",
      "[2024-08-08 11:02:20] iter = 0050, loss = 215.8676\n",
      "[2024-08-08 11:02:25] iter = 0060, loss = 217.0677\n",
      "[2024-08-08 11:02:31] iter = 0070, loss = 208.6996\n",
      "[2024-08-08 11:02:36] iter = 0080, loss = 207.8964\n",
      "[2024-08-08 11:02:41] iter = 0090, loss = 201.4914\n",
      "[2024-08-08 11:02:46] iter = 0100, loss = 199.6087\n",
      "[2024-08-08 11:02:52] iter = 0110, loss = 206.2665\n",
      "[2024-08-08 11:02:57] iter = 0120, loss = 200.1414\n",
      "[2024-08-08 11:03:03] iter = 0130, loss = 199.2965\n",
      "[2024-08-08 11:03:08] iter = 0140, loss = 204.8199\n",
      "[2024-08-08 11:03:14] iter = 0150, loss = 204.2520\n",
      "[2024-08-08 11:03:20] iter = 0160, loss = 200.1285\n",
      "[2024-08-08 11:03:25] iter = 0170, loss = 195.9187\n",
      "[2024-08-08 11:03:31] iter = 0180, loss = 200.5460\n",
      "[2024-08-08 11:03:36] iter = 0190, loss = 200.7831\n",
      "[2024-08-08 11:03:42] iter = 0200, loss = 198.2452\n",
      "[2024-08-08 11:03:47] iter = 0210, loss = 195.4955\n",
      "[2024-08-08 11:03:53] iter = 0220, loss = 190.6752\n",
      "[2024-08-08 11:03:58] iter = 0230, loss = 193.2267\n",
      "[2024-08-08 11:04:04] iter = 0240, loss = 190.6917\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 213\u001b[0m\n\u001b[0;32m    210\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRun \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m experiments for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: mean = \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m std = \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39m(\u001b[38;5;28mlen\u001b[39m(accs), key, np\u001b[38;5;241m.\u001b[39mmean(accs), np\u001b[38;5;241m.\u001b[39mstd(accs)))\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 213\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 173\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    171\u001b[0m output_real \u001b[38;5;241m=\u001b[39m net(img_real)\n\u001b[0;32m    172\u001b[0m loss_real \u001b[38;5;241m=\u001b[39m criterion(output_real, lab_real)\n\u001b[1;32m--> 173\u001b[0m gw_real \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_real\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet_parameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m gw_real \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m((_\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m gw_real))\n\u001b[0;32m    176\u001b[0m output_syn \u001b[38;5;241m=\u001b[39m net(img_syn)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DDP\\lib\\site-packages\\torch\\autograd\\__init__.py:288\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[0;32m    283\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_inputs argument is deprecated and is ignored now \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(defaults to True). To accumulate gradient for other \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    285\u001b[0m                   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts of the graph, please use torch.autograd.backward.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    287\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_outputs, \u001b[38;5;28mlen\u001b[39m(t_outputs))\n\u001b[1;32m--> 288\u001b[0m grad_outputs_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    291\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DDP\\lib\\site-packages\\torch\\autograd\\__init__.py:89\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     88\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad can be implicitly created only for scalar outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 89\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import save_image\n",
    "def main():\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Parameter Processing')\n",
    "    parser.add_argument('--method', type=str, default='DSA', help='DC/DSA')\n",
    "    parser.add_argument('--dataset', type=str, default='CIFAR10', help='dataset')\n",
    "    parser.add_argument('--model', type=str, default='ConvNet', help='model')\n",
    "    parser.add_argument('--ipc', type=int, default=1, help='image(s) per class')\n",
    "    parser.add_argument('--eval_mode', type=str, default='S', help='eval_mode')\n",
    "    parser.add_argument('--num_exp', type=int, default=5, help='the number of experiments')\n",
    "    parser.add_argument('--num_eval', type=int, default=20, help='the number of evaluating randomly initialized models')\n",
    "    parser.add_argument('--epoch_eval_train', type=int, default=300, help='epochs to train a model with synthetic data')\n",
    "    parser.add_argument('--Iteration', type=int, default=1000, help='training iterations')\n",
    "    parser.add_argument('--lr_img', type=float, default=0.1, help='learning rate for updating synthetic images')\n",
    "    parser.add_argument('--lr_net', type=float, default=0.01, help='learning rate for updating network parameters')\n",
    "    parser.add_argument('--batch_real', type=int, default=256, help='batch size for real data')\n",
    "    parser.add_argument('--batch_train', type=int, default=256, help='batch size for training networks')\n",
    "    parser.add_argument('--init', type=str, default='noise', help='noise/real: initialize synthetic images from random noise or randomly sampled real images.')\n",
    "    parser.add_argument('--dsa_strategy', type=str, default='None', help='differentiable Siamese augmentation strategy')\n",
    "    parser.add_argument('--data_path', type=str, default='data', help='dataset path')\n",
    "    parser.add_argument('--save_path', type=str, default='result', help='path to save results')\n",
    "    parser.add_argument('--dis_metric', type=str, default='ours', help='distance metric')\n",
    "\n",
    "    # Handle unrecognized arguments\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    if unknown:\n",
    "        print(f\"Unrecognized arguments: {unknown}\")\n",
    "\n",
    "    args.outer_loop, args.inner_loop = get_loops(args.ipc)\n",
    "    args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    args.dsa_param = ParamDiffAug()\n",
    "    args.dsa = True if args.method == 'DSA' else False\n",
    "\n",
    "    if not os.path.exists(args.data_path):\n",
    "        os.mkdir(args.data_path)\n",
    "\n",
    "    if not os.path.exists(args.save_path):\n",
    "        os.mkdir(args.save_path)\n",
    "\n",
    "    eval_it_pool = np.arange(0, args.Iteration+1, 500).tolist() if args.eval_mode == 'S' or args.eval_mode == 'SS' else [args.Iteration]\n",
    "    print('eval_it_pool: ', eval_it_pool)\n",
    "    channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = get_dataset(args.dataset, args.data_path)\n",
    "    model_eval_pool = get_eval_pool(args.eval_mode, args.model, args.model)\n",
    "\n",
    "    accs_all_exps = dict()\n",
    "    for key in model_eval_pool:\n",
    "        accs_all_exps[key] = []\n",
    "\n",
    "    data_save = []\n",
    "\n",
    "    for exp in range(args.num_exp):\n",
    "        print('\\n================== Exp %d ==================\\n '%exp)\n",
    "        print('Hyper-parameters: \\n', args.__dict__)\n",
    "        print('Evaluation model pool: ', model_eval_pool)\n",
    "\n",
    "        images_all = []\n",
    "        labels_all = []\n",
    "        indices_class = [[] for c in range(num_classes)]\n",
    "\n",
    "        images_all = [torch.unsqueeze(dst_train[i][0], dim=0) for i in range(len(dst_train))]\n",
    "        labels_all = [dst_train[i][1] for i in range(len(dst_train))]\n",
    "        for i, lab in enumerate(labels_all):\n",
    "            indices_class[lab].append(i)\n",
    "        images_all = torch.cat(images_all, dim=0).to(args.device)\n",
    "        labels_all = torch.tensor(labels_all, dtype=torch.long, device=args.device)\n",
    "\n",
    "        for c in range(num_classes):\n",
    "            print('class c = %d: %d real images'%(c, len(indices_class[c])))\n",
    "\n",
    "        def get_images(c, n):\n",
    "            idx_shuffle = np.random.permutation(indices_class[c])[:n]\n",
    "            return images_all[idx_shuffle]\n",
    "\n",
    "        for ch in range(channel):\n",
    "            print('real images channel %d, mean = %.4f, std = %.4f'%(ch, torch.mean(images_all[:, ch]), torch.std(images_all[:, ch])))\n",
    "\n",
    "        image_syn = torch.randn(size=(num_classes*args.ipc, channel, im_size[0], im_size[1]), dtype=torch.float, requires_grad=True, device=args.device)\n",
    "        label_syn = torch.tensor([np.ones(args.ipc)*i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1)\n",
    "\n",
    "        if args.init == 'real':\n",
    "            print('initialize synthetic data from random real images')\n",
    "            for c in range(num_classes):\n",
    "                image_syn.data[c*args.ipc:(c+1)*args.ipc] = get_images(c, args.ipc).detach().data\n",
    "        else:\n",
    "            print('initialize synthetic data from random noise')\n",
    "\n",
    "        optimizer_img = torch.optim.SGD([image_syn, ], lr=args.lr_img, momentum=0.5)\n",
    "        optimizer_img.zero_grad()\n",
    "        criterion = nn.CrossEntropyLoss().to(args.device)\n",
    "        print('%s training begins'%get_time())\n",
    "\n",
    "        for it in range(args.Iteration+1):\n",
    "\n",
    "            if it in eval_it_pool:\n",
    "                for model_eval in model_eval_pool:\n",
    "                    print('-------------------------\\nEvaluation\\nmodel_train = %s, model_eval = %s, iteration = %d'%(args.model, model_eval, it))\n",
    "                    if args.dsa:\n",
    "                        args.epoch_eval_train = 1000\n",
    "                        args.dc_aug_param = None\n",
    "                        print('DSA augmentation strategy: \\n', args.dsa_strategy)\n",
    "                        print('DSA augmentation parameters: \\n', args.dsa_param.__dict__)\n",
    "                    else:\n",
    "                        args.dc_aug_param = get_daparam(args.dataset, args.model, model_eval, args.ipc)\n",
    "                        print('DC augmentation parameters: \\n', args.dc_aug_param)\n",
    "\n",
    "                    if args.dsa or args.dc_aug_param['strategy'] != 'none':\n",
    "                        args.epoch_eval_train = 1000\n",
    "                    else:\n",
    "                        args.epoch_eval_train = 300\n",
    "\n",
    "                    accs = []\n",
    "                    for it_eval in range(args.num_eval):\n",
    "                        net_eval = get_network(model_eval, channel, num_classes, im_size).to(args.device)\n",
    "                        image_syn_eval, label_syn_eval = copy.deepcopy(image_syn.detach()), copy.deepcopy(label_syn.detach())\n",
    "                        _, acc_train, acc_test = evaluate_synset(it_eval, net_eval, image_syn_eval, label_syn_eval, testloader, args)\n",
    "                        accs.append(acc_test)\n",
    "                    print('Evaluate %d random %s, mean = %.4f std = %.4f\\n-------------------------'%(len(accs), model_eval, np.mean(accs), np.std(accs)))\n",
    "\n",
    "                    if it == args.Iteration:\n",
    "                        accs_all_exps[model_eval] += accs\n",
    "\n",
    "                save_name = os.path.join(args.save_path, 'vis_%s_%s_%s_%dipc_exp%d_iter%d.png'%(args.method, args.dataset, args.model, args.ipc, exp, it))\n",
    "                image_syn_vis = copy.deepcopy(image_syn.detach().cpu())\n",
    "                for ch in range(channel):\n",
    "                    image_syn_vis[:, ch] = image_syn_vis[:, ch]  * std[ch] + mean[ch]\n",
    "                image_syn_vis[image_syn_vis<0] = 0.0\n",
    "                image_syn_vis[image_syn_vis>1] = 1.0\n",
    "                save_image(image_syn_vis, save_name, nrow=args.ipc)\n",
    "\n",
    "            net = get_network(args.model, channel, num_classes, im_size).to(args.device)\n",
    "            net.train()\n",
    "            net_parameters = list(net.parameters())\n",
    "            optimizer_net = torch.optim.SGD(net.parameters(), lr=args.lr_net)\n",
    "            optimizer_net.zero_grad()\n",
    "            loss_avg = 0\n",
    "            args.dc_aug_param = None\n",
    "\n",
    "            for ol in range(args.outer_loop):\n",
    "\n",
    "                BN_flag = False\n",
    "                BNSizePC = 16\n",
    "                for module in net.modules():\n",
    "                    if 'BatchNorm' in module._get_name():\n",
    "                        BN_flag = True\n",
    "                if BN_flag:\n",
    "                    img_real = torch.cat([get_images(c, BNSizePC) for c in range(num_classes)], dim=0)\n",
    "                    net.train()\n",
    "                    output_real = net(img_real)\n",
    "                    for module in net.modules():\n",
    "                        if 'BatchNorm' in module._get_name():\n",
    "                            module.eval()\n",
    "\n",
    "                loss = torch.tensor(0.0).to(args.device)\n",
    "                for c in range(num_classes):\n",
    "                    img_real = get_images(c, args.batch_real)\n",
    "                    lab_real = torch.ones((img_real.shape[0],), device=args.device, dtype=torch.long) * c\n",
    "                    img_syn = image_syn[c*args.ipc:(c+1)*args.ipc].reshape((args.ipc, channel, im_size[0], im_size[1]))\n",
    "                    lab_syn = torch.ones((img_syn.shape[0],), device=args.device, dtype=torch.long) * c\n",
    "                    if args.dsa:\n",
    "                        img_syn = DiffAugment(img_syn, args.dsa_strategy, param=args.dsa_param)\n",
    "                    else:\n",
    "                        img_syn = augment(img_syn, args.dc_aug_param)\n",
    "\n",
    "                    output_real = net(img_real)\n",
    "                    loss_real = criterion(output_real, lab_real)\n",
    "                    gw_real = torch.autograd.grad(loss_real, net_parameters)\n",
    "                    gw_real = list((_.detach().clone() for _ in gw_real))\n",
    "\n",
    "                    output_syn = net(img_syn)\n",
    "                    loss_syn = criterion(output_syn, lab_syn)\n",
    "                    gw_syn = torch.autograd.grad(loss_syn, net_parameters, create_graph=True)\n",
    "\n",
    "                    loss += match_loss(gw_syn, gw_real, args)\n",
    "\n",
    "                optimizer_img.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_img.step()\n",
    "                loss_avg += loss.item()\n",
    "\n",
    "                if ol == args.outer_loop - 1:\n",
    "                    break\n",
    "\n",
    "                image_syn_train, label_syn_train = copy.deepcopy(image_syn.detach()), copy.deepcopy(label_syn.detach())\n",
    "                dst_syn_train = TensorDataset(image_syn_train, label_syn_train)\n",
    "                trainloader = torch.utils.data.DataLoader(dst_syn_train, batch_size=args.batch_train, shuffle=True, num_workers=0)\n",
    "                for il in range(args.inner_loop):\n",
    "                    epoch('train', trainloader, net, optimizer_net, criterion, args, aug = True if args.dsa else False)\n",
    "\n",
    "                if args.dsa:\n",
    "                    image_syn.data = torch.clamp(image_syn, 0.0, 1.0)\n",
    "\n",
    "            loss_avg /= (num_classes*args.outer_loop)\n",
    "\n",
    "            if it % 10 == 0:\n",
    "                print('%s iter = %04d, loss = %.4f'%(get_time(), it, loss_avg))\n",
    "\n",
    "        data_save.append([copy.deepcopy(image_syn.detach().cpu()), copy.deepcopy(label_syn.detach().cpu())])\n",
    "        torch.save({'data': data_save, 'accs_all_exps': accs_all_exps}, os.path.join(args.save_path, 'res_%s_%s_%s_%dipc.pt'%(args.method, args.dataset, args.model, args.ipc)))\n",
    "\n",
    "    print('\\n==================== Final Results ====================\\n')\n",
    "    for key in model_eval_pool:\n",
    "        accs = accs_all_exps[key]\n",
    "        print('Run %d experiments for %s: mean = %.4f std = %.4f\\n-------------------------\\n'%(len(accs), key, np.mean(accs), np.std(accs)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27315dce-2093-422a-8fa3-b14aaf32324b",
   "metadata": {},
   "source": [
    "# distillation avec la methode DM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c04faa6-bbd0-4a00-af4c-4a56180609b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (2779585543.py, line 171)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[17], line 171\u001b[1;36m\u001b[0m\n\u001b[1;33m    images_real_all = torch.cat(images_real_all, dim=0\u001b[0m\n\u001b[1;37m                                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Parameter Processing')\n",
    "    parser.add_argument('--dataset', type=str, default='CIFAR10', help='dataset')\n",
    "    parser.add_argument('--model', type=str, default='ConvNet', help='model')\n",
    "    parser.add_argument('--ipc', type=int, default=10, help='image(s) per class')\n",
    "    parser.add_argument('--eval_mode', type=str, default='SS', help='eval_mode')\n",
    "    parser.add_argument('--num_exp', type=int, default=5, help='the number of experiments')\n",
    "    parser.add_argument('--num_eval', type=int, default=20, help='the number of evaluating randomly initialized models')\n",
    "    parser.add_argument('--epoch_eval_train', type=int, default=1000, help='epochs to train a model with synthetic data')\n",
    "    parser.add_argument('--Iteration', type=int, default=500, help='training iterations')\n",
    "    parser.add_argument('--lr_img', type=float, default=1.0, help='learning rate for updating synthetic images')\n",
    "    parser.add_argument('--lr_net', type=float, default=0.01, help='learning rate for updating network parameters')\n",
    "    parser.add_argument('--batch_real', type=int, default=256, help='batch size for real data')\n",
    "    parser.add_argument('--batch_train', type=int, default=256, help='batch size for training networks')\n",
    "    parser.add_argument('--init', type=str, default='real', help='noise/real: initialize synthetic images from random noise or randomly sampled real images.')\n",
    "    parser.add_argument('--dsa_strategy', type=str, default='color_crop_cutout_flip_scale_rotate', help='differentiable Siamese augmentation strategy')\n",
    "    parser.add_argument('--data_path', type=str, default='data', help='dataset path')\n",
    "    parser.add_argument('--save_path', type=str, default='result', help='path to save results')\n",
    "    parser.add_argument('--dis_metric', type=str, default='ours', help='distance metric')\n",
    "\n",
    "    # Ignorer les arguments non reconnus lorsque vous excutez dans Jupyter ou des environnements similaires\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "    args.method = 'DM'\n",
    "    args.outer_loop, args.inner_loop = get_loops(args.ipc)\n",
    "    args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    args.dsa_param = ParamDiffAug()\n",
    "    args.dsa = False if args.dsa_strategy in ['none', 'None'] else True\n",
    "\n",
    "    if not os.path.exists(args.data_path):\n",
    "        os.mkdir(args.data_path)\n",
    "\n",
    "    if not os.path.exists(args.save_path):\n",
    "        os.mkdir(args.save_path)\n",
    "\n",
    "    eval_it_pool = np.arange(0, args.Iteration+1, 2000).tolist() if args.eval_mode == 'S' or args.eval_mode == 'SS' else [args.Iteration]\n",
    "    print('eval_it_pool: ', eval_it_pool)\n",
    "    channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = get_dataset(args.dataset, args.data_path)\n",
    "    model_eval_pool = get_eval_pool(args.eval_mode, args.model, args.model)\n",
    "\n",
    "    accs_all_exps = dict()\n",
    "    for key in model_eval_pool:\n",
    "        accs_all_exps[key] = []\n",
    "\n",
    "    data_save = []\n",
    "\n",
    "    for exp in range(args.num_exp):\n",
    "        print('\\n================== Exp %d ==================\\n ' % exp)\n",
    "        print('Hyper-parameters: \\n', args.__dict__)\n",
    "        print('Evaluation model pool: ', model_eval_pool)\n",
    "\n",
    "        ''' Organiser le dataset rel '''\n",
    "        images_all = []\n",
    "        labels_all = []\n",
    "        indices_class = [[] for c in range(num_classes)]\n",
    "\n",
    "        images_all = [torch.unsqueeze(dst_train[i][0], dim=0) for i in range(len(dst_train))]\n",
    "        labels_all = [dst_train[i][1] for i in range(len(dst_train))]\n",
    "        for i, lab in enumerate(labels_all):\n",
    "            indices_class[lab].append(i)\n",
    "        images_all = torch.cat(images_all, dim=0).to(args.device)\n",
    "        labels_all = torch.tensor(labels_all, dtype=torch.long, device=args.device)\n",
    "\n",
    "        for c in range(num_classes):\n",
    "            print('class c = %d: %d real images' % (c, len(indices_class[c])))\n",
    "\n",
    "        def get_images(c, n):\n",
    "            idx_shuffle = np.random.permutation(indices_class[c])[:n]\n",
    "            return images_all[idx_shuffle]\n",
    "\n",
    "        for ch in range(channel):\n",
    "            print('real images channel %d, mean = %.4f, std = %.4f' % (ch, torch.mean(images_all[:, ch]), torch.std(images_all[:, ch])))\n",
    "\n",
    "        ''' Initialiser les donnes synthtiques '''\n",
    "        image_syn = torch.randn(size=(num_classes * args.ipc, channel, im_size[0], im_size[1]), dtype=torch.float, requires_grad=True, device=args.device)\n",
    "        label_syn = torch.tensor([np.ones(args.ipc) * i for i in range(num_classes)], dtype=torch.long, requires_grad=False, device=args.device).view(-1)\n",
    "\n",
    "        if args.init == 'real':\n",
    "            print('initialize synthetic data from random real images')\n",
    "            for c in range(num_classes):\n",
    "                image_syn.data[c * args.ipc:(c + 1) * args.ipc] = get_images(c, args.ipc).detach().data\n",
    "        else:\n",
    "            print('initialize synthetic data from random noise')\n",
    "\n",
    "        ''' Entranement '''\n",
    "        optimizer_img = torch.optim.SGD([image_syn, ], lr=args.lr_img, momentum=0.5)\n",
    "        optimizer_img.zero_grad()\n",
    "        print('%s training begins' % get_time())\n",
    "\n",
    "        for it in range(args.Iteration + 1):\n",
    "            ''' valuer les donnes synthtiques '''\n",
    "            if it in eval_it_pool:\n",
    "                for model_eval in model_eval_pool:\n",
    "                    print('-------------------------\\nEvaluation\\nmodel_train = %s, model_eval = %s, iteration = %d' % (args.model, model_eval, it))\n",
    "\n",
    "                    print('DSA augmentation strategy: \\n', args.dsa_strategy)\n",
    "                    print('DSA augmentation parameters: \\n', args.dsa_param.__dict__)\n",
    "\n",
    "                    accs = []\n",
    "                    for it_eval in range(args.num_eval):\n",
    "                        net_eval = get_network(model_eval, channel, num_classes, im_size).to(args.device)\n",
    "                        image_syn_eval, label_syn_eval = copy.deepcopy(image_syn.detach()), copy.deepcopy(label_syn.detach())\n",
    "                        _, acc_train, acc_test = evaluate_synset(it_eval, net_eval, image_syn_eval, label_syn_eval, testloader, args)\n",
    "                        accs.append(acc_test)\n",
    "                    print('Evaluate %d random %s, mean = %.4f std = %.4f\\n-------------------------' % (len(accs), model_eval, np.mean(accs), np.std(accs)))\n",
    "\n",
    "                    if it == args.Iteration:\n",
    "                        accs_all_exps[model_eval] += accs\n",
    "\n",
    "                ''' Visualiser et enregistrer '''\n",
    "                save_name = os.path.join(args.save_path, 'vis_%s_%s_%s_%dipc_exp%d_iter%d.png' % (args.method, args.dataset, args.model, args.ipc, exp, it))\n",
    "                image_syn_vis = copy.deepcopy(image_syn.detach().cpu())\n",
    "                for ch in range(channel):\n",
    "                    image_syn_vis[:, ch] = image_syn_vis[:, ch] * std[ch] + mean[ch]\n",
    "                image_syn_vis[image_syn_vis < 0] = 0.0\n",
    "                image_syn_vis[image_syn_vis > 1] = 1.0\n",
    "                save_image(image_syn_vis, save_name, nrow=args.ipc)\n",
    "\n",
    "            ''' Entraner les donnes synthtiques '''\n",
    "            net = get_network(args.model, channel, num_classes, im_size).to(args.device)\n",
    "            net.train()\n",
    "            for param in list(net.parameters()):\n",
    "                param.requires_grad = False\n",
    "\n",
    "            embed = net.module.embed if torch.cuda.device_count() > 1 else net.embed\n",
    "\n",
    "            loss_avg = 0\n",
    "\n",
    "            ''' Mettre  jour les donnes synthtiques '''\n",
    "            if 'BN' not in args.model:\n",
    "                loss = torch.tensor(0.0).to(args.device)\n",
    "                for c in range(num_classes):\n",
    "                    img_real = get_images(c, args.batch_real)\n",
    "                    img_syn = image_syn[c * args.ipc:(c + 1) * args.ipc].reshape((args.ipc, channel, im_size[0], im_size[1]))\n",
    "\n",
    "                    if args.dsa:\n",
    "                        seed = int(time.time() * 1000) % 100000\n",
    "                        img_real = DiffAugment(img_real, args.dsa_strategy, seed=seed, param=args.dsa_param)\n",
    "                        img_syn = DiffAugment(img_syn, args.dsa_strategy, seed=seed, param=args.dsa_param)\n",
    "\n",
    "                    output_real = embed(img_real).detach()\n",
    "                    output_syn = embed(img_syn)\n",
    "\n",
    "                    loss += torch.sum((torch.mean(output_real, dim=0) - torch.mean(output_syn, dim=0)) ** 2)\n",
    "\n",
    "            else:\n",
    "                images_real_all = []\n",
    "                images_syn_all = []\n",
    "                loss = torch.tensor(0.0).to(args.device)\n",
    "                for c in range(num_classes):\n",
    "                    img_real = get_images(c, args.batch_real)\n",
    "                    img_syn = image_syn[c * args.ipc:(c + 1) * args.ipc].reshape((args.ipc, channel, im_size[0], im_size[1]))\n",
    "\n",
    "                    if args.dsa:\n",
    "                        seed = int(time.time() * 1000) % 100000\n",
    "                        img_real = DiffAugment(img_real, args.dsa_strategy, seed=seed, param=args.dsa_param)\n",
    "                        img_syn = DiffAugment(img_syn, args.dsa_strategy, seed=seed, param=args.dsa_param)\n",
    "\n",
    "                    images_real_all.append(img_real)\n",
    "                    images_syn_all.append(img_syn)\n",
    "\n",
    "                images_real_all = torch.cat(images_real_all, dim=0)\n",
    "                images_syn_all = torch.cat(images_syn_all, dim=0)\n",
    "\n",
    "                output_real = embed(images_real_all).detach()\n",
    "                output_syn = embed(images_syn_all)\n",
    "                loss += torch.sum((torch.mean(output_real, dim=0) - torch.mean(output_syn, dim=0)) ** 2)\n",
    "\n",
    "            optimizer_img.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer_img.step()\n",
    "            loss_avg += loss.item()\n",
    "\n",
    "            if it % 10 == 0:\n",
    "                loss_avg /= (num_classes * 10)\n",
    "                print('%s iter = %05d, loss = %.4f' % (get_time(), it, loss_avg))\n",
    "                loss_avg = 0\n",
    "\n",
    "        print('\\n==================== Final Results ====================\\n')\n",
    "        for key in model_eval_pool:\n",
    "            accs = accs_all_exps[key]\n",
    "            print('Run %d: %s: mean = %.4f std = %.4f' % (exp, key, np.mean(accs), np.std(accs)))\n",
    "\n",
    "        data_save.append([copy.deepcopy(image_syn.detach().cpu()), copy.deepcopy(label_syn.detach().cpu())])\n",
    "\n",
    "    torch.save({'data': data_save, 'accs_all_exps': accs_all_exps}, os.path.join(args.save_path, 'res_%s_%s_%s_%dipc.pt' % (args.method, args.dataset, args.model, args.ipc)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e92149-5291-4a78-a80b-2de3befe9c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cl with image distill DSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dfa3a90b-2083-4719-81a0-e4b50626374b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "\n",
      "==================================================================================\n",
      "method:  DSA\n",
      "=========================================\n",
      "seed:  0\n",
      "class_order:  [26, 86, 2, 55, 75, 93, 16, 73, 54, 95, 53, 92, 78, 13, 7, 30, 22, 24, 33, 8, 43, 62, 3, 71, 45, 48, 6, 99, 82, 76, 60, 80, 90, 68, 51, 27, 18, 56, 63, 74, 1, 61, 42, 41, 4, 15, 17, 40, 38, 5, 91, 59, 0, 34, 28, 50, 11, 35, 23, 52, 10, 31, 66, 57, 79, 85, 32, 84, 14, 89, 19, 29, 49, 97, 98, 69, 20, 94, 72, 77, 25, 37, 81, 46, 39, 65, 58, 12, 88, 70, 87, 36, 21, 83, 9, 96, 67, 64, 47, 44]\n",
      "augmentation strategy: \n",
      " color_crop_cutout_flip_scale_rotate\n",
      "augmentation parameters: \n",
      " {'aug_mode': 'S', 'prob_flip': 0.5, 'ratio_scale': 1.2, 'ratio_rotate': 15.0, 'ratio_crop_pad': 0.125, 'ratio_cutout': 0.5, 'brightness': 1.0, 'saturation': 2.0, 'contrast': 0.5}\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data\\\\metasets\\\\cl_data\\\\cl_res_DSA_CIFAR100_ConvNet_20ipc_10steps_seed0.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 149\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 149\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[40], line 77\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmethod \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDSA\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     76\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(args\u001b[38;5;241m.\u001b[39mdata_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetasets\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcl_data\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcl_res_DSA_CIFAR100_ConvNet_20ipc_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124msteps_seed\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39m(args\u001b[38;5;241m.\u001b[39msteps, seed_cl))\n\u001b[1;32m---> 77\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     78\u001b[0m     images_train_all \u001b[38;5;241m=\u001b[39m [data[step][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39msteps)]\n\u001b[0;32m     79\u001b[0m     labels_train_all \u001b[38;5;241m=\u001b[39m [data[step][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39msteps)]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DDP\\lib\\site-packages\\torch\\serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DDP\\lib\\site-packages\\torch\\serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DDP\\lib\\site-packages\\torch\\serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[1;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data\\\\metasets\\\\cl_data\\\\cl_res_DSA_CIFAR100_ConvNet_20ipc_10steps_seed0.pt'"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Parameter Processing')\n",
    "    parser.add_argument('--method', type=str, default='DSA', help='random/herding/DSA/DM')\n",
    "    parser.add_argument('--dataset', type=str, default='CIFAR100', help='dataset')\n",
    "    parser.add_argument('--model', type=str, default='ConvNet', help='model')\n",
    "    parser.add_argument('--ipc', type=int, default=20, help='image(s) per class')\n",
    "    parser.add_argument('--steps', type=int, default=10, help='5/10-step learning')\n",
    "    parser.add_argument('--num_eval', type=int, default=3, help='evaluation number')\n",
    "    parser.add_argument('--epoch_eval_train', type=int, default=1000, help='epochs to train a model with synthetic data')\n",
    "    parser.add_argument('--lr_net', type=float, default=0.01, help='learning rate for updating network parameters')\n",
    "    parser.add_argument('--batch_train', type=int, default=256, help='batch size for training networks')\n",
    "    parser.add_argument('--data_path', type=str, default='./data', help='dataset path')\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    args.dsa_param = ParamDiffAug()\n",
    "    args.dsa = True # augment images for all methods\n",
    "    args.dsa_strategy = 'color_crop_cutout_flip_scale_rotate' # for CIFAR10/100\n",
    "\n",
    "    if not os.path.exists(args.data_path):\n",
    "        os.mkdir(args.data_path)\n",
    "\n",
    "    channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = get_dataset(args.dataset, args.data_path)\n",
    "\n",
    "\n",
    "    ''' all training data '''\n",
    "    images_all = []\n",
    "    labels_all = []\n",
    "    indices_class = [[] for c in range(num_classes)]\n",
    "\n",
    "    images_all = [torch.unsqueeze(dst_train[i][0], dim=0) for i in range(len(dst_train))]\n",
    "    labels_all = [dst_train[i][1] for i in range(len(dst_train))]\n",
    "    for i, lab in enumerate(labels_all):\n",
    "        indices_class[lab].append(i)\n",
    "    images_all = torch.cat(images_all, dim=0).to(args.device)\n",
    "    labels_all = torch.tensor(labels_all, dtype=torch.long, device=args.device)\n",
    "\n",
    "    # for c in range(num_classes):\n",
    "    #     print('class c = %d: %d real images' % (c, len(indices_class[c])))\n",
    "\n",
    "    def get_images(c, n):  # get random n images from class c\n",
    "        idx_shuffle = np.random.permutation(indices_class[c])[:n]\n",
    "        return images_all[idx_shuffle]\n",
    "\n",
    "    print()\n",
    "    print('==================================================================================')\n",
    "    print('method: ', args.method)\n",
    "    results = np.zeros((args.steps, 5*args.num_eval))\n",
    "\n",
    "    for seed_cl in range(5):\n",
    "        num_classes_step = num_classes // args.steps\n",
    "        np.random.seed(seed_cl)\n",
    "        class_order = np.random.permutation(num_classes).tolist()\n",
    "        print('=========================================')\n",
    "        print('seed: ', seed_cl)\n",
    "        print('class_order: ', class_order)\n",
    "        print('augmentation strategy: \\n', args.dsa_strategy)\n",
    "        print('augmentation parameters: \\n', args.dsa_param.__dict__)\n",
    "\n",
    "        if args.method == 'random':\n",
    "            images_train_all = []\n",
    "            labels_train_all = []\n",
    "            for step in range(args.steps):\n",
    "                classes_current = class_order[step * num_classes_step: (step + 1) * num_classes_step]\n",
    "                images_train_all += [torch.cat([get_images(c, args.ipc) for c in classes_current], dim=0)]\n",
    "                labels_train_all += [torch.tensor([c for c in classes_current for i in range(args.ipc)], dtype=torch.long, device=args.device)]\n",
    "\n",
    "        elif args.method == 'herding':\n",
    "            fname = os.path.join(args.data_path, 'metasets', 'cl_data', 'cl_herding_CIFAR100_ConvNet_20ipc_%dsteps_seed%d.pt'%(args.steps, seed_cl))\n",
    "            data = torch.load(fname, map_location='cpu')['data']\n",
    "            images_train_all = [data[step][0] for step in range(args.steps)]\n",
    "            labels_train_all = [data[step][1] for step in range(args.steps)]\n",
    "            print('use data: ', fname)\n",
    "\n",
    "        elif args.method == 'DSA':\n",
    "            fname = os.path.join(args.data_path, 'metasets', 'cl_data', 'cl_res_DSA_CIFAR100_ConvNet_20ipc_%dsteps_seed%d.pt'%(args.steps, seed_cl))\n",
    "            data = torch.load(fname, map_location='cpu')['data']\n",
    "            images_train_all = [data[step][0] for step in range(args.steps)]\n",
    "            labels_train_all = [data[step][1] for step in range(args.steps)]\n",
    "            print('use data: ', fname)\n",
    "\n",
    "        elif args.method == 'DM':\n",
    "            fname = os.path.join(args.data_path, 'metasets', 'cl_data', 'cl_DM_CIFAR100_ConvNet_20ipc_%dsteps_seed%d.pt'%(args.steps, seed_cl))\n",
    "            data = torch.load(fname, map_location='cpu')['data']\n",
    "            images_train_all = [data[step][0] for step in range(args.steps)]\n",
    "            labels_train_all = [data[step][1] for step in range(args.steps)]\n",
    "            print('use data: ', fname)\n",
    "\n",
    "        else:\n",
    "            exit('unknown method: %s'%args.method)\n",
    "\n",
    "\n",
    "        for step in range(args.steps):\n",
    "            print('\\n-----------------------------\\nmethod %s seed %d step %d ' % (args.method, seed_cl, step))\n",
    "\n",
    "            classes_seen = class_order[: (step+1)*num_classes_step]\n",
    "            print('classes_seen: ', classes_seen)\n",
    "\n",
    "\n",
    "            ''' train data '''\n",
    "            images_train = torch.cat(images_train_all[:step+1], dim=0).to(args.device)\n",
    "            labels_train = torch.cat(labels_train_all[:step+1], dim=0).to(args.device)\n",
    "            print('train data size: ', images_train.shape)\n",
    "\n",
    "\n",
    "            ''' test data '''\n",
    "            images_test = []\n",
    "            labels_test = []\n",
    "            for i in range(len(dst_test)):\n",
    "                lab = int(dst_test[i][1])\n",
    "                if lab in classes_seen:\n",
    "                    images_test.append(torch.unsqueeze(dst_test[i][0], dim=0))\n",
    "                    labels_test.append(dst_test[i][1])\n",
    "\n",
    "            images_test = torch.cat(images_test, dim=0).to(args.device)\n",
    "            labels_test = torch.tensor(labels_test, dtype=torch.long, device=args.device)\n",
    "            dst_test_current = TensorDataset(images_test, labels_test)\n",
    "            testloader = torch.utils.data.DataLoader(dst_test_current, batch_size=256, shuffle=False, num_workers=0)\n",
    "\n",
    "            print('test set size: ', images_test.shape)\n",
    "\n",
    "\n",
    "            ''' train model on the newest memory '''\n",
    "            accs = []\n",
    "            for ep_eval in range(args.num_eval):\n",
    "                net_eval = get_network(args.model, channel, num_classes, im_size)\n",
    "                net_eval = net_eval.to(args.device)\n",
    "                img_syn_eval = copy.deepcopy(images_train.detach())\n",
    "                lab_syn_eval = copy.deepcopy(labels_train.detach())\n",
    "\n",
    "                _, acc_train, acc_test = evaluate_synset(ep_eval, net_eval, img_syn_eval, lab_syn_eval, testloader, args)\n",
    "                del net_eval, img_syn_eval, lab_syn_eval\n",
    "                gc.collect()  # to reduce memory cost\n",
    "                accs.append(acc_test)\n",
    "                results[step, seed_cl*args.num_eval + ep_eval] = acc_test\n",
    "            print('Evaluate %d random %s, mean = %.4f std = %.4f' % (len(accs), args.model, np.mean(accs), np.std(accs)))\n",
    "\n",
    "\n",
    "    results_str = ''\n",
    "    for step in range(args.steps):\n",
    "        results_str += '& %.1f$\\pm$%.1f  ' % (np.mean(results[step]) * 100, np.std(results[step]) * 100)\n",
    "    print('\\n\\n')\n",
    "    print('%d step learning %s perforamnce:'%(args.steps, args.method))\n",
    "    print(results_str)\n",
    "    print('Done')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c788fc13-bd9e-4216-be61-0dbbd15084ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: avalanche-lib==0.5 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (0.5.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from avalanche-lib==0.5) (4.11.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from avalanche-lib==0.5) (5.9.0)\n",
      "Requirement already satisfied: gputil in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from avalanche-lib==0.5) (1.4.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from avalanche-lib==0.5) (1.3.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from avalanche-lib==0.5) (3.7.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from avalanche-lib==0.5) (1.24.3)\n",
      "Requirement already satisfied: pytorchcv in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from avalanche-lib==0.5) (0.0.67)\n",
      "Requirement already satisfied: wandb in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from avalanche-lib==0.5) (0.17.5)\n",
      "Requirement already satisfied: tensorboard>=1.15 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from avalanche-lib==0.5) (2.14.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from avalanche-lib==0.5) (4.66.4)\n",
      "Requirement already satisfied: torch in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from avalanche-lib==0.5) (2.0.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from avalanche-lib==0.5) (0.15.0)\n",
      "Requirement already satisfied: torchmetrics in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from avalanche-lib==0.5) (1.4.0.post0)\n",
      "Requirement already satisfied: gdown in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from avalanche-lib==0.5) (5.2.0)\n",
      "Requirement already satisfied: qpsolvers[open_source_solvers] in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from avalanche-lib==0.5) (4.2.0)\n",
      "Requirement already satisfied: dill in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from avalanche-lib==0.5) (0.3.8)\n",
      "Requirement already satisfied: packaging in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from avalanche-lib==0.5) (23.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from tensorboard>=1.15->avalanche-lib==0.5) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from tensorboard>=1.15->avalanche-lib==0.5) (1.64.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from tensorboard>=1.15->avalanche-lib==0.5) (2.30.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from tensorboard>=1.15->avalanche-lib==0.5) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from tensorboard>=1.15->avalanche-lib==0.5) (3.6)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from tensorboard>=1.15->avalanche-lib==0.5) (5.27.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from tensorboard>=1.15->avalanche-lib==0.5) (2.32.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from tensorboard>=1.15->avalanche-lib==0.5) (59.5.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from tensorboard>=1.15->avalanche-lib==0.5) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from tensorboard>=1.15->avalanche-lib==0.5) (3.0.3)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from tensorboard>=1.15->avalanche-lib==0.5) (0.43.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from gdown->avalanche-lib==0.5) (4.12.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from gdown->avalanche-lib==0.5) (3.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from matplotlib->avalanche-lib==0.5) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from matplotlib->avalanche-lib==0.5) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from matplotlib->avalanche-lib==0.5) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from matplotlib->avalanche-lib==0.5) (1.4.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from matplotlib->avalanche-lib==0.5) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from matplotlib->avalanche-lib==0.5) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from matplotlib->avalanche-lib==0.5) (2.9.0.post0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from matplotlib->avalanche-lib==0.5) (6.1.1)\n",
      "Requirement already satisfied: daqp>=0.5.1 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from qpsolvers[open_source_solvers]->avalanche-lib==0.5) (0.5.1)\n",
      "Requirement already satisfied: ecos>=2.0.8 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from qpsolvers[open_source_solvers]->avalanche-lib==0.5) (2.0.14)\n",
      "Requirement already satisfied: osqp>=0.6.2 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from qpsolvers[open_source_solvers]->avalanche-lib==0.5) (0.6.7.post0)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from qpsolvers[open_source_solvers]->avalanche-lib==0.5) (1.10.1)\n",
      "Requirement already satisfied: scs>=3.2.0 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from qpsolvers[open_source_solvers]->avalanche-lib==0.5) (3.2.6)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from scikit-learn->avalanche-lib==0.5) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from scikit-learn->avalanche-lib==0.5) (3.5.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from torch->avalanche-lib==0.5) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from torch->avalanche-lib==0.5) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from torch->avalanche-lib==0.5) (3.1.4)\n",
      "Requirement already satisfied: lightning-utilities>=0.8.0 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from torchmetrics->avalanche-lib==0.5) (0.11.3.post0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from tqdm->avalanche-lib==0.5) (0.4.6)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from wandb->avalanche-lib==0.5) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from wandb->avalanche-lib==0.5) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from wandb->avalanche-lib==0.5) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from wandb->avalanche-lib==0.5) (3.10.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from wandb->avalanche-lib==0.5) (6.0.1)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from wandb->avalanche-lib==0.5) (2.7.1)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from wandb->avalanche-lib==0.5) (1.3.3)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb->avalanche-lib==0.5) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->avalanche-lib==0.5) (4.0.11)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->avalanche-lib==0.5) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->avalanche-lib==0.5) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard>=1.15->avalanche-lib==0.5) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard>=1.15->avalanche-lib==0.5) (2.0.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib->avalanche-lib==0.5) (3.17.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from markdown>=2.6.8->tensorboard>=1.15->avalanche-lib==0.5) (7.0.1)\n",
      "Requirement already satisfied: qdldl in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from osqp>=0.6.2->qpsolvers[open_source_solvers]->avalanche-lib==0.5) (0.1.7.post4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=1.15->avalanche-lib==0.5) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=1.15->avalanche-lib==0.5) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=1.15->avalanche-lib==0.5) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=1.15->avalanche-lib==0.5) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard>=1.15->avalanche-lib==0.5) (2.1.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from beautifulsoup4->gdown->avalanche-lib==0.5) (2.5)\n",
      "Requirement already satisfied: cvxopt>=1.2.6 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from qpsolvers[open_source_solvers]->avalanche-lib==0.5) (1.3.2)\n",
      "Requirement already satisfied: qpalm>=1.2.1 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from qpsolvers[open_source_solvers]->avalanche-lib==0.5) (1.2.3)\n",
      "Requirement already satisfied: highspy>=1.1.2.dev3 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from qpsolvers[open_source_solvers]->avalanche-lib==0.5) (1.7.2)\n",
      "Requirement already satisfied: quadprog>=0.1.11 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from qpsolvers[open_source_solvers]->avalanche-lib==0.5) (0.1.12)\n",
      "Requirement already satisfied: proxsuite>=0.2.9 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from qpsolvers[open_source_solvers]->avalanche-lib==0.5) (0.6.6)\n",
      "Requirement already satisfied: clarabel>=0.4.1 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from qpsolvers[open_source_solvers]->avalanche-lib==0.5) (0.9.0)\n",
      "Requirement already satisfied: piqp>=0.2.2 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from qpsolvers[open_source_solvers]->avalanche-lib==0.5) (0.4.1)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from requests[socks]->gdown->avalanche-lib==0.5) (1.7.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from sympy->torch->avalanche-lib==0.5) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->avalanche-lib==0.5) (5.0.1)\n",
      "Requirement already satisfied: cmeel in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from proxsuite>=0.2.9->qpsolvers[open_source_solvers]->avalanche-lib==0.5) (0.53.3)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.15->avalanche-lib==0.5) (0.6.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard>=1.15->avalanche-lib==0.5) (3.2.2)\n",
      "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in c:\\users\\ahmed\\anaconda3\\envs\\ddp\\lib\\site-packages (from cmeel->proxsuite>=0.2.9->qpsolvers[open_source_solvers]->avalanche-lib==0.5) (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: qpsolvers 4.2.0 does not provide the extra 'open-source-solvers'\n"
     ]
    }
   ],
   "source": [
    "pip install avalanche-lib==0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "324a481f-3f0d-44f0-8ba3-d9ef0fb75795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval_it_pool:  [0, 500, 1000]\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Num. examples processed: 50000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from avalanche.benchmarks.datasets import CIFAR10, CIFAR100\n",
    "from avalanche.benchmarks.utils import classification_dataset\n",
    "import avalanche.benchmarks.scenarios.dataset_scenario\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from avalanche.training.supervised import (\n",
    "    Cumulative, Naive, ICaRL, LwF, EWC, GenerativeReplay, JointTraining, CWRStar\n",
    ")\n",
    "\n",
    "from avalanche.evaluation.metrics import (\n",
    "    Accuracy, TaskAwareAccuracy, accuracy_metrics, loss_metrics, forgetting_metrics,\n",
    "    cpu_usage_metrics, gpu_usage_metrics, MAC_metrics\n",
    ")\n",
    "from avalanche.logging import InteractiveLogger, WandBLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin, ReplayPlugin\n",
    "import avalanche.checkpointing.checkpoint\n",
    "\n",
    "import os\n",
    "import json\n",
    "parser = argparse.ArgumentParser(description='Parameter Processing')\n",
    "method='DSA'\n",
    "dataset='CIFAR100'\n",
    "model='ConvNet'\n",
    "ipc=1 # help='image(s) per class'\n",
    "eval_mode='S' # help='eval_mode'\n",
    "num_exp=5 # help='the number of experiments'\n",
    "num_eval=20 # help='the number of evaluating randomly initialized models')\n",
    "epoch_eval_train=300 # help='epochs to train a model with synthetic data')\n",
    "Iteration=1000 # help='training iterations')\n",
    "lr_img=0.1 # help='learning rate for updating synthetic images')\n",
    "lr_net=0.01 # help='learning rate for updating network parameters')\n",
    "batch_real=256 # help='batch size for real data')\n",
    "batch_train=256 # help='batch size for training networks')\n",
    "init='noise' # help='noise/real: initialize synthetic images from random noise or randomly sampled real images.')\n",
    "dsa_strategy='color_crop_cutout_flip_scale_rotate' # help='differentiable Siamese augmentation strategy')\n",
    "data_path='./data' # help='dataset path')\n",
    "save_path='result'# help='path to save results')\n",
    "dis_metric='ours' # help='distance metric')\n",
    "steps=10 # help='5/10-step learning')\n",
    "num_eval=3 # help='evaluation number')\n",
    "\n",
    "# Handle unrecognized arguments\n",
    "\n",
    "device =  'cpu'\n",
    "dsa = True # augment images for all methods\n",
    "dsa_strategy = 'color_crop_cutout_flip_scale_rotate' # for CIFAR10/100\n",
    "\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "eval_it_pool = np.arange(0, Iteration+1, 500).tolist() if eval_mode == 'S' or eval_mode == 'SS' else [args.Iteration]\n",
    "print('eval_it_pool: ', eval_it_pool)\n",
    "\n",
    "# Correctly loading CIFAR100 datasets\n",
    "# channel, im_size, num_classes, class_names, mean, std, dst_train, dst_test, testloader = get_dataset(CIFAR100, args.data_path)\n",
    "channel = 3\n",
    "im_size = (32, 32)\n",
    "num_classes = 100\n",
    "mean = [0.4914, 0.4822, 0.4465]\n",
    "std = [0.2023, 0.1994, 0.2010]\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
    "dst_train = datasets.CIFAR100('./data/cifar100', train=True, download=True, transform=transform) # no augmentation\n",
    "dst_test = datasets.CIFAR100('./data/cifar100', train=False, download=True, transform=transform)\n",
    "class_names = dst_train.classes\n",
    "\n",
    "model_eval_pool = get_eval_pool('S','ConvNet','ConvNet')\n",
    "\n",
    "# Iterate over CIFAR100 dataset\n",
    "for i, example in enumerate(dst_train):\n",
    "    pass\n",
    "print(\"Num. examples processed: {}\".format(i + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84316c23-dd18-41c1-aef7-bac2890fa6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import (\n",
    "    List,\n",
    "    Any,\n",
    "    Sequence,\n",
    "    Union,\n",
    "    Optional,\n",
    "    TypeVar,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    Tuple,\n",
    "    Mapping,\n",
    "    overload,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "da22acbc-d363-4fdd-9152-06864ab2f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avalanche.benchmarks.utils.utils import*\n",
    "from typing import TypeVar, SupportsInt, Sequence, Protocol\n",
    "from avalanche.benchmarks.utils.data import AvalancheDataset\n",
    "from avalanche.benchmarks.utils.data_attribute import *\n",
    "\n",
    "T_co = TypeVar(\"T_co\", covariant=True)\n",
    "TTargetType_co = TypeVar(\"TTargetType_co\", covariant=True)\n",
    "TTargetType = int\n",
    "\n",
    "class IDataset(Protocol[T_co]):\n",
    "    \"\"\"\n",
    "    Protocol definition of a Dataset.\n",
    "\n",
    "    Note: no __add__ method is defined.\n",
    "    \"\"\"\n",
    "\n",
    "    def __getitem__(self, index: int) -> T_co: ...\n",
    "\n",
    "    def __len__(self) -> int: ...\n",
    "        \n",
    "class IDatasetWithTargets(IDataset[T_co], Protocol[T_co, TTargetType_co]):\n",
    "    \"\"\"\n",
    "    Protocol definition of a Dataset that has a valid targets field.\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def targets(self) -> Sequence[TTargetType_co]:\n",
    "        \"\"\"\n",
    "        A sequence of elements describing the targets of each pattern.\n",
    "        \"\"\"\n",
    "        ...\n",
    "TClassificationDataset = TypeVar(\n",
    "    \"TClassificationDataset\", bound=\"ClassificationDataset\"\n",
    ")        \n",
    "class TaskAwareClassificationDataset(AvalancheDataset[T_co]):\n",
    "    @property\n",
    "    def task_pattern_indices(self) -> Dict[int, Sequence[int]]:\n",
    "        \"\"\"A dictionary mapping task ids to their sample indices.\"\"\"\n",
    "        return self.targets_task_labels.val_to_idx  # type: ignore\n",
    "\n",
    "    @property\n",
    "    def task_set(self: TClassificationDataset) -> TaskSet[TClassificationDataset]:\n",
    "        \"\"\"Returns the datasets's ``TaskSet``, which is a mapping <task-id,\n",
    "        task-dataset>.\"\"\"\n",
    "        return TaskSet(self)\n",
    "\n",
    "    def subset(self, indices):\n",
    "        data = super().subset(indices)\n",
    "        return data.with_transforms(self._flat_data._transform_groups.current_group)\n",
    "\n",
    "    def concat(self, other):\n",
    "        data = super().concat(other)\n",
    "        return data.with_transforms(self._flat_data._transform_groups.current_group)\n",
    "\n",
    "    def __hash__(self):\n",
    "        return id(self)                \n",
    "class TaskAwareSupervisedClassificationDataset(TaskAwareClassificationDataset[T_co]):\n",
    "    # TODO: remove? ClassificationDataset should have targets\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        assert \"targets\" in self._data_attributes, (\n",
    "            \"The supervised version of the ClassificationDataset requires \"\n",
    "            + \"the targets field\"\n",
    "        )\n",
    "        assert \"targets_task_labels\" in self._data_attributes, (\n",
    "            \"The supervised version of the ClassificationDataset requires \"\n",
    "            + \"the targets_task_labels field\"\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def targets(self) -> DataAttribute[TTargetType]:\n",
    "        return self._data_attributes[\"targets\"]\n",
    "\n",
    "    @property\n",
    "    def targets_task_labels(self) -> DataAttribute[int]:\n",
    "        return self._data_attributes[\"targets_task_labels\"]\n",
    "        \n",
    "\n",
    "class ISupportedClassificationDataset(IDatasetWithTargets[T_co, SupportsInt], Protocol):\n",
    "    \"\"\"\n",
    "    Protocol definition of a Dataset that has a valid targets field (like the\n",
    "    Datasets in the torchvision package) for classification.\n",
    "\n",
    "    For classification purposes, the targets field must be a sequence of ints.\n",
    "    describing the class label of each pattern.\n",
    "\n",
    "    This class however describes a targets field as a sequence of elements\n",
    "    that can be converted to `int`. The main reason for this choice is that\n",
    "    the targets field of some torchvision datasets is a Tensor. This means that\n",
    "    this protocol class supports both sequence of native ints and Tensor of ints\n",
    "    (or longs).\n",
    "\n",
    "    On the contrary, class :class:`IClassificationDataset` strictly\n",
    "    defines a `targets` field as sequence of native `int`s.\n",
    "    \"\"\"\n",
    "\n",
    "    @property\n",
    "    def targets(self) -> Sequence[SupportsInt]:\n",
    "        \"\"\"\n",
    "        A sequence of ints or a PyTorch Tensor or a NumPy ndarray describing the\n",
    "        label of each pattern contained in the dataset.\n",
    "        \"\"\"\n",
    "        ...\n",
    "        \n",
    "class TaskAwareSupervisedClassificationDataset(TaskAwareClassificationDataset[T_co]):\n",
    "    # TODO: remove? ClassificationDataset should have targets\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        assert \"targets\" in self._data_attributes, (\n",
    "            \"The supervised version of the ClassificationDataset requires \"\n",
    "            + \"the targets field\"\n",
    "        )\n",
    "        assert \"targets_task_labels\" in self._data_attributes, (\n",
    "            \"The supervised version of the ClassificationDataset requires \"\n",
    "            + \"the targets_task_labels field\"\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def targets(self) -> DataAttribute[TTargetType]:\n",
    "        return self._data_attributes[\"targets\"]\n",
    "\n",
    "    @property\n",
    "    def targets_task_labels(self) -> DataAttribute[int]:\n",
    "        return self._data_attributes[\"targets_task_labels\"]      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "73b2bc8f-6094-49b0-9ddd-54ac03bfd0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avalanche.benchmarks.utils.transform_groups import*\n",
    "def _as_taskaware_supervised_classification_dataset(\n",
    "    dataset,\n",
    "    *,\n",
    "    transform: Optional[XTransform] = None,\n",
    "    target_transform: Optional[YTransform] = None,\n",
    "    transform_groups: Optional[Mapping[str, TransformGroupDef]] = None,\n",
    "    initial_transform_group: Optional[str] = None,\n",
    "    task_labels: Optional[Union[int, Sequence[int]]] = None,\n",
    "    targets: Optional[Sequence[TTargetType]] = None,\n",
    "    collate_fn: Optional[Callable[[List], Any]] = None\n",
    ") -> TaskAwareSupervisedClassificationDataset:\n",
    "    if (\n",
    "        transform is not None\n",
    "        or target_transform is not None\n",
    "        or transform_groups is not None\n",
    "        or initial_transform_group is not None\n",
    "        or task_labels is not None\n",
    "        or targets is not None\n",
    "        or collate_fn is not None\n",
    "        or not isinstance(dataset, TaskAwareSupervisedClassificationDataset)\n",
    "    ):\n",
    "        result_dataset = _make_taskaware_classification_dataset(\n",
    "            dataset=dataset,\n",
    "            transform=transform,\n",
    "            target_transform=target_transform,\n",
    "            transform_groups=transform_groups,\n",
    "            initial_transform_group=initial_transform_group,\n",
    "            task_labels=task_labels,\n",
    "            targets=targets,\n",
    "            collate_fn=collate_fn,\n",
    "        )\n",
    "\n",
    "        if not isinstance(result_dataset, TaskAwareSupervisedClassificationDataset):\n",
    "            raise ValueError(\n",
    "                \"The given dataset does not have supervision fields \"\n",
    "                \"(targets, task_labels).\"\n",
    "            )\n",
    "\n",
    "        return result_dataset\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "94a5104a-29b0-478e-9157-e303b9b9f442",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self, num_classes=110):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.Conv2d(64, 64, kernel_size=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveMaxPool2d(1),\n",
    "            nn.Dropout(p=0.25),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(nn.Linear(64, num_classes))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7fb47aa-c97c-4b49-aa94-9ca0612d5191",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classes_seen:  [97, 92, 34, 33, 70, 14, 39, 50, 16, 96, 90, 18, 19, 73, 94, 28, 22, 65, 11, 79, 38, 27, 30, 47, 69, 21, 48, 13, 44, 63, 25, 42, 7, 80, 46, 49, 20, 81, 1, 4, 58, 72, 74, 55, 5, 43, 32, 95, 54, 51, 67, 31, 99, 26, 76, 52, 10, 86, 71, 62, 3, 6, 59, 57, 60, 77, 98, 82, 45, 87, 93, 0, 2, 75, 23, 78, 15, 64, 17, 84, 88, 91, 8, 53, 29, 40, 85, 56, 89, 83, 37, 35, 41, 9, 61, 66, 68, 12, 36, 24]\n",
      "use data:  C:\\Users\\ahmed\\OneDrive\\Documents\\GitHub\\DatasetCondensation\\cl_data\\cl_res_DSA_CIFAR100_ConvNet_20ipc_10steps_seed0.pt\n",
      "train data size:  torch.Size([2000, 3, 32, 32])\n",
      "test set size:  torch.Size([10000, 3, 32, 32])\n",
      "cifar100 distilled\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unsupported dataset: must have a valid targets field or has to be a Tensor Dataset with at least 2 Tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 225\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcifar100 distilled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 225\u001b[0m     benchmark_instance \u001b[38;5;241m=\u001b[39m \u001b[43mcifar100_distilled_benchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m     check_vision_benchmark(benchmark_instance)\n",
      "Cell \u001b[1;32mIn[37], line 213\u001b[0m, in \u001b[0;36mcifar100_distilled_benchmark\u001b[1;34m()\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcifar100_distilled_benchmark\u001b[39m():\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnc_benchmark\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_experiences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DDP\\lib\\site-packages\\avalanche\\benchmarks\\scenarios\\deprecated\\generators.py:250\u001b[0m, in \u001b[0;36mnc_benchmark\u001b[1;34m(train_dataset, test_dataset, n_experiences, task_labels, shuffle, seed, fixed_class_order, per_exp_classes, class_ids_from_zero_from_first_exp, class_ids_from_zero_in_each_exp, one_dataset_per_exp, train_transform, eval_transform, reproducibility_data)\u001b[0m\n\u001b[0;32m    248\u001b[0m         n_experiences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataset)\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 250\u001b[0m     seq_train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43m_as_taskaware_supervised_classification_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    253\u001b[0m     seq_test_dataset \u001b[38;5;241m=\u001b[39m _as_taskaware_supervised_classification_dataset(test_dataset)\n\u001b[0;32m    255\u001b[0m transform_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(train\u001b[38;5;241m=\u001b[39m(train_transform, \u001b[38;5;28;01mNone\u001b[39;00m), \u001b[38;5;28meval\u001b[39m\u001b[38;5;241m=\u001b[39m(eval_transform, \u001b[38;5;28;01mNone\u001b[39;00m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DDP\\lib\\site-packages\\avalanche\\benchmarks\\utils\\classification_dataset.py:1096\u001b[0m, in \u001b[0;36m_as_taskaware_supervised_classification_dataset\u001b[1;34m(dataset, transform, target_transform, transform_groups, initial_transform_group, task_labels, targets, collate_fn)\u001b[0m\n\u001b[0;32m   1075\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_as_taskaware_supervised_classification_dataset\u001b[39m(\n\u001b[0;32m   1076\u001b[0m     dataset,\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1084\u001b[0m     collate_fn: Optional[Callable[[List], Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TaskAwareSupervisedClassificationDataset:\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1087\u001b[0m         transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m target_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1094\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, TaskAwareSupervisedClassificationDataset)\n\u001b[0;32m   1095\u001b[0m     ):\n\u001b[1;32m-> 1096\u001b[0m         result_dataset \u001b[38;5;241m=\u001b[39m \u001b[43m_make_taskaware_classification_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtransform_groups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform_groups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1101\u001b[0m \u001b[43m            \u001b[49m\u001b[43minitial_transform_group\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_transform_group\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1102\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtask_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1103\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1104\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1105\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1107\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result_dataset, TaskAwareSupervisedClassificationDataset):\n\u001b[0;32m   1108\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1109\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe given dataset does not have supervision fields \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1110\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(targets, task_labels).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1111\u001b[0m             )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DDP\\lib\\site-packages\\avalanche\\benchmarks\\utils\\classification_dataset.py:306\u001b[0m, in \u001b[0;36m_make_taskaware_classification_dataset\u001b[1;34m(dataset, transform, target_transform, transform_groups, initial_transform_group, task_labels, targets, collate_fn)\u001b[0m\n\u001b[0;32m    297\u001b[0m is_supervised \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(dataset, TaskAwareSupervisedClassificationDataset)\n\u001b[0;32m    299\u001b[0m transform_gs \u001b[38;5;241m=\u001b[39m _init_transform_groups(\n\u001b[0;32m    300\u001b[0m     transform_groups,\n\u001b[0;32m    301\u001b[0m     transform,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    304\u001b[0m     dataset,\n\u001b[0;32m    305\u001b[0m )\n\u001b[1;32m--> 306\u001b[0m targets_data: Optional[DataAttribute[TTargetType]] \u001b[38;5;241m=\u001b[39m \u001b[43m_init_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    307\u001b[0m task_labels_data: Optional[DataAttribute[\u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m _init_task_labels(\n\u001b[0;32m    308\u001b[0m     dataset, task_labels\n\u001b[0;32m    309\u001b[0m )\n\u001b[0;32m    311\u001b[0m das: List[DataAttribute] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DDP\\lib\\site-packages\\avalanche\\benchmarks\\utils\\classification_dataset.py:361\u001b[0m, in \u001b[0;36m_init_targets\u001b[1;34m(dataset, targets, check_shape)\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    355\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid amount of target labels. It must be equal to the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    356\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumber of patterns in the dataset. Got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, expected \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    357\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(targets), \u001b[38;5;28mlen\u001b[39m(dataset))\n\u001b[0;32m    358\u001b[0m         )\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataAttribute(targets, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 361\u001b[0m targets \u001b[38;5;241m=\u001b[39m \u001b[43m_traverse_supported_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_select_targets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(targets, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DDP\\lib\\site-packages\\avalanche\\benchmarks\\utils\\utils.py:365\u001b[0m, in \u001b[0;36m_traverse_supported_dataset\u001b[1;34m(dataset, values_selector, indices)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m initial_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m initial_error\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find the needed data in the given dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DDP\\lib\\site-packages\\avalanche\\benchmarks\\utils\\utils.py:301\u001b[0m, in \u001b[0;36m_traverse_supported_dataset\u001b[1;34m(dataset, values_selector, indices)\u001b[0m\n\u001b[0;32m    299\u001b[0m initial_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    300\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 301\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mvalues_selector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DDP\\lib\\site-packages\\avalanche\\benchmarks\\utils\\classification_dataset.py:941\u001b[0m, in \u001b[0;36m_select_targets\u001b[1;34m(dataset, indices)\u001b[0m\n\u001b[0;32m    939\u001b[0m     found_targets \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mtensors[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 941\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    942\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported dataset: must have a valid targets field \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    943\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor has to be a Tensor Dataset with at least 2 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    944\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    945\u001b[0m     )\n\u001b[0;32m    947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    948\u001b[0m     found_targets \u001b[38;5;241m=\u001b[39m SubSequence(found_targets, indices\u001b[38;5;241m=\u001b[39mindices)\n",
      "\u001b[1;31mValueError\u001b[0m: Unsupported dataset: must have a valid targets field or has to be a Tensor Dataset with at least 2 Tensors"
     ]
    }
   ],
   "source": [
    "from typing import (\n",
    "    List,\n",
    "    Any,\n",
    "    Sequence,\n",
    "    Union,\n",
    "    Optional,\n",
    "    TypeVar,\n",
    "    Callable,\n",
    "    Dict,\n",
    "    Tuple,\n",
    "    Mapping,\n",
    "    overload,\n",
    ")\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Sequence, Optional, Union, Any\n",
    "from avalanche.benchmarks.utils.classification_dataset import *\n",
    "from torchvision import transforms\n",
    "from avalanche.benchmarks.utils.utils import (\n",
    "    TaskSet,\n",
    "    _count_unique,\n",
    "    find_common_transforms_group,\n",
    "    _init_task_labels,\n",
    "    _init_transform_groups,\n",
    "    _split_user_def_targets,\n",
    "    _split_user_def_task_label,\n",
    "    _traverse_supported_dataset,\n",
    ")\n",
    "\n",
    "from avalanche.benchmarks.classic.classic_benchmarks_utils import (\n",
    "    check_vision_benchmark,\n",
    ")\n",
    "\n",
    "from avalanche.benchmarks.datasets.external_datasets.cifar import (\n",
    "    get_cifar100_dataset,\n",
    "    get_cifar10_dataset,\n",
    ")\n",
    "\n",
    "def _concat_taskaware_classification_datasets_sequentially(\n",
    "    train_dataset_list: Sequence[ISupportedClassificationDataset],\n",
    "    test_dataset_list: Sequence[ISupportedClassificationDataset],\n",
    ") -> Tuple[\n",
    "    TaskAwareSupervisedClassificationDataset,\n",
    "    TaskAwareSupervisedClassificationDataset,\n",
    "    List[list],\n",
    "]:\n",
    "    \n",
    "    remapped_train_datasets: List[TaskAwareSupervisedClassificationDataset] = []\n",
    "    remapped_test_datasets: List[TaskAwareSupervisedClassificationDataset] = []\n",
    "    next_remapped_idx = 0\n",
    "\n",
    "    train_dataset_list_sup = list(\n",
    "        map(_as_taskaware_supervised_classification_dataset, train_dataset_list)\n",
    "    )\n",
    "    test_dataset_list_sup = list(\n",
    "        map(_as_taskaware_supervised_classification_dataset, test_dataset_list)\n",
    "    )\n",
    "    del train_dataset_list\n",
    "    del test_dataset_list\n",
    "\n",
    "    # Obtain the number of classes of each dataset\n",
    "    classes_per_dataset = [\n",
    "        _count_unique(\n",
    "            train_dataset_list_sup[dataset_idx].targets,\n",
    "            test_dataset_list_sup[dataset_idx].targets,\n",
    "        )\n",
    "        for dataset_idx in range(len(train_dataset_list_sup))\n",
    "    ]\n",
    "\n",
    "    new_class_ids_per_dataset = []\n",
    "    for dataset_idx in range(len(train_dataset_list_sup)):\n",
    "        # Get the train and test sets of the dataset\n",
    "        train_set = train_dataset_list_sup[dataset_idx]\n",
    "        test_set = test_dataset_list_sup[dataset_idx]\n",
    "\n",
    "        # Get the classes in the dataset\n",
    "        dataset_classes = set(map(int, train_set.targets))\n",
    "\n",
    "        # The class IDs for this dataset will be in range\n",
    "        # [n_classes_in_previous_datasets,\n",
    "        #       n_classes_in_previous_datasets + classes_in_this_dataset)\n",
    "        new_classes = list(\n",
    "            range(\n",
    "                next_remapped_idx,\n",
    "                next_remapped_idx + classes_per_dataset[dataset_idx],\n",
    "            )\n",
    "        )\n",
    "        new_class_ids_per_dataset.append(new_classes)\n",
    "\n",
    "        # AvalancheSubset is used to apply the class IDs transformation.\n",
    "        # Remember, the class_mapping parameter must be a list in which:\n",
    "        # new_class_id = class_mapping[original_class_id]\n",
    "        # Hence, a list of size equal to the maximum class index is created\n",
    "        # Only elements corresponding to the present classes are remapped\n",
    "        class_mapping = [-1] * (max(dataset_classes) + 1)\n",
    "        j = 0\n",
    "        for i in dataset_classes:\n",
    "            class_mapping[i] = new_classes[j]\n",
    "            j += 1\n",
    "\n",
    "        a = _taskaware_classification_subset(train_set, class_mapping=class_mapping)\n",
    "\n",
    "        # Create remapped datasets and append them to the final list\n",
    "        remapped_train_datasets.append(\n",
    "            _taskaware_classification_subset(train_set, class_mapping=class_mapping)\n",
    "        )\n",
    "        remapped_test_datasets.append(\n",
    "            _taskaware_classification_subset(test_set, class_mapping=class_mapping)\n",
    "        )\n",
    "        next_remapped_idx += classes_per_dataset[dataset_idx]\n",
    "\n",
    "    return (\n",
    "        _concat_taskaware_classification_datasets(remapped_train_datasets),\n",
    "        _concat_taskaware_classification_datasets(remapped_test_datasets),\n",
    "        new_class_ids_per_dataset,\n",
    "    )\n",
    "\n",
    "from avalanche.benchmarks import nc_benchmark, NCScenario\n",
    "\n",
    "_default_cifar100_train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2762)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "_default_cifar100_eval_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5071, 0.4865, 0.4409), (0.2673, 0.2564, 0.2762)),\n",
    "    ]\n",
    ")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "steps=10 # help='5/10-step learning')\n",
    "num_classes = 100\n",
    "num_classes_step = num_classes // steps\n",
    "class_order = np.random.permutation(num_classes).tolist()\n",
    "\n",
    "\n",
    "''' organize the real dataset '''\n",
    "images_all = []\n",
    "labels_all = []\n",
    "indices_class = [[] for c in range(num_classes)]\n",
    "\n",
    "images_all = [torch.unsqueeze(dst_train[i][0], dim=0) for i in range(len(dst_train))]\n",
    "labels_all = [dst_train[i][1] for i in range(len(dst_train))]\n",
    "for i, lab in enumerate(labels_all):\n",
    "    indices_class[lab].append(i)\n",
    "images_all = torch.cat(images_all, dim=0).to(device)\n",
    "labels_all = torch.tensor(labels_all, dtype=torch.long, device=device)\n",
    "\n",
    "\n",
    "# for c in range(num_classes):\n",
    "#     print('class c = %d: %d real images' % (c, len(indices_class[c])))\n",
    "\n",
    "\n",
    "def get_images(c, n):  # get random n images from class c\n",
    "    idx_shuffle = np.random.permutation(indices_class[c])[:n]\n",
    "    return images_all[idx_shuffle]\n",
    "\n",
    "data_path = r'C:\\Users\\ahmed\\OneDrive\\Documents\\GitHub\\DatasetCondensation\\cl_data'\n",
    "classes_seen = class_order[: (10)*num_classes_step]\n",
    "print('classes_seen: ', classes_seen)\n",
    "\n",
    "\n",
    "if method == 'DSA':\n",
    "    # fname = os.path.join(data_path, 'metasets', 'cl_data', 'cl_res_DSA_CIFAR100_ConvNet_20ipc_%dsteps_seed%d.pt'%(steps, 0))\n",
    "    fname = os.path.join(data_path,'cl_res_DSA_CIFAR100_ConvNet_20ipc_%dsteps_seed%d.pt'%(steps, 0))\n",
    "    data = torch.load(fname, map_location='cpu')['data']\n",
    "    images_train_all = [data[step][0] for step in range(steps)]\n",
    "    labels_train_all = [data[step][1] for step in range(steps)]\n",
    "    print('use data: ', fname)\n",
    "\n",
    "elif method == 'DM':\n",
    "    # fname = os.path.join(data_path, 'metasets', 'cl_data', 'cl_DM_CIFAR100_ConvNet_20ipc_%dsteps_seed%d.pt'%(args.steps, 0))\n",
    "    fname = os.path.join(data_path,'cl_DM_CIFAR100_ConvNet_20ipc_%dsteps_seed%d.pt'%(steps, 0))\n",
    "    data = torch.load(fname, map_location='cpu')['data']\n",
    "    images_train_all = [data[step][0] for step in range(steps)]\n",
    "    labels_train_all = [data[step][1] for step in range(steps)]\n",
    "    print('use data: ', fname)\n",
    "\n",
    "else:\n",
    "    exit('unknown method: %s'%method)\n",
    "\n",
    "\n",
    "''' train data '''\n",
    "images_train = torch.cat(images_train_all[:10], dim=0).to(device)\n",
    "labels_train = torch.cat(labels_train_all[:10], dim=0).to(device)\n",
    "print('train data size: ', images_train.shape)\n",
    "\n",
    "\n",
    "''' test data '''\n",
    "images_test = []\n",
    "labels_test = []\n",
    "for i in range(len(dst_test)):\n",
    "    lab = int(dst_test[i][1])\n",
    "    if lab in classes_seen:\n",
    "        images_test.append(torch.unsqueeze(dst_test[i][0], dim=0))\n",
    "        labels_test.append(dst_test[i][1])\n",
    "\n",
    "images_test = torch.cat(images_test, dim=0).to(device)\n",
    "labels_test = torch.tensor(labels_test, dtype=torch.long, device=device)\n",
    "dst_test_current = TensorDataset(images_test, labels_test)\n",
    "testloader = torch.utils.data.DataLoader(dst_test_current, batch_size=256, shuffle=False, num_workers=0)\n",
    "\n",
    "print('test set size: ', images_test.shape)\n",
    "\n",
    "\n",
    "def cifar100_distilled_benchmark():\n",
    "    return nc_benchmark(\n",
    "        train_dataset=images_train,\n",
    "        test_dataset=images_test,\n",
    "        n_experiences=steps,\n",
    "        task_labels=False\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    print(\"cifar100 distilled\")\n",
    "    benchmark_instance = cifar100_distilled_benchmark()\n",
    "    check_vision_benchmark(benchmark_instance)\n",
    "\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f76581-055f-40c4-938c-e5f17468f31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision.transforms import ToTensor\n",
    "from avalanche.benchmarks import nc_benchmark\n",
    "import numpy as np\n",
    "\n",
    "# Dfinition des arguments\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.method = 'DSA'\n",
    "        self.dataset = 'CIFAR100'\n",
    "        self.model = 'ConvNet'\n",
    "        self.ipc = 20\n",
    "        self.steps = 10  # Assurez-vous que 'steps' est dfini\n",
    "        self.epoch_eval_train = 300\n",
    "        self.lr_net = 0.01\n",
    "        self.batch_train = 256\n",
    "        self.data_path = r'C:\\Users\\ahmed\\OneDrive\\Documents\\GitHub\\DatasetCondensation\\cl_data'\n",
    "        self.save_path = './result'\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Initialisation de la variable device\n",
    "device = args.device\n",
    "\n",
    "# Charger les donnes\n",
    "def load_data():\n",
    "    if args.method == 'DSA':\n",
    "        fname = os.path.join(args.data_path, 'cl_res_DSA_CIFAR100_ConvNet_20ipc_%dsteps_seed%d.pt' % (args.steps, 0))\n",
    "    elif args.method == 'DM':\n",
    "        fname = os.path.join(args.data_path, 'cl_DM_CIFAR100_ConvNet_20ipc_%dsteps_seed%d.pt' % (args.steps, 0))\n",
    "    else:\n",
    "        raise ValueError(f'Unknown method: {args.method}')\n",
    "\n",
    "    # Vrifier l'existence du fichier\n",
    "    if not os.path.exists(fname):\n",
    "        raise FileNotFoundError(f\"File not found: {fname}\")\n",
    "    \n",
    "    # Charger le fichier\n",
    "    print(f\"Loading file from: {fname}\")\n",
    "    data = torch.load(fname, map_location='cpu')['data']\n",
    "    \n",
    "    images_train_all = [data[step][0] for step in range(args.steps)]\n",
    "    labels_train_all = [data[step][1] for step in range(args.steps)]\n",
    "    \n",
    "    return images_train_all, labels_train_all\n",
    "\n",
    "images_train_all, labels_train_all = load_data()\n",
    "\n",
    "# Concatner les donnes\n",
    "images_train = torch.cat(images_train_all, dim=0).to(device)\n",
    "labels_train = torch.cat(labels_train_all, dim=0).to(device)\n",
    "print('Train data size: ', images_train.shape)\n",
    "\n",
    "# Charger et prparer les donnes de test\n",
    "dst_test = CIFAR100(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "# Prparer les donnes de test\n",
    "images_test = []\n",
    "labels_test = []\n",
    "for i in range(len(dst_test)):\n",
    "    img, lab = dst_test[i]\n",
    "    if lab in set(labels_train.cpu().numpy()):  # Si l'tiquette de test fait partie des classes d'entranement\n",
    "        images_test.append(img)\n",
    "        labels_test.append(lab)\n",
    "\n",
    "images_test = torch.stack(images_test).to(device)\n",
    "labels_test = torch.tensor(labels_test, dtype=torch.long, device=device)\n",
    "print('Test data size: ', images_test.shape)\n",
    "\n",
    "# Crer les TensorDatasets pour l'entranement et les tests\n",
    "train_dataset = TensorDataset(images_train, labels_train)\n",
    "test_dataset = TensorDataset(images_test, labels_test)\n",
    "\n",
    "# Fonction pour crer le benchmark\n",
    "def cifar100_distilled_benchmark(steps):\n",
    "    return nc_benchmark(\n",
    "        train_dataset=train_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        n_experiences=steps,\n",
    "        task_labels=False\n",
    "    )\n",
    "\n",
    "# Code principal\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"CIFAR100 distilled\")\n",
    "    benchmark_instance = cifar100_distilled_benchmark(args.steps)\n",
    "    # Suite de votre code pour entraner et valuer le modle...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "19773d60-29fb-488b-95b4-55769ce9ab89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda:0\n",
      "Mapping cuda:0 to cuda:0\n",
      "[InteractiveLogger] Resuming from checkpoint. Current time is 2024-08-04 14:34:41 +0100\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'train_stream'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 113\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;66;03m# Parse known arguments and ignore the rest\u001b[39;00m\n\u001b[0;32m    112\u001b[0m     args, _ \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_known_args(sys\u001b[38;5;241m.\u001b[39margv)\n\u001b[1;32m--> 113\u001b[0m \u001b[43mmain_with_checkpointing\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[55], line 82\u001b[0m, in \u001b[0;36mmain_with_checkpointing\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     79\u001b[0m strategy, initial_exp \u001b[38;5;241m=\u001b[39m maybe_load_checkpoint(strategy, fname)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# STEP 3: USE THE \"initial_exp\" to resume training\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_exp \u001b[38;5;129;01min\u001b[39;00m \u001b[43mbenchmark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_stream\u001b[49m[initial_exp:]:\n\u001b[0;32m     83\u001b[0m     strategy\u001b[38;5;241m.\u001b[39mtrain(train_exp, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, persistent_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     84\u001b[0m     strategy\u001b[38;5;241m.\u001b[39meval(benchmark\u001b[38;5;241m.\u001b[39mtest_stream, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'train_stream'"
     ]
    }
   ],
   "source": [
    "#At each experience, train model with data from all previous experiences\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from avalanche.benchmarks.utils import classification_dataset\n",
    "import avalanche.benchmarks.scenarios.dataset_scenario\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from avalanche.training.supervised import (\n",
    "    Cumulative, Naive, ICaRL, LwF, EWC, GenerativeReplay, JointTraining, CWRStar\n",
    ")\n",
    "\n",
    "from avalanche.evaluation.metrics import (\n",
    "    Accuracy, TaskAwareAccuracy, accuracy_metrics, loss_metrics, forgetting_metrics,\n",
    "    cpu_usage_metrics, gpu_usage_metrics, MAC_metrics\n",
    ")\n",
    "from avalanche.logging import InteractiveLogger, WandBLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin, ReplayPlugin\n",
    "from avalanche.checkpointing import maybe_load_checkpoint, save_checkpoint\n",
    "from avalanche.training.determinism.rng_manager import RNGManager\n",
    "\n",
    "def main_with_checkpointing(args):\n",
    "    # STEP 1: SET THE RANDOM SEEDS to guarantee reproducibility\n",
    "    RNGManager.set_random_seeds(1234)\n",
    "\n",
    "    # Nothing new here...\n",
    "    device = torch.device(\n",
    "        f\"cuda:{args.cuda}\" if torch.cuda.is_available() and args.cuda >= 0 else \"cpu\"\n",
    "    )\n",
    "    print(\"Using device\", device)\n",
    "\n",
    "    # CL Benchmark Creation (as usual)\n",
    "    benchmark = cifar100_distilled_benchmark(args.steps)  # Pass steps as an argument\n",
    "    model = SimpleCNN(num_classes=100)\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    # Create the evaluation plugin (as usual)\n",
    "    evaluation_plugin = EvaluationPlugin(\n",
    "        accuracy_metrics(experience=True, stream=True), loggers=[InteractiveLogger()]\n",
    "    )\n",
    "\n",
    "    # choose some metrics and evaluation method\n",
    "    interactive_logger = InteractiveLogger()\n",
    "    eval_plugin = EvaluationPlugin(\n",
    "        accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "        loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "        loggers=[interactive_logger],\n",
    "    )\n",
    "\n",
    "    # Create the strategy using Cumulative\n",
    "    strategy = Cumulative(\n",
    "        model=model,                # Ensure your model is capable of handling the increased number of classes\n",
    "        optimizer=optimizer,        # Optimizer, e.g., Adam or SGD\n",
    "        criterion=criterion,        # Loss function, e.g., CrossEntropyLoss\n",
    "        train_mb_size=64,           # Batch size; adjust based on your hardware and model requirements\n",
    "        train_epochs=30,            # Increased number of epochs to ensure adequate training\n",
    "        eval_mb_size=64,            # Evaluation batch size\n",
    "        device=device,              # Device, e.g., 'cuda' or 'cpu'\n",
    "        evaluator=eval_plugin # Evaluation plugin or metric, e.g., accuracy\n",
    "    )\n",
    "\n",
    "    # STEP 2: TRY TO LOAD THE LAST CHECKPOINT\n",
    "    # if the checkpoint exists, load it into the newly created strategy\n",
    "    # the method also loads the experience counter, so we know where to\n",
    "    # resume training\n",
    "    fname = \"./checkpoint/Cumulative.pkl\"  # name of the checkpoint file\n",
    "    os.makedirs(os.path.dirname(fname), exist_ok=True)  # Ensure the checkpoint directory exists\n",
    "    strategy, initial_exp = maybe_load_checkpoint(strategy, fname)\n",
    "\n",
    "    # STEP 3: USE THE \"initial_exp\" to resume training\n",
    "    for train_exp in benchmark.train_stream[initial_exp:]:\n",
    "        strategy.train(train_exp, num_workers=4, persistent_workers=True)\n",
    "        strategy.eval(benchmark.test_stream, num_workers=4)\n",
    "\n",
    "        # STEP 4: SAVE the checkpoint after training on each experience.\n",
    "        save_checkpoint(strategy, fname)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if 'ipykernel_launcher' in sys.argv[0]:\n",
    "        # Running in a Jupyter environment\n",
    "        class Args:\n",
    "            cuda = 0\n",
    "            steps = 10  # Dfinir une valeur par dfaut pour steps\n",
    "        args = Args()\n",
    "    else:\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument(\n",
    "            \"--cuda\",\n",
    "            type=int,\n",
    "            default=0,\n",
    "            help=\"Select zero-indexed cuda device. -1 to use CPU.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--steps\",\n",
    "            type=int,\n",
    "            default=10,\n",
    "            help=\"Number of steps for the benchmark.\",\n",
    "        )\n",
    "        # Parse known arguments and ignore the rest\n",
    "        args, _ = parser.parse_known_args(sys.argv)\n",
    "    main_with_checkpointing(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa24159c-786c-4bff-9a36-776bda4dc047",
   "metadata": {},
   "source": [
    "# cl with dataset distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7319673-d86f-4386-9acd-a887034c3a2e",
   "metadata": {},
   "source": [
    "## Cumulative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e6753-ab28-4d7d-97c2-3e8dd420c5df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from avalanche.benchmarks.utils import classification_dataset\n",
    "\n",
    "\n",
    "import avalanche.benchmarks.scenarios.dataset_scenario\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.training.supervised import Cumulative\n",
    "from avalanche.evaluation.metrics import accuracy_metrics, loss_metrics\n",
    "from avalanche.logging import InteractiveLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.checkpointing import maybe_load_checkpoint, save_checkpoint\n",
    "from avalanche.training.determinism.rng_manager import RNGManager\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import TensorDataset\n",
    "from avalanche.benchmarks import nc_benchmark\n",
    "\n",
    "# Dfinition des arguments\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.method = 'DSA'\n",
    "        self.dataset = 'CIFAR100'\n",
    "        self.model = 'ConvNet'\n",
    "        self.ipc = 20\n",
    "        self.steps = 10  # Assurez-vous que 'steps' est dfini\n",
    "        self.epoch_eval_train = 300\n",
    "        self.lr_net = 0.01\n",
    "        self.batch_train = 256\n",
    "        self.data_path = r'C:\\Users\\ahmed\\OneDrive\\Documents\\GitHub\\DatasetCondensation\\cl_data'\n",
    "        self.save_path = './result'\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Initialisation de la variable device\n",
    "device = args.device\n",
    "device = 'cpu'\n",
    "# Charger les donnes\n",
    "def load_data():\n",
    "    if args.method == 'DSA':\n",
    "        fname = os.path.join(args.data_path, 'cl_res_DSA_CIFAR100_ConvNet_20ipc_%dsteps_seed%d.pt' % (args.steps, 0))\n",
    "    elif args.method == 'DM':\n",
    "        fname = os.path.join(args.data_path, 'cl_DM_CIFAR100_ConvNet_20ipc_%dsteps_seed%d.pt' % (args.steps, 0))\n",
    "    else:\n",
    "        raise ValueError(f'Unknown method: {args.method}')\n",
    "\n",
    "    # Vrifier l'existence du fichier\n",
    "    if not os.path.exists(fname):\n",
    "        raise FileNotFoundError(f\"File not found: {fname}\")\n",
    "    \n",
    "    # Charger le fichier\n",
    "    print(f\"Loading file from: {fname}\")\n",
    "    data = torch.load(fname, map_location='cpu')['data']\n",
    "    \n",
    "    images_train_all = [data[step][0] for step in range(args.steps)]\n",
    "    labels_train_all = [data[step][1] for step in range(args.steps)]\n",
    "    \n",
    "    return images_train_all, labels_train_all\n",
    "\n",
    "images_train_all, labels_train_all = load_data()\n",
    "\n",
    "# Concatner les donnes\n",
    "images_train = torch.cat(images_train_all, dim=0).to(device)\n",
    "labels_train = torch.cat(labels_train_all, dim=0).to(device)\n",
    "print('Train data size: ', images_train.shape)\n",
    "\n",
    "# Charger et prparer les donnes de test\n",
    "dst_test = CIFAR100(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "# Prparer les donnes de test\n",
    "images_test = []\n",
    "labels_test = []\n",
    "for i in range(len(dst_test)):\n",
    "    img, lab = dst_test[i]\n",
    "    if lab in set(labels_train.cpu().numpy()):  # Si l'tiquette de test fait partie des classes d'entranement\n",
    "        images_test.append(img)\n",
    "        labels_test.append(lab)\n",
    "\n",
    "images_test = torch.stack(images_test).to(device)\n",
    "labels_test = torch.tensor(labels_test, dtype=torch.long, device=device)\n",
    "print('Test data size: ', images_test.shape)\n",
    "\n",
    "# Crer les TensorDatasets pour l'entranement et les tests\n",
    "train_dataset = TensorDataset(images_train, labels_train)\n",
    "test_dataset = TensorDataset(images_test, labels_test)\n",
    "\n",
    "# Dfinir un modle simple de CNN\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class SimpleCNN64(nn.Module):\n",
    "    #p=0.05\n",
    "\n",
    "    def __init__(self, num_classes=100):\n",
    "        super(SimpleCNN64, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p=0.05),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p=0.05),\n",
    "            nn.Conv2d(64, 64, kernel_size=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveMaxPool2d(1),\n",
    "            nn.Dropout(p=0.05),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(nn.Linear(64, num_classes))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Fonction pour crer le benchmark\n",
    "def cifar100_distilled_benchmark(steps):\n",
    "    return nc_benchmark(\n",
    "        train_dataset=train_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        n_experiences=steps,\n",
    "        task_labels=False\n",
    "    )\n",
    "\n",
    "def main_with_checkpointing(args):\n",
    "    # STEP 1: SET THE RANDOM SEEDS to guarantee reproducibility\n",
    "    RNGManager.set_random_seeds(1234)\n",
    "\n",
    "    # Nothing new here...\n",
    "    device = torch.device(\n",
    "        f\"cuda:{args.cuda}\" if torch.cuda.is_available() and args.cuda >= 0 else \"cpu\"\n",
    "    )\n",
    "    print(\"Using device\", device)\n",
    "\n",
    "    # CL Benchmark Creation (as usual)\n",
    "    benchmark = cifar100_distilled_benchmark(args.steps)  # Pass steps as an argument\n",
    "    model = SimpleCNN64(num_classes=100)\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    # Create the evaluation plugin (as usual)\n",
    "    evaluation_plugin = EvaluationPlugin(\n",
    "        accuracy_metrics(experience=True, stream=True), loggers=[InteractiveLogger()]\n",
    "    )\n",
    "\n",
    "    # choose some metrics and evaluation method\n",
    "    interactive_logger = InteractiveLogger()\n",
    "    eval_plugin = EvaluationPlugin(\n",
    "        accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "        loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "        loggers=[interactive_logger],\n",
    "    )\n",
    "\n",
    "    # Create the strategy using Cumulative\n",
    "    strategy = Cumulative(\n",
    "        model=model,                # Ensure your model is capable of handling the increased number of classes\n",
    "        optimizer=optimizer,        # Optimizer, e.g., Adam or SGD\n",
    "        criterion=criterion,        # Loss function, e.g., CrossEntropyLoss\n",
    "        train_mb_size=400,           # Batch size; adjust based on your hardware and model requirements\n",
    "        train_epochs=30,            # Increased number of epochs to ensure adequate training\n",
    "        eval_mb_size=2000,            # Evaluation batch size\n",
    "        device=device,              # Device, e.g., 'cuda' or 'cpu'\n",
    "        evaluator=eval_plugin # Evaluation plugin or metric, e.g., accuracy\n",
    "    )\n",
    "\n",
    "    # STEP 2: TRY TO LOAD THE LAST CHECKPOINT\n",
    "    # if the checkpoint exists, load it into the newly created strategy\n",
    "    # the method also loads the experience counter, so we know where to\n",
    "    # resume training\n",
    "    fname = \"./checkpoint/Cumulative.pkl\"  # name of the checkpoint file\n",
    "    os.makedirs(os.path.dirname(fname), exist_ok=True)  # Ensure the checkpoint directory exists\n",
    "    #strategy, initial_exp = maybe_load_checkpoint(strategy, fname)\n",
    "\n",
    "\n",
    "    initial_exp=0\n",
    "    # STEP 3: USE THE \"initial_exp\" to resume training\n",
    "    for train_exp in benchmark.train_stream[initial_exp:]:\n",
    "        strategy.train(train_exp, num_workers=4, persistent_workers=True)\n",
    "        strategy.eval(benchmark.test_stream, num_workers=4)\n",
    "\n",
    "        # STEP 4: SAVE the checkpoint after training on each experience.\n",
    "        save_checkpoint(strategy, fname)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if 'ipykernel_launcher' in sys.argv[0]:\n",
    "        # Running in a Jupyter environment\n",
    "        class Args:\n",
    "            cuda = 0\n",
    "            steps = 10  # Dfinir une valeur par dfaut pour steps\n",
    "        args = Args()\n",
    "    else:\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument(\n",
    "            \"--cuda\",\n",
    "            type=int,\n",
    "            default=0,\n",
    "            help=\"Select zero-indexed cuda device. -1 to use CPU.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--steps\",\n",
    "            type=int,\n",
    "            default=10,\n",
    "            help=\"Number of steps for the benchmark.\",\n",
    "        )\n",
    "        # Parse known arguments and ignore the rest\n",
    "        args, _ = parser.parse_known_args(sys.argv)\n",
    "    main_with_checkpointing(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255e856e-776a-4edd-b112-ab9e725be7bd",
   "metadata": {},
   "source": [
    "## naive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bc4c2e7-f909-4e95-8c5b-951c166fc1db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file from: C:\\Users\\ahmed\\OneDrive\\Documents\\GitHub\\DatasetCondensation\\cl_data\\cl_res_DSA_CIFAR100_ConvNet_20ipc_10steps_seed0.pt\n",
      "Train data size:  torch.Size([2000, 3, 32, 32])\n",
      "Files already downloaded and verified\n",
      "Test data size:  torch.Size([10000, 3, 32, 32])\n",
      "Using device cuda:0\n",
      "-- >> Start of training phase << --\n",
      "100%|| 4/4 [00:07<00:00,  1.93s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5677\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4292\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0850\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 23.53it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.9040\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.2662\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1550\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 45.10it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.5793\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.5072\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1600\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 135.21it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 5.5326\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3173\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1600\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 119.62it/s]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.7508\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.8841\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1900\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 102.06it/s]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.6884\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.5355\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1500\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 110.30it/s]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.4793\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.8908\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1750\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 136.00it/s]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.1201\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.4594\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 105.94it/s]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.5886\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2558\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2300\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 112.10it/s]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.2581\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.8717\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2450\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 120.57it/s]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.5442\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.7402\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2750\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 107.14it/s]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1603\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3658\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2300\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 87.39it/s]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1370\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9157\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2700\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 109.54it/s]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.0122\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.7790\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3200\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 111.39it/s]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.9967\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2075\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2550\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 89.77it/s]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.8247\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9753\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2950\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 112.70it/s]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.8688\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.3228\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3050\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 112.01it/s]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.6963\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.4369\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3800\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 95.24it/s]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.5604\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9972\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.4300\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 117.38it/s]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.4491\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.1243\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.4650\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 111.86it/s]\n",
      "Epoch 20 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.6713\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.7472\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3900\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.7500\n",
      "100%|| 4/4 [00:00<00:00, 114.98it/s]\n",
      "Epoch 21 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.3830\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.2526\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.5000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.6250\n",
      "100%|| 4/4 [00:00<00:00, 102.33it/s]\n",
      "Epoch 22 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.3015\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.3357\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.4800\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.5000\n",
      "100%|| 4/4 [00:00<00:00, 118.02it/s]\n",
      "Epoch 23 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.3188\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.7730\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.4750\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.5000\n",
      "100%|| 4/4 [00:00<00:00, 120.74it/s]\n",
      "Epoch 24 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.2164\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.1341\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.5200\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.5000\n",
      "100%|| 4/4 [00:00<00:00, 115.71it/s]\n",
      "Epoch 25 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.1647\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.8189\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.6100\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.7500\n",
      "100%|| 4/4 [00:00<00:00, 117.00it/s]\n",
      "Epoch 26 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.1160\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.6144\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.6200\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.7500\n",
      "100%|| 4/4 [00:00<00:00, 109.49it/s]\n",
      "Epoch 27 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9628\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.0366\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.6600\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.6250\n",
      "100%|| 4/4 [00:00<00:00, 115.07it/s]\n",
      "Epoch 28 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9292\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.1819\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.6650\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.5000\n",
      "100%|| 4/4 [00:00<00:00, 113.51it/s]\n",
      "Epoch 29 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.0775\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.8047\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.6150\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.7500\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.14it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 2.3709\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.2310\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.12it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 6.0368\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.12it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 6.1464\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.15it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 6.0711\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.12it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 6.1169\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.00it/s]\n",
      "> Eval on experience 5 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp005 = 6.1774\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp005 = 0.0000\n",
      "-- Starting eval on experience 6 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.13it/s]\n",
      "> Eval on experience 6 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp006 = 6.1350\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp006 = 0.0000\n",
      "-- Starting eval on experience 7 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.24it/s]\n",
      "> Eval on experience 7 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp007 = 5.8715\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp007 = 0.0000\n",
      "-- Starting eval on experience 8 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.27it/s]\n",
      "> Eval on experience 8 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp008 = 6.0241\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp008 = 0.0000\n",
      "-- Starting eval on experience 9 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.13it/s]\n",
      "> Eval on experience 9 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp009 = 5.9850\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp009 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 5.6935\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.0231\n",
      "-- >> Start of training phase << --\n",
      "100%|| 4/4 [00:04<00:00,  1.11s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 9.0938\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.8754\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 23.99it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5217\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4631\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0050\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 123.01it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3920\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.9541\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0850\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 100.52it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.3894\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.0498\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1650\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 115.29it/s]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.9346\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 12.9367\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1050\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 140.56it/s]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.9830\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.8492\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1350\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 110.97it/s]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.7271\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.7184\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1250\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 130.81it/s]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.3788\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.0445\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1400\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 124.95it/s]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.2075\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.9843\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0950\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 133.14it/s]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.9333\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.7727\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1450\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 139.20it/s]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.6212\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.5697\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1750\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 135.21it/s]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.4445\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.1446\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1650\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 134.88it/s]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1999\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2387\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1750\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 128.95it/s]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.2709\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.7220\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 121.91it/s]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1749\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3193\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2200\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 129.53it/s]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1473\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2613\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 130.60it/s]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.9892\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9512\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2550\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 115.30it/s]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.9134\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3670\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2650\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 115.41it/s]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.8669\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9182\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2600\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 112.80it/s]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.8474\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.7181\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2350\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 110.93it/s]\n",
      "Epoch 20 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.8205\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.0274\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2950\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 120.27it/s]\n",
      "Epoch 21 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.8427\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.3531\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3400\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.7500\n",
      "100%|| 4/4 [00:00<00:00, 138.87it/s]\n",
      "Epoch 22 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.6952\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.6846\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3750\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 108.86it/s]\n",
      "Epoch 23 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.6644\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.8250\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3400\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 117.61it/s]\n",
      "Epoch 24 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.7401\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.1775\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3750\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.5000\n",
      "100%|| 4/4 [00:00<00:00, 113.48it/s]\n",
      "Epoch 25 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.7389\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.8643\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3300\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 127.10it/s]\n",
      "Epoch 26 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.5891\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.1148\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3850\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.6250\n",
      "100%|| 4/4 [00:00<00:00, 122.81it/s]\n",
      "Epoch 27 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.5306\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.5565\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.4000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 112.47it/s]\n",
      "Epoch 28 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.5142\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.9704\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.4650\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.6250\n",
      "100%|| 4/4 [00:00<00:00, 115.82it/s]\n",
      "Epoch 29 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.5371\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.3232\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.4000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.6250\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.04it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 5.1405\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.14it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 3.0066\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.1260\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.12it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 5.2676\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.11it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 5.3766\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.11it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 5.2891\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.08it/s]\n",
      "> Eval on experience 5 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp005 = 5.4317\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp005 = 0.0000\n",
      "-- Starting eval on experience 6 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.04it/s]\n",
      "> Eval on experience 6 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp006 = 5.3347\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp006 = 0.0000\n",
      "-- Starting eval on experience 7 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.16it/s]\n",
      "> Eval on experience 7 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp007 = 5.3490\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp007 = 0.0000\n",
      "-- Starting eval on experience 8 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.04it/s]\n",
      "> Eval on experience 8 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp008 = 5.4046\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp008 = 0.0000\n",
      "-- Starting eval on experience 9 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.15it/s]\n",
      "> Eval on experience 9 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp009 = 5.4102\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp009 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 5.1011\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.0126\n",
      "-- >> Start of training phase << --\n",
      "100%|| 4/4 [00:04<00:00,  1.11s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 7.2342\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6581\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 24.28it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6183\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5917\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 38.75it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5796\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5471\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 115.99it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4863\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3629\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 134.77it/s]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.9947\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.1821\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0600\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 134.87it/s]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.7541\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.3250\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1250\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 115.57it/s]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 6.6561\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6182\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0700\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 139.01it/s]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3209\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2881\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0600\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 130.37it/s]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.1306\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4898\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1050\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 130.68it/s]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.9117\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.7919\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 154.00it/s]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.6514\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.0100\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 124.96it/s]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.2581\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.1743\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1100\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 115.79it/s]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.8044\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.0642\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1250\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 140.48it/s]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.6467\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.6120\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1550\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 128.33it/s]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.4825\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3951\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1900\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 137.07it/s]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3346\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3592\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1350\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 137.40it/s]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.2940\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.6058\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2050\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 143.60it/s]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1162\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9585\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2200\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 124.57it/s]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1359\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.5988\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2650\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 135.90it/s]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1002\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.1492\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2650\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 127.95it/s]\n",
      "Epoch 20 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.0817\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.8373\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2700\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 138.16it/s]\n",
      "Epoch 21 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.9955\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9770\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 132.20it/s]\n",
      "Epoch 22 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.8701\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.6898\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 139.91it/s]\n",
      "Epoch 23 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.8637\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.0286\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3200\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 137.37it/s]\n",
      "Epoch 24 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.7622\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.7958\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3850\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 127.16it/s]\n",
      "Epoch 25 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.7958\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.0211\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3600\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 149.04it/s]\n",
      "Epoch 26 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.6929\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.0736\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.4200\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 138.72it/s]\n",
      "Epoch 27 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.8994\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.9606\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3300\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.6250\n",
      "100%|| 4/4 [00:00<00:00, 113.04it/s]\n",
      "Epoch 28 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.7606\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.7007\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 132.24it/s]\n",
      "Epoch 29 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.8913\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2383\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3450\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.29it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 4.9117\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.28it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 4.7525\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.29it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 3.1060\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.1570\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.18it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 5.2328\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.20it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 5.0816\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.18it/s]\n",
      "> Eval on experience 5 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp005 = 5.1127\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp005 = 0.0000\n",
      "-- Starting eval on experience 6 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.24it/s]\n",
      "> Eval on experience 6 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp006 = 5.0712\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp006 = 0.0000\n",
      "-- Starting eval on experience 7 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.11it/s]\n",
      "> Eval on experience 7 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp007 = 5.0426\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp007 = 0.0000\n",
      "-- Starting eval on experience 8 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  2.95it/s]\n",
      "> Eval on experience 8 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp008 = 5.1602\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp008 = 0.0000\n",
      "-- Starting eval on experience 9 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.16it/s]\n",
      "> Eval on experience 9 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp009 = 5.0619\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp009 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 4.8533\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.0157\n",
      "-- >> Start of training phase << --\n",
      "100%|| 4/4 [00:04<00:00,  1.10s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 6.4816\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.7684\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 23.87it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6472\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6253\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 51.21it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3756\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.0843\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 134.18it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.2738\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.4362\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1150\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 104.52it/s]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.2324\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.3343\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1700\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 128.43it/s]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.1117\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.5119\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1400\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 132.40it/s]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.2492\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.4981\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1400\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 138.74it/s]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.9833\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.7729\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1600\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 144.16it/s]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.7751\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.5547\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1850\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 126.06it/s]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.4445\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.0601\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2200\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 141.64it/s]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.4577\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.2678\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1800\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 143.73it/s]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.2498\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9920\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2350\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 125.49it/s]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1837\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.8946\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2200\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 144.41it/s]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1018\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9340\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2350\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 129.31it/s]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.0050\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.0762\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2650\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 133.66it/s]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.0179\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.0285\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2700\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 122.71it/s]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.9839\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.5659\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2350\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 116.82it/s]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1066\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3440\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2800\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 125.65it/s]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.0584\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.7782\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2650\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 122.88it/s]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.9544\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2808\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2650\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 124.94it/s]\n",
      "Epoch 20 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.8830\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3697\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3200\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 126.16it/s]\n",
      "Epoch 21 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.7662\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.8745\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3900\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 130.33it/s]\n",
      "Epoch 22 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.8507\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.1007\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3600\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 121.92it/s]\n",
      "Epoch 23 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.8452\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.0015\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2850\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 129.30it/s]\n",
      "Epoch 24 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.0009\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.7847\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3100\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 118.56it/s]\n",
      "Epoch 25 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1053\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.5485\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3450\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 108.98it/s]\n",
      "Epoch 26 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.8913\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.1935\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3900\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 112.03it/s]\n",
      "Epoch 27 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.6161\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.4735\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.4250\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.6250\n",
      "100%|| 4/4 [00:00<00:00, 131.02it/s]\n",
      "Epoch 28 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.6539\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.1911\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.4400\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.5000\n",
      "100%|| 4/4 [00:00<00:00, 103.08it/s]\n",
      "Epoch 29 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.6220\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9596\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.4200\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.22it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 5.9456\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.30it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 6.2486\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.24it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 6.1417\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.27it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 2.5714\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.1620\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.22it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 6.5080\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.26it/s]\n",
      "> Eval on experience 5 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp005 = 6.5284\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp005 = 0.0000\n",
      "-- Starting eval on experience 6 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.29it/s]\n",
      "> Eval on experience 6 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp006 = 6.2988\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp006 = 0.0000\n",
      "-- Starting eval on experience 7 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.25it/s]\n",
      "> Eval on experience 7 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp007 = 6.4277\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp007 = 0.0000\n",
      "-- Starting eval on experience 8 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.29it/s]\n",
      "> Eval on experience 8 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp008 = 6.7088\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp008 = 0.0000\n",
      "-- Starting eval on experience 9 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.26it/s]\n",
      "> Eval on experience 9 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp009 = 6.3796\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp009 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 5.9759\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.0162\n",
      "-- >> Start of training phase << --\n",
      "100%|| 4/4 [00:04<00:00,  1.06s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 6.7186\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.7805\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 46.83it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.7466\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6673\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 133.37it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5748\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4889\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 119.12it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2119\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.4334\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0550\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 106.80it/s]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.1975\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.4070\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1100\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 125.64it/s]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.8125\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.9213\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1450\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 126.24it/s]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.7891\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.5355\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 123.17it/s]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.7467\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.4075\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1300\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 134.49it/s]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.1784\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.7203\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1250\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 143.90it/s]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.7828\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.5917\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1050\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 125.32it/s]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.5642\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.1069\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1250\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 131.75it/s]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.5468\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.4315\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1300\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 121.92it/s]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3936\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.5036\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1050\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 140.58it/s]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.4341\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2096\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1950\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 147.57it/s]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.2648\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.1026\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2300\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 151.55it/s]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.2028\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2735\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1900\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 134.65it/s]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1643\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.8330\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1550\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 134.75it/s]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1372\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.4067\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2500\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 114.44it/s]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1113\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.4234\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2200\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 135.73it/s]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1277\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.1555\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2550\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 140.71it/s]\n",
      "Epoch 20 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1284\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9597\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2700\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 137.12it/s]\n",
      "Epoch 21 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.9490\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.0606\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2950\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 136.52it/s]\n",
      "Epoch 22 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.9023\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.7869\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3050\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.5000\n",
      "100%|| 4/4 [00:00<00:00, 130.26it/s]\n",
      "Epoch 23 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.9584\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.5798\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3300\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 121.27it/s]\n",
      "Epoch 24 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.8214\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9563\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3450\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 125.32it/s]\n",
      "Epoch 25 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.7796\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.6741\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3600\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.5000\n",
      "100%|| 4/4 [00:00<00:00, 131.45it/s]\n",
      "Epoch 26 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.7944\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.1833\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3550\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 134.26it/s]\n",
      "Epoch 27 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.7090\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.5131\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3500\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 133.89it/s]\n",
      "Epoch 28 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.6723\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2420\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3500\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 109.31it/s]\n",
      "Epoch 29 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.7346\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.6910\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3800\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.35it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 5.3164\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.30it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 5.0371\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.26it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 4.8455\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.28it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 5.2375\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.29it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 3.0970\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.1910\n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.26it/s]\n",
      "> Eval on experience 5 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp005 = 5.4672\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp005 = 0.0000\n",
      "-- Starting eval on experience 6 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.27it/s]\n",
      "> Eval on experience 6 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp006 = 5.3343\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp006 = 0.0000\n",
      "-- Starting eval on experience 7 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.30it/s]\n",
      "> Eval on experience 7 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp007 = 5.2585\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp007 = 0.0000\n",
      "-- Starting eval on experience 8 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.27it/s]\n",
      "> Eval on experience 8 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp008 = 5.3860\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp008 = 0.0000\n",
      "-- Starting eval on experience 9 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.27it/s]\n",
      "> Eval on experience 9 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp009 = 5.3971\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp009 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 5.0377\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.0191\n",
      "-- >> Start of training phase << --\n",
      "100%|| 4/4 [00:04<00:00,  1.07s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 8.0103\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.7710\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 23.70it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.7278\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6828\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 84.52it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.7007\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6916\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 160.75it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6724\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6536\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 108.87it/s]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6342\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6123\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 136.40it/s]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5811\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5593\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 137.09it/s]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4993\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5202\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0050\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 144.72it/s]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2940\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.9910\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0850\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 134.30it/s]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.4533\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.9595\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0950\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 134.63it/s]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.9086\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 5.4963\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1900\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 123.06it/s]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.8670\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.5350\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1250\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 138.92it/s]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.0785\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.2066\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1400\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 147.85it/s]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.0211\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3624\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1350\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 122.96it/s]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.9338\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.7422\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1350\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 134.78it/s]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.6139\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.7214\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1950\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 133.22it/s]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.6030\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.0471\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1550\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 135.05it/s]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.5472\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.6491\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1550\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 129.48it/s]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.4285\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.8320\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1200\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 143.91it/s]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.4091\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.0745\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1650\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 123.67it/s]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.6045\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.6468\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1650\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 143.24it/s]\n",
      "Epoch 20 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.4616\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3730\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1750\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 133.82it/s]\n",
      "Epoch 21 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3492\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9029\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.5000\n",
      "100%|| 4/4 [00:00<00:00, 149.44it/s]\n",
      "Epoch 22 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.2616\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.7982\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1950\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.5000\n",
      "100%|| 4/4 [00:00<00:00, 127.80it/s]\n",
      "Epoch 23 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.2042\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.9509\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2050\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 142.72it/s]\n",
      "Epoch 24 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1489\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.8172\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2300\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 133.71it/s]\n",
      "Epoch 25 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.2256\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3549\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2150\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 143.26it/s]\n",
      "Epoch 26 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1980\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2487\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2250\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 135.45it/s]\n",
      "Epoch 27 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1405\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3056\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2450\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 137.98it/s]\n",
      "Epoch 28 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1056\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9299\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2100\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 110.89it/s]\n",
      "Epoch 29 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.0902\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.1443\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2600\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.29it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 5.3619\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.23it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 5.3154\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.31it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 5.4492\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.26it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 5.5967\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.26it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 5.3001\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.30it/s]\n",
      "> Eval on experience 5 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp005 = 2.8275\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp005 = 0.1860\n",
      "-- Starting eval on experience 6 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.29it/s]\n",
      "> Eval on experience 6 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp006 = 5.5372\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp006 = 0.0000\n",
      "-- Starting eval on experience 7 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.21it/s]\n",
      "> Eval on experience 7 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp007 = 5.5081\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp007 = 0.0000\n",
      "-- Starting eval on experience 8 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.28it/s]\n",
      "> Eval on experience 8 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp008 = 5.7843\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp008 = 0.0000\n",
      "-- Starting eval on experience 9 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.28it/s]\n",
      "> Eval on experience 9 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp009 = 5.4725\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp009 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 5.2153\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.0186\n",
      "-- >> Start of training phase << --\n",
      "100%|| 4/4 [00:04<00:00,  1.04s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 6.6294\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.7866\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 26.25it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.7658\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.7721\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 136.16it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.7416\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.7332\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 132.50it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.7149\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.7018\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 129.87it/s]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6840\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6703\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 130.72it/s]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6505\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6398\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 111.98it/s]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6147\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5889\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 105.75it/s]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5780\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5619\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 130.55it/s]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5403\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5116\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 130.48it/s]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5016\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4828\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 134.41it/s]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4624\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4530\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 121.77it/s]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4208\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4119\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 162.03it/s]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3794\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3749\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0050\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 138.46it/s]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3336\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3137\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0200\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 135.23it/s]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2699\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2329\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1100\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 122.69it/s]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.0693\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.5931\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1450\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 126.40it/s]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.1880\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.6732\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1050\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 114.81it/s]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.1326\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 5.9499\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1900\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 135.69it/s]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.0504\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.1102\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1200\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 133.62it/s]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.7173\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.8330\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1150\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 123.38it/s]\n",
      "Epoch 20 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.3203\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.5232\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2300\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 126.24it/s]\n",
      "Epoch 21 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.2549\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.2361\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2250\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 130.93it/s]\n",
      "Epoch 22 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.8326\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.7628\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2150\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 130.34it/s]\n",
      "Epoch 23 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.6791\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.8343\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2250\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 136.52it/s]\n",
      "Epoch 24 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.4291\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.7135\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2500\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 125.17it/s]\n",
      "Epoch 25 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3221\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.4170\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2700\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 132.99it/s]\n",
      "Epoch 26 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3237\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.7426\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2050\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 135.94it/s]\n",
      "Epoch 27 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.2891\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.3414\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2500\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 122.47it/s]\n",
      "Epoch 28 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3326\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.6957\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3300\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.6250\n",
      "100%|| 4/4 [00:00<00:00, 131.13it/s]\n",
      "Epoch 29 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3433\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3818\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2300\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.37it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 6.3921\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.30it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 6.2573\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.29it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 6.4036\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.29it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 6.5352\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.31it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 6.4407\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.30it/s]\n",
      "> Eval on experience 5 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp005 = 6.2730\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp005 = 0.0000\n",
      "-- Starting eval on experience 6 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.24it/s]\n",
      "> Eval on experience 6 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp006 = 2.4564\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp006 = 0.1460\n",
      "-- Starting eval on experience 7 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.30it/s]\n",
      "> Eval on experience 7 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp007 = 6.6465\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp007 = 0.0000\n",
      "-- Starting eval on experience 8 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.28it/s]\n",
      "> Eval on experience 8 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp008 = 6.6507\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp008 = 0.0000\n",
      "-- Starting eval on experience 9 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.23it/s]\n",
      "> Eval on experience 9 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp009 = 6.5668\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp009 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 6.0622\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.0146\n",
      "-- >> Start of training phase << --\n",
      "100%|| 4/4 [00:04<00:00,  1.05s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 7.2848\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 5.2226\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 32.03it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.9311\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.8640\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 153.20it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.8555\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.8334\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 134.79it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.8242\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.8166\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 143.73it/s]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.7785\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.7393\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 117.73it/s]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.7236\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.7204\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 140.10it/s]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6744\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6603\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 143.63it/s]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6241\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5598\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 143.64it/s]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5649\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5065\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 141.54it/s]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4825\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4495\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 143.47it/s]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3374\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2516\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 123.36it/s]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.0467\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.7188\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 121.36it/s]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.3100\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.6805\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0700\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 144.00it/s]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.1072\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.0480\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0950\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 134.05it/s]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.2647\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.9352\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1650\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 139.92it/s]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.0948\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.1457\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1600\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 154.86it/s]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.9435\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.2495\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1700\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 128.66it/s]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.7977\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.5174\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1900\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 133.59it/s]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.4397\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.1148\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2100\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 154.64it/s]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.2474\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.0266\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 140.48it/s]\n",
      "Epoch 20 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1891\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.0907\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2550\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 133.99it/s]\n",
      "Epoch 21 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.0223\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.7513\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2800\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 136.74it/s]\n",
      "Epoch 22 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.0315\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9059\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2600\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 134.52it/s]\n",
      "Epoch 23 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.9492\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.8969\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2800\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 148.41it/s]\n",
      "Epoch 24 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.9113\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9044\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3200\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 134.52it/s]\n",
      "Epoch 25 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.9279\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.6266\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2700\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 154.00it/s]\n",
      "Epoch 26 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.8743\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9934\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3050\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 148.97it/s]\n",
      "Epoch 27 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.8622\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.5456\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3400\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.5000\n",
      "100%|| 4/4 [00:00<00:00, 125.82it/s]\n",
      "Epoch 28 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.7999\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.7549\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3250\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 128.77it/s]\n",
      "Epoch 29 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.8026\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9443\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3350\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.32it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 6.4460\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.26it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 6.3743\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.26it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 6.2444\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.07it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 6.3530\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.08it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 6.3529\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  2.96it/s]\n",
      "> Eval on experience 5 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp005 = 6.2907\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp005 = 0.0000\n",
      "-- Starting eval on experience 6 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  2.82it/s]\n",
      "> Eval on experience 6 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp006 = 5.3001\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp006 = 0.0000\n",
      "-- Starting eval on experience 7 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.61it/s]\n",
      "> Eval on experience 7 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp007 = 2.5029\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp007 = 0.1700\n",
      "-- Starting eval on experience 8 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  2.94it/s]\n",
      "> Eval on experience 8 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp008 = 6.7086\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp008 = 0.0000\n",
      "-- Starting eval on experience 9 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.07it/s]\n",
      "> Eval on experience 9 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp009 = 6.5914\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp009 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 5.9164\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.0170\n",
      "-- >> Start of training phase << --\n",
      "100%|| 4/4 [00:04<00:00,  1.12s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 7.3509\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.8978\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 26.11it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.8889\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.8718\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 108.15it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.8587\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.8359\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 143.38it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.8286\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.8186\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 144.27it/s]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.7960\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.7770\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 135.26it/s]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.7605\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.7486\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 133.88it/s]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.7230\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.7181\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 106.73it/s]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6839\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6603\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 127.33it/s]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6429\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6349\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 149.88it/s]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6000\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5649\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 142.33it/s]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5542\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5134\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 127.39it/s]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5054\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4893\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 126.23it/s]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4488\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4229\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 138.66it/s]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3790\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3271\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 140.27it/s]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2701\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2309\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0500\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 128.74it/s]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.9802\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.7461\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1050\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 145.32it/s]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.0125\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.7405\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1450\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 134.75it/s]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.7652\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.8971\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0950\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 134.80it/s]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 5.1506\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.0536\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1600\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 125.42it/s]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.0998\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.6794\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1600\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 125.17it/s]\n",
      "Epoch 20 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.2676\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.8563\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 139.84it/s]\n",
      "Epoch 21 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.9087\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.5112\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1050\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 139.59it/s]\n",
      "Epoch 22 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.0130\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.5631\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1200\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 150.04it/s]\n",
      "Epoch 23 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.9597\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.5267\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1350\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 133.92it/s]\n",
      "Epoch 24 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.8402\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.8423\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1300\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 135.36it/s]\n",
      "Epoch 25 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.0955\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.9729\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1400\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 128.91it/s]\n",
      "Epoch 26 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.8793\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.4723\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1450\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 120.02it/s]\n",
      "Epoch 27 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.6338\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2662\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1350\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 142.25it/s]\n",
      "Epoch 28 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.4014\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9845\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1250\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 128.83it/s]\n",
      "Epoch 29 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1794\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.0506\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2550\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.09it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 11.9234\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.18it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 12.1554\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.01it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 12.2491\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.17it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 12.2210\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.13it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 12.2736\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.13it/s]\n",
      "> Eval on experience 5 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp005 = 12.1503\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp005 = 0.0000\n",
      "-- Starting eval on experience 6 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.17it/s]\n",
      "> Eval on experience 6 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp006 = 12.0445\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp006 = 0.0000\n",
      "-- Starting eval on experience 7 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.15it/s]\n",
      "> Eval on experience 7 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp007 = 11.9445\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp007 = 0.0000\n",
      "-- Starting eval on experience 8 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.08it/s]\n",
      "> Eval on experience 8 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp008 = 3.2546\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp008 = 0.1000\n",
      "-- Starting eval on experience 9 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.14it/s]\n",
      "> Eval on experience 9 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp009 = 12.2938\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp009 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 11.2510\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.0100\n",
      "-- >> Start of training phase << --\n",
      "100%|| 4/4 [00:04<00:00,  1.08s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 9.4113\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 5.1300\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 24.57it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 5.0644\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 5.0424\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 46.59it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.9921\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.9493\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 133.97it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.9047\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.8315\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 120.71it/s]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.7437\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5907\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 144.19it/s]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4748\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4801\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 129.67it/s]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.5964\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.1741\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0750\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 143.80it/s]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.7751\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.5534\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1350\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 144.61it/s]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.7411\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.3645\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1100\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 151.56it/s]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.8435\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.9162\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0950\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 155.98it/s]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.5764\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.5952\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1500\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 127.01it/s]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.5463\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.9754\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1450\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 143.91it/s]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.6714\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.6089\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1400\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 133.56it/s]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.6106\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.1448\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1450\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 128.73it/s]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.5122\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.9334\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1700\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 117.31it/s]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.4920\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.1452\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1650\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 134.19it/s]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3350\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.1937\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1500\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 130.53it/s]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1515\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.8821\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2350\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.6250\n",
      "100%|| 4/4 [00:00<00:00, 111.56it/s]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1420\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.5702\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2200\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 138.37it/s]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1269\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9093\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.5000\n",
      "100%|| 4/4 [00:00<00:00, 144.17it/s]\n",
      "Epoch 20 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.9260\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.8901\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2800\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 155.06it/s]\n",
      "Epoch 21 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.8789\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.7356\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3350\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.5000\n",
      "100%|| 4/4 [00:00<00:00, 141.24it/s]\n",
      "Epoch 22 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.9979\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.5936\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3350\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 142.71it/s]\n",
      "Epoch 23 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.0384\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.4548\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3200\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 143.68it/s]\n",
      "Epoch 24 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.9729\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9849\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3650\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 137.42it/s]\n",
      "Epoch 25 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.8560\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.1538\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3600\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 144.71it/s]\n",
      "Epoch 26 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.8032\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.7310\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3400\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 149.17it/s]\n",
      "Epoch 27 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.8913\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.4506\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2950\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.5000\n",
      "100%|| 4/4 [00:00<00:00, 149.56it/s]\n",
      "Epoch 28 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.9422\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.4389\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3450\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.6250\n",
      "100%|| 4/4 [00:00<00:00, 159.94it/s]\n",
      "Epoch 29 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.6758\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.2781\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3750\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.5000\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.29it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 5.4253\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.26it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 5.2543\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.27it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 5.3913\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.27it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 5.6185\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.20it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 5.5163\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  3.18it/s]\n",
      "> Eval on experience 5 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp005 = 5.3883\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp005 = 0.0000\n",
      "-- Starting eval on experience 6 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.25it/s]\n",
      "> Eval on experience 6 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp006 = 5.1368\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp006 = 0.0000\n",
      "-- Starting eval on experience 7 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.21it/s]\n",
      "> Eval on experience 7 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp007 = 5.2487\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp007 = 0.0000\n",
      "-- Starting eval on experience 8 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.28it/s]\n",
      "> Eval on experience 8 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp008 = 5.0774\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp008 = 0.0000\n",
      "-- Starting eval on experience 9 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:04<00:00,  3.27it/s]\n",
      "> Eval on experience 9 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp009 = 2.8580\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp009 = 0.1560\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 5.0915\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.0156\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from avalanche.benchmarks.utils import classification_dataset\n",
    "\n",
    "from avalanche.training.supervised import (\n",
    "    Cumulative, Naive, ICaRL, LwF, EWC, GenerativeReplay, JointTraining, CWRStar\n",
    ")\n",
    "import avalanche.benchmarks.scenarios.dataset_scenario\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.training.supervised import Cumulative\n",
    "from avalanche.evaluation.metrics import accuracy_metrics, loss_metrics\n",
    "from avalanche.logging import InteractiveLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.checkpointing import maybe_load_checkpoint, save_checkpoint\n",
    "from avalanche.training.determinism.rng_manager import RNGManager\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import TensorDataset\n",
    "from avalanche.benchmarks import nc_benchmark\n",
    "\n",
    "# Dfinition des arguments\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.method = 'DSA'\n",
    "        self.dataset = 'CIFAR100'\n",
    "        self.model = 'ConvNet'\n",
    "        self.ipc = 20\n",
    "        self.steps = 10  # Assurez-vous que 'steps' est dfini\n",
    "        self.epoch_eval_train = 300\n",
    "        self.lr_net = 0.01\n",
    "        self.batch_train = 256\n",
    "        self.data_path = r'C:\\Users\\ahmed\\OneDrive\\Documents\\GitHub\\DatasetCondensation\\cl_data'\n",
    "        self.save_path = './result'\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Initialisation de la variable device\n",
    "device = args.device\n",
    "device = 'cpu'\n",
    "# Charger les donnes\n",
    "def load_data():\n",
    "    if args.method == 'DSA':\n",
    "        fname = os.path.join(args.data_path, 'cl_res_DSA_CIFAR100_ConvNet_20ipc_%dsteps_seed%d.pt' % (args.steps, 0))\n",
    "    elif args.method == 'DM':\n",
    "        fname = os.path.join(args.data_path, 'cl_DM_CIFAR100_ConvNet_20ipc_%dsteps_seed%d.pt' % (args.steps, 0))\n",
    "    else:\n",
    "        raise ValueError(f'Unknown method: {args.method}')\n",
    "\n",
    "    # Vrifier l'existence du fichier\n",
    "    if not os.path.exists(fname):\n",
    "        raise FileNotFoundError(f\"File not found: {fname}\")\n",
    "    \n",
    "    # Charger le fichier\n",
    "    print(f\"Loading file from: {fname}\")\n",
    "    data = torch.load(fname, map_location='cpu')['data']\n",
    "    \n",
    "    images_train_all = [data[step][0] for step in range(args.steps)]\n",
    "    labels_train_all = [data[step][1] for step in range(args.steps)]\n",
    "    \n",
    "    return images_train_all, labels_train_all\n",
    "\n",
    "images_train_all, labels_train_all = load_data()\n",
    "\n",
    "# Concatner les donnes\n",
    "images_train = torch.cat(images_train_all, dim=0).to(device)\n",
    "labels_train = torch.cat(labels_train_all, dim=0).to(device)\n",
    "print('Train data size: ', images_train.shape)\n",
    "\n",
    "# Charger et prparer les donnes de test\n",
    "dst_test = CIFAR100(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "# Prparer les donnes de test\n",
    "images_test = []\n",
    "labels_test = []\n",
    "for i in range(len(dst_test)):\n",
    "    img, lab = dst_test[i]\n",
    "    if lab in set(labels_train.cpu().numpy()):  # Si l'tiquette de test fait partie des classes d'entranement\n",
    "        images_test.append(img)\n",
    "        labels_test.append(lab)\n",
    "\n",
    "images_test = torch.stack(images_test).to(device)\n",
    "labels_test = torch.tensor(labels_test, dtype=torch.long, device=device)\n",
    "print('Test data size: ', images_test.shape)\n",
    "\n",
    "# Crer les TensorDatasets pour l'entranement et les tests\n",
    "train_dataset = TensorDataset(images_train, labels_train)\n",
    "test_dataset = TensorDataset(images_test, labels_test)\n",
    "\n",
    "# Dfinir un modle simple de CNN\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Fonction pour crer le benchmark\n",
    "def cifar100_distilled_benchmark(steps):\n",
    "    return nc_benchmark(\n",
    "        train_dataset=train_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        n_experiences=steps,\n",
    "        task_labels=False\n",
    "    )\n",
    "\n",
    "def main_with_checkpointing(args):\n",
    "    # STEP 1: SET THE RANDOM SEEDS to guarantee reproducibility\n",
    "    RNGManager.set_random_seeds(1234)\n",
    "\n",
    "    # Nothing new here...\n",
    "    device = torch.device(\n",
    "        f\"cuda:{args.cuda}\" if torch.cuda.is_available() and args.cuda >= 0 else \"cpu\"\n",
    "    )\n",
    "    print(\"Using device\", device)\n",
    "\n",
    "    # CL Benchmark Creation (as usual)\n",
    "    benchmark = cifar100_distilled_benchmark(args.steps)  # Pass steps as an argument\n",
    "    model = SimpleCNN(num_classes=100)\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    # Create the evaluation plugin (as usual)\n",
    "    evaluation_plugin = EvaluationPlugin(\n",
    "        accuracy_metrics(experience=True, stream=True), loggers=[InteractiveLogger()]\n",
    "    )\n",
    "    \n",
    "\n",
    "    # choose some metrics and evaluation method\n",
    "    interactive_logger = InteractiveLogger()\n",
    "    eval_plugin = EvaluationPlugin(\n",
    "        accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "        loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "        loggers=[interactive_logger],\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Create the strategy (as usual)\n",
    "    strategy = Naive(\n",
    "        model=model,                # Ensure your model is capable of handling the increased number of classes\n",
    "        optimizer=optimizer,        # Optimizer, e.g., Adam or SGD\n",
    "        criterion=criterion,        # Loss function, e.g., CrossEntropyLoss\n",
    "        train_mb_size=64,           # Batch size; adjust based on your hardware and model requirements\n",
    "        train_epochs=30,            # Increased number of epochs to ensure adequate training\n",
    "        eval_mb_size=64,            # Evaluation batch size\n",
    "        device=device,              # Device, e.g., 'cuda' or 'cpu'\n",
    "        evaluator=eval_plugin # Evaluation plugin or metric, e.g., accuracy\n",
    "    )\n",
    "\n",
    "    \n",
    "    # STEP 2: TRY TO LOAD THE LAST CHECKPOINT\n",
    "    # if the checkpoint exists, load it into the newly created strategy\n",
    "    # the method also loads the experience counter, so we know where to\n",
    "    # resume training\n",
    "    fname = \"./checkpoint/Naive1.pkl\"  # name of the checkpoint file\n",
    "    os.makedirs(os.path.dirname(fname), exist_ok=True)  # Ensure the checkpoint directory exists\n",
    "    #strategy, initial_exp = maybe_load_checkpoint(strategy, fname)\n",
    "\n",
    "    initial_exp=0\n",
    "    # STEP 3: USE THE \"initial_exp\" to resume training\n",
    "    for train_exp in benchmark.train_stream[initial_exp:]:\n",
    "        strategy.train(train_exp, num_workers=4, persistent_workers=True)\n",
    "        strategy.eval(benchmark.test_stream, num_workers=4)\n",
    "\n",
    "        # STEP 4: SAVE the checkpoint after training on each experience.\n",
    "        save_checkpoint(strategy, fname)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if 'ipykernel_launcher' in sys.argv[0]:\n",
    "        # Running in a Jupyter environment\n",
    "        class Args:\n",
    "            cuda = 0\n",
    "            steps = 10  # Dfinir une valeur par dfaut pour steps\n",
    "        args = Args()\n",
    "    else:\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument(\n",
    "            \"--cuda\",\n",
    "            type=int,\n",
    "            default=0,\n",
    "            help=\"Select zero-indexed cuda device. -1 to use CPU.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--steps\",\n",
    "            type=int,\n",
    "            default=10,\n",
    "            help=\"Number of steps for the benchmark.\",\n",
    "        )\n",
    "        # Parse known arguments and ignore the rest\n",
    "        args, _ = parser.parse_known_args(sys.argv)\n",
    "    main_with_checkpointing(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d7be20-a5a6-4924-a352-6b8c3bdd7fab",
   "metadata": {},
   "source": [
    "## Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59907b87-e16e-4e4d-8ba3-1ad837fd6225",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file from: C:\\Users\\ahmed\\OneDrive\\Documents\\GitHub\\DatasetCondensation\\cl_data\\cl_res_DSA_CIFAR100_ConvNet_20ipc_10steps_seed0.pt\n",
      "Train data size:  torch.Size([2000, 3, 32, 32])\n",
      "Files already downloaded and verified\n",
      "Test data size:  torch.Size([10000, 3, 32, 32])\n",
      "Using device cuda:0\n",
      "-- >> Start of training phase << --\n",
      "100%|| 4/4 [00:05<00:00,  1.38s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5983\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5959\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0050\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 21.55it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5769\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5570\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0050\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 46.36it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5394\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5168\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0250\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 82.90it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4715\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4374\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0450\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 76.88it/s]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3502\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.1861\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0800\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 71.13it/s]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.9596\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.5325\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1150\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 70.72it/s]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.2012\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2094\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1600\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 84.83it/s]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.4422\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.2684\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0850\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 102.13it/s]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.0265\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.5627\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0800\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 70.21it/s]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.8998\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9582\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1350\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.5000\n",
      "100%|| 4/4 [00:00<00:00, 87.19it/s]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.7607\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.5539\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1100\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 94.30it/s]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.6578\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.9817\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0800\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 94.93it/s]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.6729\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.7497\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0800\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 86.98it/s]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.5035\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3196\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1500\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 91.04it/s]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.5268\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.5172\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0750\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 92.33it/s]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.4777\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.4151\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1050\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 95.50it/s]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.4209\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2906\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1350\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 99.47it/s] \n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.4794\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.6831\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0750\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 92.09it/s]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3816\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.4651\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1300\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 89.47it/s]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.4395\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2774\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1550\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 96.25it/s]\n",
      "Epoch 20 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3508\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.4247\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1450\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 96.66it/s]\n",
      "Epoch 21 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.2917\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3180\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1450\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 95.27it/s]\n",
      "Epoch 22 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.2260\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2209\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1250\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 82.79it/s]\n",
      "Epoch 23 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.2292\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.1918\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1650\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 94.81it/s] \n",
      "Epoch 24 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1488\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.6154\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1700\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 102.14it/s]\n",
      "Epoch 25 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1760\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.1588\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1300\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 103.31it/s]\n",
      "Epoch 26 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1465\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.2411\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1600\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 98.22it/s] \n",
      "Epoch 27 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1083\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9996\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2400\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 89.76it/s]\n",
      "Epoch 28 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1051\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.1925\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2050\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 98.76it/s] \n",
      "Epoch 29 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.0901\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.6023\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1800\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  2.82it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 3.0765\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.1020\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  2.83it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 5.0538\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.65it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 5.0500\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.58it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 5.0428\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.65it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 5.0003\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.55it/s]\n",
      "> Eval on experience 5 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp005 = 5.1045\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp005 = 0.0000\n",
      "-- Starting eval on experience 6 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:07<00:00,  2.28it/s]\n",
      "> Eval on experience 6 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp006 = 5.0663\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp006 = 0.0000\n",
      "-- Starting eval on experience 7 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:07<00:00,  2.08it/s]\n",
      "> Eval on experience 7 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp007 = 5.0223\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp007 = 0.0000\n",
      "-- Starting eval on experience 8 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:09<00:00,  1.72it/s]\n",
      "> Eval on experience 8 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp008 = 5.0737\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp008 = 0.0000\n",
      "-- Starting eval on experience 9 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:13<00:00,  1.22it/s]\n",
      "> Eval on experience 9 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp009 = 5.0563\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp009 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 4.8547\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.0102\n",
      "-- >> Start of training phase << --\n",
      "100%|| 4/4 [00:10<00:00,  2.69s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 5.0002\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.0598\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0975\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0625\n",
      "100%|| 4/4 [00:00<00:00, 35.75it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.1999\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2178\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0625\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 33.26it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2149\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.1752\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0525\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 33.13it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.1631\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.1004\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0375\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0625\n",
      "100%|| 4/4 [00:00<00:00, 32.75it/s]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.0405\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.9317\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0475\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 35.17it/s]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.9474\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.9951\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0450\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 30.27it/s]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.8716\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.8246\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0575\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0625\n",
      "100%|| 4/4 [00:00<00:00, 34.86it/s]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.8051\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.7595\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0425\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0625\n",
      "100%|| 4/4 [00:00<00:00, 33.16it/s]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.6864\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.6044\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0425\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 33.09it/s]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.5900\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.5792\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0775\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 34.23it/s]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.4358\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.2680\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0325\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 32.00it/s]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.2952\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.4023\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0325\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0625\n",
      "100%|| 4/4 [00:00<00:00, 35.05it/s]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.2292\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.1676\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0475\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0625\n",
      "100%|| 4/4 [00:00<00:00, 33.81it/s]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.1712\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.2287\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0425\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0625\n",
      "100%|| 4/4 [00:00<00:00, 33.18it/s]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.1648\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.0687\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0525\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 34.41it/s]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.1781\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.2785\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0450\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0625\n",
      "100%|| 4/4 [00:00<00:00, 48.36it/s]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.1217\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.1200\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0475\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0625\n",
      "100%|| 4/4 [00:00<00:00, 33.61it/s]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.0820\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.0942\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0575\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 34.31it/s]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.1133\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.1794\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0425\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 34.50it/s]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.1108\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.1506\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0475\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 32.20it/s]\n",
      "Epoch 20 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.1000\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.9659\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0375\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 33.11it/s]\n",
      "Epoch 21 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.0859\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.1656\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0650\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0625\n",
      "100%|| 4/4 [00:00<00:00, 31.64it/s]\n",
      "Epoch 22 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.0661\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.0897\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0625\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 35.04it/s]\n",
      "Epoch 23 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.0644\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.0157\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0475\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0625\n",
      "100%|| 4/4 [00:00<00:00, 33.17it/s]\n",
      "Epoch 24 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.0608\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.0691\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0500\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 33.80it/s]\n",
      "Epoch 25 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.0500\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.1565\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0525\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 31.91it/s]\n",
      "Epoch 26 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.0548\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.0107\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0575\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1875\n",
      "100%|| 4/4 [00:00<00:00, 35.07it/s]\n",
      "Epoch 27 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.0407\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.0677\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0725\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0625\n",
      "100%|| 4/4 [00:00<00:00, 36.16it/s]\n",
      "Epoch 28 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.0415\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.1254\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0725\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0625\n",
      "100%|| 4/4 [00:00<00:00, 36.16it/s]\n",
      "Epoch 29 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.0148\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.9396\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0750\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1875\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:10<00:00,  1.47it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 3.3049\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.1000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:08<00:00,  1.90it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 3.4175\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:07<00:00,  2.07it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 5.5358\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:08<00:00,  1.91it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 5.6323\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:08<00:00,  1.99it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 5.5551\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.37it/s]\n",
      "> Eval on experience 5 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp005 = 5.6640\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp005 = 0.0000\n",
      "-- Starting eval on experience 6 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.38it/s]\n",
      "> Eval on experience 6 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp006 = 5.5493\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp006 = 0.0000\n",
      "-- Starting eval on experience 7 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.64it/s]\n",
      "> Eval on experience 7 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp007 = 5.5839\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp007 = 0.0000\n",
      "-- Starting eval on experience 8 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.43it/s]\n",
      "> Eval on experience 8 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp008 = 5.5847\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp008 = 0.0000\n",
      "-- Starting eval on experience 9 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.45it/s]\n",
      "> Eval on experience 9 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp009 = 5.5427\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp009 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 5.1370\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.0100\n",
      "-- >> Start of training phase << --\n",
      "100%|| 4/4 [00:05<00:00,  1.32s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 5.0595\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.6656\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0373\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0694\n",
      "100%|| 4/4 [00:00<00:00, 44.22it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2640\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.7941\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0285\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 49.68it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2490\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.8098\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0263\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 39.40it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2208\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.8078\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0285\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0972\n",
      "100%|| 4/4 [00:00<00:00, 56.91it/s]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.1935\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.7347\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0395\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0833\n",
      "100%|| 4/4 [00:00<00:00, 50.06it/s]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.1490\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.6908\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0285\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 55.59it/s]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.1004\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.6209\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0263\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 44.31it/s]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.0707\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.5588\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0329\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0694\n",
      "100%|| 4/4 [00:00<00:00, 39.59it/s]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.0088\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.4773\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0351\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 35.97it/s]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.9743\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.5190\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0417\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 44.31it/s]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.9060\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.4401\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0570\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0833\n",
      "100%|| 4/4 [00:00<00:00, 44.29it/s]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.8521\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.4123\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0461\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0694\n",
      "100%|| 4/4 [00:00<00:00, 36.00it/s]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.7857\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.3627\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0461\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0833\n",
      "100%|| 4/4 [00:00<00:00, 50.47it/s]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.7146\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.3609\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0307\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 48.08it/s]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.6247\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.4272\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0373\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0556\n",
      "100%|| 4/4 [00:00<00:00, 49.03it/s]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.5539\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.3576\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0241\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 49.23it/s]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.4850\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.3827\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0351\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 44.14it/s]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.4224\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.4716\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0482\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 49.70it/s]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.3953\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.5095\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0504\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 49.82it/s]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.3815\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.4364\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0461\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 55.60it/s]\n",
      "Epoch 20 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.3378\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.2812\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0636\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0694\n",
      "100%|| 4/4 [00:00<00:00, 49.82it/s]\n",
      "Epoch 21 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.3359\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.3629\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0548\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0694\n",
      "100%|| 4/4 [00:00<00:00, 49.98it/s]\n",
      "Epoch 22 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.3527\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.4029\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0570\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0556\n",
      "100%|| 4/4 [00:00<00:00, 57.13it/s]\n",
      "Epoch 23 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.3128\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.4186\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0548\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 58.80it/s]\n",
      "Epoch 24 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.2633\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.2539\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0855\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1111\n",
      "100%|| 4/4 [00:00<00:00, 49.66it/s]\n",
      "Epoch 25 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.2391\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.2258\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0921\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1111\n",
      "100%|| 4/4 [00:00<00:00, 57.09it/s]\n",
      "Epoch 26 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.2608\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.3047\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0636\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0833\n",
      "100%|| 4/4 [00:00<00:00, 49.88it/s]\n",
      "Epoch 27 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.2683\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.2603\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0680\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0694\n",
      "100%|| 4/4 [00:00<00:00, 56.67it/s]\n",
      "Epoch 28 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.2423\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.2286\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0680\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 57.12it/s]\n",
      "Epoch 29 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.2237\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.3432\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0614\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.33it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 3.5473\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0640\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.65it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 3.6826\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.51it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 3.5037\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.1330\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  2.68it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 5.7760\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.55it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 5.6855\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.55it/s]\n",
      "> Eval on experience 5 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp005 = 5.7895\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp005 = 0.0000\n",
      "-- Starting eval on experience 6 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:07<00:00,  2.24it/s]\n",
      "> Eval on experience 6 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp006 = 5.6980\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp006 = 0.0000\n",
      "-- Starting eval on experience 7 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.37it/s]\n",
      "> Eval on experience 7 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp007 = 5.7280\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp007 = 0.0000\n",
      "-- Starting eval on experience 8 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.63it/s]\n",
      "> Eval on experience 8 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp008 = 5.7025\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp008 = 0.0000\n",
      "-- Starting eval on experience 9 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  2.68it/s]\n",
      "> Eval on experience 9 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp009 = 5.7147\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp009 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 5.0828\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.0197\n",
      "-- >> Start of training phase << --\n",
      "100%|| 4/4 [00:06<00:00,  1.53s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 5.4296\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.7312\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0373\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0833\n",
      "100%|| 4/4 [00:00<00:00, 24.85it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4463\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.9116\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0241\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 29.95it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3878\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.9898\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0175\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 26.74it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3839\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.0409\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0241\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 31.15it/s]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3652\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.0327\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0197\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 34.54it/s]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3401\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.9964\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0197\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 29.30it/s]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3034\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.9594\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0351\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0972\n",
      "100%|| 4/4 [00:00<00:00, 28.98it/s]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2817\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.9467\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0132\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 30.54it/s]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2401\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.8573\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0263\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 32.91it/s]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2112\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.8194\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0263\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0556\n",
      "100%|| 4/4 [00:00<00:00, 28.35it/s]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.1727\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.8071\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0307\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0694\n",
      "100%|| 4/4 [00:00<00:00, 27.42it/s]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.1366\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.7531\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0219\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0556\n",
      "100%|| 4/4 [00:00<00:00, 32.31it/s]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.0838\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.8202\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0417\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0694\n",
      "100%|| 4/4 [00:00<00:00, 32.76it/s]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.0480\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.7994\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0395\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0556\n",
      "100%|| 4/4 [00:00<00:00, 28.23it/s]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.0379\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.7011\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0219\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0556\n",
      "100%|| 4/4 [00:00<00:00, 26.51it/s]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.9665\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.6113\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0285\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0694\n",
      "100%|| 4/4 [00:00<00:00, 26.47it/s]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.9081\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.6745\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0395\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0833\n",
      "100%|| 4/4 [00:00<00:00, 30.57it/s]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.8800\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.6387\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0395\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0972\n",
      "100%|| 4/4 [00:00<00:00, 33.02it/s]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.8105\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.6760\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0395\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 30.54it/s]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.7210\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.5229\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0548\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0833\n",
      "100%|| 4/4 [00:00<00:00, 27.20it/s]\n",
      "Epoch 20 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.7303\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.6458\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0395\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 30.69it/s]\n",
      "Epoch 21 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.6687\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.6824\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0592\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0556\n",
      "100%|| 4/4 [00:00<00:00, 31.74it/s]\n",
      "Epoch 22 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.6527\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.6644\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0417\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 35.30it/s]\n",
      "Epoch 23 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.6442\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.7251\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0482\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 30.56it/s]\n",
      "Epoch 24 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.6098\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.7056\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0526\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 27.87it/s]\n",
      "Epoch 25 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.5707\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.5998\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0592\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0694\n",
      "100%|| 4/4 [00:00<00:00, 33.60it/s]\n",
      "Epoch 26 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.5753\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.6487\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0614\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 28.76it/s]\n",
      "Epoch 27 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.5734\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.6835\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0702\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 26.58it/s]\n",
      "Epoch 28 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.5546\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.6526\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0482\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 28.68it/s]\n",
      "Epoch 29 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.5346\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.7278\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0724\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0694\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:09<00:00,  1.70it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 3.8361\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:09<00:00,  1.71it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 3.8618\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0180\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:09<00:00,  1.72it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 3.8481\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0700\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:09<00:00,  1.74it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 3.5925\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.1190\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:10<00:00,  1.53it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 5.9995\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:11<00:00,  1.43it/s]\n",
      "> Eval on experience 5 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp005 = 6.2428\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp005 = 0.0000\n",
      "-- Starting eval on experience 6 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:11<00:00,  1.42it/s]\n",
      "> Eval on experience 6 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp006 = 6.0449\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp006 = 0.0000\n",
      "-- Starting eval on experience 7 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:11<00:00,  1.43it/s]\n",
      "> Eval on experience 7 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp007 = 6.1104\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp007 = 0.0000\n",
      "-- Starting eval on experience 8 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:11<00:00,  1.43it/s]\n",
      "> Eval on experience 8 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp008 = 6.0831\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp008 = 0.0000\n",
      "-- Starting eval on experience 9 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:08<00:00,  1.85it/s]\n",
      "> Eval on experience 9 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp009 = 6.0933\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp009 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 5.1713\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.0207\n",
      "-- >> Start of training phase << --\n",
      "100%|| 4/4 [00:07<00:00,  1.78s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 5.6572\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.9215\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0307\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0833\n",
      "100%|| 4/4 [00:00<00:00, 37.16it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.8896\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.0445\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0175\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 42.78it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6645\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.0931\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0175\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 49.91it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5595\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.1061\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0241\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 41.38it/s]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5078\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.1789\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0132\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 44.07it/s]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4682\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.1716\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0132\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 45.98it/s]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4264\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.1443\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0219\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 52.52it/s]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4074\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.1804\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0132\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 51.66it/s]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3840\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.1442\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0110\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 51.61it/s]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3561\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.1312\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0154\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0556\n",
      "100%|| 4/4 [00:00<00:00, 56.77it/s]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3240\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.1348\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0175\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 53.09it/s]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3023\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.1151\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0197\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 53.64it/s]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2692\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.0619\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0175\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 53.69it/s]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2538\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.0841\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0197\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 51.74it/s]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2088\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.0563\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0241\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 47.57it/s]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.1805\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.0473\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0175\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 53.25it/s]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.1304\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.0003\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0329\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0556\n",
      "100%|| 4/4 [00:00<00:00, 52.17it/s]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.0683\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.9316\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0132\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 47.81it/s]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.0128\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.9334\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0197\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 55.14it/s]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.9808\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.9525\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0461\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 59.44it/s]\n",
      "Epoch 20 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.8963\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.0067\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0614\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 57.57it/s]\n",
      "Epoch 21 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.8589\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.9619\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0526\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 57.00it/s]\n",
      "Epoch 22 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.7529\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.9608\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0702\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 56.70it/s]\n",
      "Epoch 23 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.7109\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.0290\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0702\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 52.12it/s]\n",
      "Epoch 24 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.6776\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.9368\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0724\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 52.82it/s]\n",
      "Epoch 25 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.6298\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.9865\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0899\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 54.26it/s]\n",
      "Epoch 26 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.6407\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.9518\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0658\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 51.16it/s]\n",
      "Epoch 27 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.6486\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.9891\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0570\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 49.03it/s]\n",
      "Epoch 28 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.6497\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.8719\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0592\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 55.96it/s]\n",
      "Epoch 29 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.6185\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.9372\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0592\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:08<00:00,  1.99it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 4.0533\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0740\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:07<00:00,  2.24it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 4.0512\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.50it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 4.1044\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.48it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 4.1544\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.60it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 3.7976\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0570\n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.49it/s]\n",
      "> Eval on experience 5 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp005 = 5.9806\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp005 = 0.0000\n",
      "-- Starting eval on experience 6 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.49it/s]\n",
      "> Eval on experience 6 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp006 = 5.8271\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp006 = 0.0000\n",
      "-- Starting eval on experience 7 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.44it/s]\n",
      "> Eval on experience 7 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp007 = 5.8570\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp007 = 0.0000\n",
      "-- Starting eval on experience 8 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.42it/s]\n",
      "> Eval on experience 8 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp008 = 5.8559\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp008 = 0.0000\n",
      "-- Starting eval on experience 9 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.44it/s]\n",
      "> Eval on experience 9 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp009 = 5.7956\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp009 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 4.9477\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.0131\n",
      "-- >> Start of training phase << --\n",
      "100%|| 4/4 [00:04<00:00,  1.23s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 5.8743\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2719\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0241\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 49.61it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 5.0150\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2869\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0154\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 49.37it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.8371\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2598\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0066\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 52.16it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.7104\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2658\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0088\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 54.20it/s]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6637\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3080\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0088\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 45.48it/s]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5862\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2328\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0132\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 51.49it/s]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5561\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2809\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0154\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 49.39it/s]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5181\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2898\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0066\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 57.00it/s]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4954\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2834\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0088\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 50.63it/s]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4768\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2703\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0219\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0556\n",
      "100%|| 4/4 [00:00<00:00, 55.26it/s]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4566\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2882\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0154\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 60.33it/s]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4253\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2570\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0175\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 49.48it/s]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4006\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2651\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0197\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 43.97it/s]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3837\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2721\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0197\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 51.41it/s]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3512\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2451\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0241\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0556\n",
      "100%|| 4/4 [00:00<00:00, 50.02it/s]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3313\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2567\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0066\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 55.83it/s]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3110\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2321\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0088\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 48.99it/s]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2831\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2370\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0088\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 52.39it/s]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2550\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2194\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0154\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 55.67it/s]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2208\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.1979\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0285\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 55.04it/s]\n",
      "Epoch 20 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.1804\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2029\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0504\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 52.62it/s]\n",
      "Epoch 21 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.1324\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.1691\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0307\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 56.24it/s]\n",
      "Epoch 22 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.0942\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.1317\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0417\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0556\n",
      "100%|| 4/4 [00:00<00:00, 50.90it/s]\n",
      "Epoch 23 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.0349\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.1828\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0548\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 50.83it/s]\n",
      "Epoch 24 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.9873\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.1155\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0395\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 42.76it/s]\n",
      "Epoch 25 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.9114\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2058\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0526\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 40.77it/s]\n",
      "Epoch 26 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.8484\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.1815\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0592\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 36.38it/s]\n",
      "Epoch 27 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.8340\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.1468\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0702\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0556\n",
      "100%|| 4/4 [00:00<00:00, 46.04it/s]\n",
      "Epoch 28 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.8200\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3016\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0899\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 47.15it/s]\n",
      "Epoch 29 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.7773\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2241\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0921\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.31it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 4.3568\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.58it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 4.3742\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.57it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 4.2807\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.47it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 4.3244\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.55it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 4.2354\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:05<00:00,  2.69it/s]\n",
      "> Eval on experience 5 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp005 = 3.5520\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp005 = 0.1920\n",
      "-- Starting eval on experience 6 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.64it/s]\n",
      "> Eval on experience 6 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp006 = 5.9946\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp006 = 0.0000\n",
      "-- Starting eval on experience 7 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:08<00:00,  1.85it/s]\n",
      "> Eval on experience 7 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp007 = 5.9812\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp007 = 0.0000\n",
      "-- Starting eval on experience 8 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:09<00:00,  1.61it/s]\n",
      "> Eval on experience 8 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp008 = 5.9993\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp008 = 0.0000\n",
      "-- Starting eval on experience 9 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:10<00:00,  1.55it/s]\n",
      "> Eval on experience 9 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp009 = 5.9323\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp009 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 4.9031\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.0192\n",
      "-- >> Start of training phase << --\n",
      "100%|| 4/4 [00:08<00:00,  2.24s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 5.6133\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3779\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0241\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 30.81it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 5.1215\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3355\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0154\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 26.78it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.8849\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3664\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0175\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 27.90it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.7632\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3393\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0197\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 26.77it/s]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6884\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3476\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0154\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 25.31it/s]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6502\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4075\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0154\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 28.90it/s]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6168\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3621\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0110\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 27.07it/s]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5898\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4022\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0175\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 29.75it/s]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5572\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3715\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0241\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 34.13it/s]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5423\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3596\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0154\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 28.82it/s]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5127\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3628\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0110\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 28.90it/s]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4955\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3609\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0066\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 25.84it/s]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4533\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3287\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0197\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 27.19it/s]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4043\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3685\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0285\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 26.73it/s]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3472\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3119\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0175\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0556\n",
      "100%|| 4/4 [00:00<00:00, 24.61it/s]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3031\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3921\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0154\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 26.47it/s]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2327\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2618\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0329\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 25.30it/s]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.1572\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2325\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0329\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 27.25it/s]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.1121\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3358\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0461\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 26.12it/s]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.0796\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3618\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0482\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 25.60it/s]\n",
      "Epoch 20 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.0489\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2640\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0439\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 26.63it/s]\n",
      "Epoch 21 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.0143\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3277\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0548\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 32.06it/s]\n",
      "Epoch 22 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.0378\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4481\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0592\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 29.23it/s]\n",
      "Epoch 23 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.9898\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3243\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0592\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 30.13it/s]\n",
      "Epoch 24 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.9751\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2078\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0504\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 29.12it/s]\n",
      "Epoch 25 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.9588\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.2855\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0570\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 29.67it/s]\n",
      "Epoch 26 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.9518\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3837\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0789\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 29.72it/s]\n",
      "Epoch 27 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.8948\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.1719\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0768\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 27.64it/s]\n",
      "Epoch 28 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.9369\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3179\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0921\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 38.43it/s]\n",
      "Epoch 29 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.9391\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3186\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0702\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:11<00:00,  1.34it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 4.4175\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:11<00:00,  1.42it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 4.4040\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:11<00:00,  1.43it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 4.4252\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:12<00:00,  1.25it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 4.4755\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:12<00:00,  1.33it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 4.4688\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:08<00:00,  1.88it/s]\n",
      "> Eval on experience 5 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp005 = 4.3989\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp005 = 0.0000\n",
      "-- Starting eval on experience 6 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:07<00:00,  2.03it/s]\n",
      "> Eval on experience 6 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp006 = 3.8929\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp006 = 0.1200\n",
      "-- Starting eval on experience 7 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:07<00:00,  2.05it/s]\n",
      "> Eval on experience 7 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp007 = 5.5071\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp007 = 0.0000\n",
      "-- Starting eval on experience 8 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:07<00:00,  2.00it/s]\n",
      "> Eval on experience 8 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp008 = 5.4598\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp008 = 0.0000\n",
      "-- Starting eval on experience 9 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:08<00:00,  1.95it/s]\n",
      "> Eval on experience 9 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp009 = 5.4877\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp009 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 4.6937\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.0120\n",
      "-- >> Start of training phase << --\n",
      "100%|| 4/4 [00:07<00:00,  1.94s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 5.4003\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5098\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0088\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 46.01it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 5.1546\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5081\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0110\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 48.78it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 5.0334\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5096\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0066\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 56.77it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.8373\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5132\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0066\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 45.82it/s]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.7337\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4803\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0088\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 46.94it/s]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6727\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4465\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0154\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 55.95it/s]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6378\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4698\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0219\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0833\n",
      "100%|| 4/4 [00:00<00:00, 54.99it/s]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6177\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4657\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0197\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 46.00it/s]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6173\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5061\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0022\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 42.33it/s]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5935\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4737\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0154\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0556\n",
      "100%|| 4/4 [00:00<00:00, 53.17it/s]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5784\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4881\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0110\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 48.18it/s]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5603\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4980\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0329\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 49.59it/s]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5424\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4273\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0175\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 58.11it/s]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5289\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4703\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0088\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 53.43it/s]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5033\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4894\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0154\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 55.48it/s]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4644\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4397\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0175\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 49.31it/s]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4193\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4257\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0154\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 55.73it/s]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3609\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4190\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0263\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 56.60it/s]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2750\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4824\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0373\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 56.71it/s]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2898\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.8839\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0482\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 52.31it/s]\n",
      "Epoch 20 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.1837\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4843\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0461\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 57.31it/s]\n",
      "Epoch 21 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2138\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4442\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0526\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 56.06it/s]\n",
      "Epoch 22 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2254\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5183\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0636\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 49.12it/s]\n",
      "Epoch 23 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.1729\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5866\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0592\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 59.77it/s]\n",
      "Epoch 24 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.1688\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5103\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0439\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 48.76it/s]\n",
      "Epoch 25 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.1741\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4470\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0395\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 56.12it/s]\n",
      "Epoch 26 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.1595\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4970\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0526\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 50.02it/s]\n",
      "Epoch 27 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.1036\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4636\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0526\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 53.14it/s]\n",
      "Epoch 28 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.1297\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4802\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0636\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 52.76it/s]\n",
      "Epoch 29 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.1176\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4366\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0789\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:07<00:00,  2.18it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 4.5229\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.36it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 4.5551\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.55it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 4.5481\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.59it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 4.5256\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.38it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 4.5441\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.49it/s]\n",
      "> Eval on experience 5 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp005 = 4.5008\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp005 = 0.0000\n",
      "-- Starting eval on experience 6 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.54it/s]\n",
      "> Eval on experience 6 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp006 = 4.5219\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp006 = 0.0000\n",
      "-- Starting eval on experience 7 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.53it/s]\n",
      "> Eval on experience 7 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp007 = 4.1017\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp007 = 0.1160\n",
      "-- Starting eval on experience 8 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.47it/s]\n",
      "> Eval on experience 8 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp008 = 5.2163\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp008 = 0.0000\n",
      "-- Starting eval on experience 9 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.55it/s]\n",
      "> Eval on experience 9 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp009 = 5.2364\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp009 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 4.6273\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.0116\n",
      "-- >> Start of training phase << --\n",
      "100%|| 4/4 [00:05<00:00,  1.26s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 5.1627\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5723\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0088\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 46.11it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.9140\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5324\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0044\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 57.86it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.8979\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5733\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 54.43it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.8115\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5556\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0088\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 48.72it/s]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.7559\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5249\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0066\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 53.84it/s]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6922\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5353\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0154\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 52.26it/s]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6589\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5475\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0088\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 56.40it/s]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6458\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5478\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 49.95it/s]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6371\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5791\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0110\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 51.62it/s]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6325\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5738\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0110\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 54.70it/s]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6196\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5547\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0110\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 53.08it/s]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6048\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5327\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0175\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 56.65it/s]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5980\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5373\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0066\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 54.58it/s]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5949\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5416\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0154\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 55.02it/s]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5826\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5351\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0110\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 54.23it/s]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5778\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5498\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0154\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 56.37it/s]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5686\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5306\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0066\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 58.01it/s]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5502\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5213\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0066\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 52.07it/s]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5349\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4775\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0088\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 52.47it/s]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5367\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5448\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0197\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 54.96it/s]\n",
      "Epoch 20 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5070\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5079\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0110\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 55.90it/s]\n",
      "Epoch 21 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4977\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5442\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0110\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 62.80it/s]\n",
      "Epoch 22 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4661\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4548\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0241\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0556\n",
      "100%|| 4/4 [00:00<00:00, 51.14it/s]\n",
      "Epoch 23 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4364\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4734\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0197\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 53.13it/s]\n",
      "Epoch 24 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4049\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4756\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0373\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 61.11it/s]\n",
      "Epoch 25 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3789\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5006\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0439\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 56.56it/s]\n",
      "Epoch 26 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3162\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4745\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0548\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 60.38it/s]\n",
      "Epoch 27 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2962\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4465\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0636\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 59.40it/s]\n",
      "Epoch 28 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2213\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4777\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0680\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 57.00it/s]\n",
      "Epoch 29 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2103\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5746\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0702\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:07<00:00,  2.15it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 4.5633\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.46it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 4.6846\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.51it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 4.7094\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:10<00:00,  1.57it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 4.6304\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:09<00:00,  1.62it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 4.5997\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:10<00:00,  1.56it/s]\n",
      "> Eval on experience 5 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp005 = 4.6418\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp005 = 0.0000\n",
      "-- Starting eval on experience 6 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:10<00:00,  1.47it/s]\n",
      "> Eval on experience 6 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp006 = 4.6149\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp006 = 0.0000\n",
      "-- Starting eval on experience 7 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:12<00:00,  1.32it/s]\n",
      "> Eval on experience 7 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp007 = 4.5059\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp007 = 0.0000\n",
      "-- Starting eval on experience 8 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:12<00:00,  1.27it/s]\n",
      "> Eval on experience 8 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp008 = 4.0371\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp008 = 0.1230\n",
      "-- Starting eval on experience 9 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:12<00:00,  1.29it/s]\n",
      "> Eval on experience 9 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp009 = 5.2985\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp009 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 4.6286\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.0123\n",
      "-- >> Start of training phase << --\n",
      "100%|| 4/4 [00:10<00:00,  2.64s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 5.2302\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5539\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0154\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 44.95it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.9723\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6061\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0088\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 26.84it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.8749\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5855\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0132\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 24.87it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.8081\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5887\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0154\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 28.40it/s]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.7691\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6304\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0044\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 25.51it/s]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.7274\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5786\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0044\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 30.84it/s]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6985\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6201\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0088\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 33.75it/s]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6861\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5709\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0088\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 30.46it/s]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6640\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5952\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0132\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 30.48it/s]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6484\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6268\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0175\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 25.03it/s]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6359\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6436\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0241\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 31.08it/s]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.6131\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6702\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0307\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 28.99it/s]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5522\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6273\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0417\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 35.07it/s]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5482\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6657\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0329\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 29.44it/s]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4561\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.7661\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0395\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 30.64it/s]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4385\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6222\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0461\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 30.53it/s]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.4205\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6718\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0241\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 30.89it/s]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3367\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.7067\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0482\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 30.16it/s]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3423\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.9413\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0351\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 26.46it/s]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2617\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.7274\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0461\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 30.68it/s]\n",
      "Epoch 20 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.3306\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5977\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0592\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 30.56it/s]\n",
      "Epoch 21 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2834\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6426\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0570\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 26.44it/s]\n",
      "Epoch 22 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2479\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.7239\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0724\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 26.60it/s]\n",
      "Epoch 23 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2331\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6150\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0680\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0139\n",
      "100%|| 4/4 [00:00<00:00, 35.76it/s]\n",
      "Epoch 24 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2594\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6789\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0636\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 28.74it/s]\n",
      "Epoch 25 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2321\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.4646\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0636\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0556\n",
      "100%|| 4/4 [00:00<00:00, 26.08it/s]\n",
      "Epoch 26 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2602\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.6071\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0768\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 28.08it/s]\n",
      "Epoch 27 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2141\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5115\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0614\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0417\n",
      "100%|| 4/4 [00:00<00:00, 28.45it/s]\n",
      "Epoch 28 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2273\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.7646\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0746\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0278\n",
      "100%|| 4/4 [00:00<00:00, 30.95it/s]\n",
      "Epoch 29 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.2074\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.5567\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0592\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:11<00:00,  1.41it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 4.6199\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:08<00:00,  1.96it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 4.6876\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:08<00:00,  1.99it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 4.6500\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:07<00:00,  2.01it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 4.6661\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:07<00:00,  2.08it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 4.6979\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- Starting eval on experience 5 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:07<00:00,  2.00it/s]\n",
      "> Eval on experience 5 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp005 = 4.6555\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp005 = 0.0000\n",
      "-- Starting eval on experience 6 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:07<00:00,  2.18it/s]\n",
      "> Eval on experience 6 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp006 = 4.6493\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp006 = 0.0000\n",
      "-- Starting eval on experience 7 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:07<00:00,  2.17it/s]\n",
      "> Eval on experience 7 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp007 = 4.6013\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp007 = 0.0000\n",
      "-- Starting eval on experience 8 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:07<00:00,  2.08it/s]\n",
      "> Eval on experience 8 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp008 = 4.4650\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp008 = 0.0190\n",
      "-- Starting eval on experience 9 (Task 0) from test stream --\n",
      "100%|| 16/16 [00:06<00:00,  2.43it/s]\n",
      "> Eval on experience 9 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp009 = 4.2814\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp009 = 0.0990\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 4.5974\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.0118\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from avalanche.benchmarks.utils import classification_dataset\n",
    "\n",
    "\n",
    "import avalanche.benchmarks.scenarios.dataset_scenario\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.training.supervised import Cumulative\n",
    "from avalanche.evaluation.metrics import accuracy_metrics, loss_metrics\n",
    "from avalanche.logging import InteractiveLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.checkpointing import maybe_load_checkpoint, save_checkpoint\n",
    "from avalanche.training.determinism.rng_manager import RNGManager\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import TensorDataset\n",
    "from avalanche.benchmarks import nc_benchmark\n",
    "\n",
    "\n",
    "from avalanche.training.supervised.strategy_wrappers import Replay\n",
    "\n",
    "\n",
    "\n",
    "# Dfinition des arguments\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.method = 'DSA'\n",
    "        self.dataset = 'CIFAR100'\n",
    "        self.model = 'ConvNet'\n",
    "        self.ipc = 20\n",
    "        self.steps = 10  # Assurez-vous que 'steps' est dfini\n",
    "        self.epoch_eval_train = 300\n",
    "        self.lr_net = 0.01\n",
    "        self.batch_train = 256\n",
    "        self.data_path = r'C:\\Users\\ahmed\\OneDrive\\Documents\\GitHub\\DatasetCondensation\\cl_data'\n",
    "        self.save_path = './result'\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Initialisation de la variable device\n",
    "device = args.device\n",
    "device = 'cpu'\n",
    "# Charger les donnes\n",
    "def load_data():\n",
    "    if args.method == 'DSA':\n",
    "        fname = os.path.join(args.data_path, 'cl_res_DSA_CIFAR100_ConvNet_20ipc_%dsteps_seed%d.pt' % (args.steps, 0))\n",
    "    elif args.method == 'DM':\n",
    "        fname = os.path.join(args.data_path, 'cl_DM_CIFAR100_ConvNet_20ipc_%dsteps_seed%d.pt' % (args.steps, 0))\n",
    "    else:\n",
    "        raise ValueError(f'Unknown method: {args.method}')\n",
    "\n",
    "    # Vrifier l'existence du fichier\n",
    "    if not os.path.exists(fname):\n",
    "        raise FileNotFoundError(f\"File not found: {fname}\")\n",
    "    \n",
    "    # Charger le fichier\n",
    "    print(f\"Loading file from: {fname}\")\n",
    "    data = torch.load(fname, map_location='cpu')['data']\n",
    "    \n",
    "    images_train_all = [data[step][0] for step in range(args.steps)]\n",
    "    labels_train_all = [data[step][1] for step in range(args.steps)]\n",
    "    \n",
    "    return images_train_all, labels_train_all\n",
    "\n",
    "images_train_all, labels_train_all = load_data()\n",
    "\n",
    "# Concatner les donnes\n",
    "images_train = torch.cat(images_train_all, dim=0).to(device)\n",
    "labels_train = torch.cat(labels_train_all, dim=0).to(device)\n",
    "print('Train data size: ', images_train.shape)\n",
    "\n",
    "# Charger et prparer les donnes de test\n",
    "dst_test = CIFAR100(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "# Prparer les donnes de test\n",
    "images_test = []\n",
    "labels_test = []\n",
    "for i in range(len(dst_test)):\n",
    "    img, lab = dst_test[i]\n",
    "    if lab in set(labels_train.cpu().numpy()):  # Si l'tiquette de test fait partie des classes d'entranement\n",
    "        images_test.append(img)\n",
    "        labels_test.append(lab)\n",
    "\n",
    "images_test = torch.stack(images_test).to(device)\n",
    "labels_test = torch.tensor(labels_test, dtype=torch.long, device=device)\n",
    "print('Test data size: ', images_test.shape)\n",
    "\n",
    "# Crer les TensorDatasets pour l'entranement et les tests\n",
    "train_dataset = TensorDataset(images_train, labels_train)\n",
    "test_dataset = TensorDataset(images_test, labels_test)\n",
    "\n",
    "# Dfinir un modle simple de CNN\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class SimpleCNN1(nn.Module):\n",
    "    \n",
    "\n",
    "    def __init__(self, num_classes=100):\n",
    "        super(SimpleCNN1, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.Conv2d(64, 64, kernel_size=1, padding=0),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveMaxPool2d(1),\n",
    "            nn.Dropout(p=0.25),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(nn.Linear(64, num_classes))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Fonction pour crer le benchmark\n",
    "def cifar100_distilled_benchmark(steps):\n",
    "    return nc_benchmark(\n",
    "        train_dataset=train_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        n_experiences=steps,\n",
    "        task_labels=False\n",
    "    )\n",
    "\n",
    "def main_with_checkpointing(args):\n",
    "    # STEP 1: SET THE RANDOM SEEDS to guarantee reproducibility\n",
    "    RNGManager.set_random_seeds(1234)\n",
    "\n",
    "    # Nothing new here...\n",
    "    device = torch.device(\n",
    "        f\"cuda:{args.cuda}\" if torch.cuda.is_available() and args.cuda >= 0 else \"cpu\"\n",
    "    )\n",
    "    print(\"Using device\", device)\n",
    "\n",
    "    # CL Benchmark Creation (as usual)\n",
    "    benchmark = cifar100_distilled_benchmark(args.steps)  # Pass steps as an argument\n",
    "    model = SimpleCNN1(num_classes=100)\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    # Create the evaluation plugin (as usual)\n",
    "    evaluation_plugin = EvaluationPlugin(\n",
    "        accuracy_metrics(experience=True, stream=True), loggers=[InteractiveLogger()]\n",
    "    )\n",
    "\n",
    "    # choose some metrics and evaluation method\n",
    "    interactive_logger = InteractiveLogger()\n",
    "    eval_plugin = EvaluationPlugin(\n",
    "        accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "        loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "        loggers=[interactive_logger],\n",
    "    )\n",
    "\n",
    "    # Create the strategy using Replay\n",
    "    strategy = Replay(\n",
    "        model=model,                # Ensure your model is capable of handling the increased number of classes\n",
    "        optimizer=optimizer,        # Optimizer, e.g., Adam or SGD\n",
    "        criterion=criterion,        # Loss function, e.g., CrossEntropyLoss\n",
    "        mem_size=2000,               # Size of the replay buffer\n",
    "        train_mb_size=64,           # Batch size; adjust based on your hardware and model requirements\n",
    "        train_epochs=30,            # Increased number of epochs to ensure adequate training\n",
    "        eval_mb_size=64,            # Evaluation batch size\n",
    "        device=device,              # Device, e.g., 'cuda' or 'cpu'\n",
    "        evaluator=eval_plugin # Evaluation plugin or metric, e.g., accuracy\n",
    "    )\n",
    "\n",
    "\n",
    "    # STEP 2: TRY TO LOAD THE LAST CHECKPOINT\n",
    "    # if the checkpoint exists, load it into the newly created strategy\n",
    "    # the method also loads the experience counter, so we know where to\n",
    "    # resume training\n",
    "    fname = \"./checkpoint/Replay.pkl\"  # name of the checkpoint file\n",
    "    os.makedirs(os.path.dirname(fname), exist_ok=True)  # Ensure the checkpoint directory exists\n",
    "    #strategy, initial_exp = maybe_load_checkpoint(strategy, fname)\n",
    "\n",
    "\n",
    "    initial_exp=0\n",
    "    # STEP 3: USE THE \"initial_exp\" to resume training\n",
    "    for train_exp in benchmark.train_stream[initial_exp:]:\n",
    "        strategy.train(train_exp, num_workers=4, persistent_workers=True)\n",
    "        strategy.eval(benchmark.test_stream, num_workers=4)\n",
    "\n",
    "        # STEP 4: SAVE the checkpoint after training on each experience.\n",
    "        save_checkpoint(strategy, fname)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if 'ipykernel_launcher' in sys.argv[0]:\n",
    "        # Running in a Jupyter environment\n",
    "        class Args:\n",
    "            cuda = 0\n",
    "            steps = 10  # Dfinir une valeur par dfaut pour steps\n",
    "        args = Args()\n",
    "    else:\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument(\n",
    "            \"--cuda\",\n",
    "            type=int,\n",
    "            default=0,\n",
    "            help=\"Select zero-indexed cuda device. -1 to use CPU.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--steps\",\n",
    "            type=int,\n",
    "            default=10,\n",
    "            help=\"Number of steps for the benchmark.\",\n",
    "        )\n",
    "        # Parse known arguments and ignore the rest\n",
    "        args, _ = parser.parse_known_args(sys.argv)\n",
    "    main_with_checkpointing(args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8e2c42-dc40-459c-b7ba-d8ec378b9a05",
   "metadata": {},
   "source": [
    "##  GenerativeReplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d3b45686-3368-49e5-9b5a-ca01afb9e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple VAE generator model (example, adjust to your needs)\n",
    "class MlpVAE(nn.Module):\n",
    "    def __init__(self, input_shape, nhid, device):\n",
    "        super(MlpVAE, self).__init__()\n",
    "        self.device = device\n",
    "        self.fc1 = nn.Linear(np.prod(input_shape), nhid)\n",
    "        self.fc21 = nn.Linear(nhid, nhid)\n",
    "        self.fc22 = nn.Linear(nhid, nhid)\n",
    "        self.fc3 = nn.Linear(nhid, np.prod(input_shape))\n",
    "        self.fc4 = nn.Linear(nhid, np.prod(input_shape))\n",
    "    \n",
    "    def encode(self, x):\n",
    "        h1 = F.relu(self.fc1(x.view(-1, np.prod(x.size()[1:]))))\n",
    "        return self.fc21(h1), self.fc22(h1)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h3 = F.relu(self.fc3(z))\n",
    "        return torch.sigmoid(self.fc4(h3))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c25ac734-890a-43bb-b28c-9dadfd36e527",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file from: C:\\Users\\ahmed\\OneDrive\\Documents\\GitHub\\DatasetCondensation\\cl_data\\cl_res_DSA_CIFAR100_ConvNet_20ipc_10steps_seed0.pt\n",
      "Train data size:  torch.Size([2000, 3, 32, 32])\n",
      "Files already downloaded and verified\n",
      "Test data size:  torch.Size([10000, 3, 32, 32])\n",
      "Using device cuda:0\n",
      "-- >> Start of training phase << --\n",
      "100%|| 4/4 [00:05<00:00,  1.29s/it]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 4.5652\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 4.3164\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.0700\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 21.45it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.8783\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.0401\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 28.53it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.5670\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.3148\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 140.61it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.4102\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.4612\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1550\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 103.83it/s]\n",
      "Epoch 4 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.2773\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 3.1191\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 130.00it/s]\n",
      "Epoch 5 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 3.2460\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.7658\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2500\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 139.36it/s]\n",
      "Epoch 6 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.6062\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.5992\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.1400\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 74.46it/s]\n",
      "Epoch 7 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.3397\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.6830\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2550\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 113.08it/s]\n",
      "Epoch 8 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.1915\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.1043\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2450\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 64.65it/s]\n",
      "Epoch 9 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.9794\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.8125\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2850\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 70.40it/s]\n",
      "Epoch 10 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.9789\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.6934\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2700\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.5000\n",
      "100%|| 4/4 [00:00<00:00, 87.96it/s]\n",
      "Epoch 11 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.9600\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.8812\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3400\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.2500\n",
      "100%|| 4/4 [00:00<00:00, 96.31it/s]\n",
      "Epoch 12 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.8685\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.6977\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3200\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.0000\n",
      "100%|| 4/4 [00:00<00:00, 80.22it/s]\n",
      "Epoch 13 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.0510\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.8885\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.2900\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 93.83it/s]\n",
      "Epoch 14 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 2.0283\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9265\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3100\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.5000\n",
      "100%|| 4/4 [00:00<00:00, 89.75it/s]\n",
      "Epoch 15 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.8371\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.7416\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.3000\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 175.04it/s]\n",
      "Epoch 16 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.5750\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 2.0104\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.4350\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.1250\n",
      "100%|| 4/4 [00:00<00:00, 110.12it/s]\n",
      "Epoch 17 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.4669\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.5338\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.4600\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.5000\n",
      "100%|| 4/4 [00:00<00:00, 170.29it/s]\n",
      "Epoch 18 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.4039\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.9641\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.4800\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 104.97it/s]\n",
      "Epoch 19 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.2689\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.8918\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.5250\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.7500\n",
      "100%|| 4/4 [00:00<00:00, 88.36it/s]\n",
      "Epoch 20 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.1935\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.0718\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.5750\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.7500\n",
      "100%|| 4/4 [00:00<00:00, 111.95it/s]\n",
      "Epoch 21 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.2363\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.0145\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.5350\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.6250\n",
      "100%|| 4/4 [00:00<00:00, 116.35it/s]\n",
      "Epoch 22 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.0867\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.4635\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.6200\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.3750\n",
      "100%|| 4/4 [00:00<00:00, 142.63it/s]\n",
      "Epoch 23 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.1667\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.0872\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.5850\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.7500\n",
      "100%|| 4/4 [00:00<00:00, 175.67it/s]\n",
      "Epoch 24 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.0256\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.0368\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.6450\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.6250\n",
      "100%|| 4/4 [00:00<00:00, 175.38it/s]\n",
      "Epoch 25 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9907\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.9814\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.6800\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.6250\n",
      "100%|| 4/4 [00:00<00:00, 79.74it/s]\n",
      "Epoch 26 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9838\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.7781\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.6350\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.7500\n",
      "100%|| 4/4 [00:00<00:00, 139.22it/s]\n",
      "Epoch 27 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 1.0171\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.7813\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.6700\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.7500\n",
      "100%|| 4/4 [00:00<00:00, 161.12it/s]\n",
      "Epoch 28 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9398\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 1.1623\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.6450\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.5000\n",
      "100%|| 4/4 [00:00<00:00, 167.60it/s]\n",
      "Epoch 29 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.8026\n",
      "\tLoss_MB/train_phase/train_stream/Task000 = 0.6429\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.6850\n",
      "\tTop1_Acc_MB/train_phase/train_stream/Task000 = 0.8750\n",
      "-- >> Start of training phase << --\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x3072 and 256x3072)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[45], line 244\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;66;03m# Parse known arguments and ignore the rest\u001b[39;00m\n\u001b[0;32m    243\u001b[0m     args, _ \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_known_args(sys\u001b[38;5;241m.\u001b[39margv)\n\u001b[1;32m--> 244\u001b[0m \u001b[43mmain_with_checkpointing\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[45], line 214\u001b[0m, in \u001b[0;36mmain_with_checkpointing\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# STEP 3: USE THE \"initial_exp\" to resume training\u001b[39;00m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_exp \u001b[38;5;129;01min\u001b[39;00m benchmark\u001b[38;5;241m.\u001b[39mtrain_stream[initial_exp:]:\n\u001b[1;32m--> 214\u001b[0m     \u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_exp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersistent_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m     strategy\u001b[38;5;241m.\u001b[39meval(benchmark\u001b[38;5;241m.\u001b[39mtest_stream, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# STEP 4: SAVE the checkpoint after training on each experience.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DDP\\lib\\site-packages\\avalanche\\training\\templates\\base_sgd.py:211\u001b[0m, in \u001b[0;36mBaseSGDTemplate.train\u001b[1;34m(self, experiences, eval_streams, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    205\u001b[0m     experiences: Union[TDatasetExperience, Iterable[TDatasetExperience]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    210\u001b[0m ):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_streams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluator\u001b[38;5;241m.\u001b[39mget_last_metrics()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DDP\\lib\\site-packages\\avalanche\\training\\templates\\base.py:164\u001b[0m, in \u001b[0;36mBaseTemplate.train\u001b[1;34m(self, experiences, eval_streams, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_before_training_exp(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_exp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperience, eval_streams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_after_training_exp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_training(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_cleanup()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DDP\\lib\\site-packages\\avalanche\\training\\templates\\base.py:323\u001b[0m, in \u001b[0;36mBaseTemplate._after_training_exp\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_after_training_exp\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 323\u001b[0m     \u001b[43mtrigger_plugins\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mafter_training_exp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DDP\\lib\\site-packages\\avalanche\\training\\utils.py:75\u001b[0m, in \u001b[0;36mtrigger_plugins\u001b[1;34m(strategy, event, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m strategy\u001b[38;5;241m.\u001b[39mplugins:\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(p, event):\n\u001b[1;32m---> 75\u001b[0m         \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DDP\\lib\\site-packages\\avalanche\\training\\plugins\\generative_replay.py:171\u001b[0m, in \u001b[0;36mTrainGeneratorAfterExpPlugin.after_training_exp\u001b[1;34m(self, strategy, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m plugin \u001b[38;5;129;01min\u001b[39;00m strategy\u001b[38;5;241m.\u001b[39mplugins:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(plugin) \u001b[38;5;129;01mis\u001b[39;00m GenerativeReplayPlugin:\n\u001b[1;32m--> 171\u001b[0m         \u001b[43mplugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperience\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DDP\\lib\\site-packages\\avalanche\\training\\templates\\base_sgd.py:211\u001b[0m, in \u001b[0;36mBaseSGDTemplate.train\u001b[1;34m(self, experiences, eval_streams, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    205\u001b[0m     experiences: Union[TDatasetExperience, Iterable[TDatasetExperience]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    210\u001b[0m ):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_streams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluator\u001b[38;5;241m.\u001b[39mget_last_metrics()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DDP\\lib\\site-packages\\avalanche\\training\\templates\\base.py:163\u001b[0m, in \u001b[0;36mBaseTemplate.train\u001b[1;34m(self, experiences, eval_streams, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperience \u001b[38;5;129;01min\u001b[39;00m experiences_list:\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_before_training_exp(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_exp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_streams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_training_exp(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_training(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DDP\\lib\\site-packages\\avalanche\\training\\templates\\base_sgd.py:337\u001b[0m, in \u001b[0;36mBaseSGDTemplate._train_exp\u001b[1;34m(self, experience, eval_streams, **kwargs)\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_training_epoch(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DDP\\lib\\site-packages\\avalanche\\training\\templates\\update_type\\sgd_update.py:31\u001b[0m, in \u001b[0;36mSGDUpdate.training_epoch\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_before_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmb_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Loss & Backward\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DDP\\lib\\site-packages\\avalanche\\training\\templates\\problem_type\\supervised_problem.py:47\u001b[0m, in \u001b[0;36mSupervisedProblem.forward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     46\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the model's output given the current mini-batch.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mavalanche_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmb_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmb_task_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DDP\\lib\\site-packages\\avalanche\\models\\utils.py:22\u001b[0m, in \u001b[0;36mavalanche_forward\u001b[1;34m(model, x, task_labels)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model(x, task_labels)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# no task labels\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DDP\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[44], line 28\u001b[0m, in \u001b[0;36mMlpVAE.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     26\u001b[0m mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(x)\n\u001b[0;32m     27\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparameterize(mu, logvar)\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m, mu, logvar\n",
      "Cell \u001b[1;32mIn[44], line 23\u001b[0m, in \u001b[0;36mMlpVAE.decode\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, z):\n\u001b[0;32m     22\u001b[0m     h3 \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(z))\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh3\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DDP\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\DDP\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x3072 and 256x3072)"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from avalanche.benchmarks.utils import classification_dataset\n",
    "\n",
    "\n",
    "import avalanche.benchmarks.scenarios.dataset_scenario\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.training.supervised import Cumulative\n",
    "from avalanche.evaluation.metrics import accuracy_metrics, loss_metrics\n",
    "from avalanche.logging import InteractiveLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.checkpointing import maybe_load_checkpoint, save_checkpoint\n",
    "from avalanche.training.determinism.rng_manager import RNGManager\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import TensorDataset\n",
    "from avalanche.benchmarks import nc_benchmark\n",
    "\n",
    "\n",
    "from avalanche.training.supervised.strategy_wrappers import Replay\n",
    "\n",
    "from torch.optim import Adam, SGD\n",
    "from avalanche.checkpointing import maybe_load_checkpoint, save_checkpoint\n",
    "import sys\n",
    "from avalanche.training.determinism.rng_manager import RNGManager\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "\n",
    "\n",
    "# Dfinition des arguments\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.method = 'DSA'\n",
    "        self.dataset = 'CIFAR100'\n",
    "        self.model = 'ConvNet'\n",
    "        self.ipc = 20\n",
    "        self.steps = 10  # Assurez-vous que 'steps' est dfini\n",
    "        self.epoch_eval_train = 300\n",
    "        self.lr_net = 0.01\n",
    "        self.batch_train = 256\n",
    "        self.data_path = r'C:\\Users\\ahmed\\OneDrive\\Documents\\GitHub\\DatasetCondensation\\cl_data'\n",
    "        self.save_path = './result'\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Initialisation de la variable device\n",
    "device = args.device\n",
    "device = 'cpu'\n",
    "# Charger les donnes\n",
    "def load_data():\n",
    "    if args.method == 'DSA':\n",
    "        fname = os.path.join(args.data_path, 'cl_res_DSA_CIFAR100_ConvNet_20ipc_%dsteps_seed%d.pt' % (args.steps, 0))\n",
    "    elif args.method == 'DM':\n",
    "        fname = os.path.join(args.data_path, 'cl_DM_CIFAR100_ConvNet_20ipc_%dsteps_seed%d.pt' % (args.steps, 0))\n",
    "    else:\n",
    "        raise ValueError(f'Unknown method: {args.method}')\n",
    "\n",
    "    # Vrifier l'existence du fichier\n",
    "    if not os.path.exists(fname):\n",
    "        raise FileNotFoundError(f\"File not found: {fname}\")\n",
    "    \n",
    "    # Charger le fichier\n",
    "    print(f\"Loading file from: {fname}\")\n",
    "    data = torch.load(fname, map_location='cpu')['data']\n",
    "    \n",
    "    images_train_all = [data[step][0] for step in range(args.steps)]\n",
    "    labels_train_all = [data[step][1] for step in range(args.steps)]\n",
    "    \n",
    "    return images_train_all, labels_train_all\n",
    "\n",
    "images_train_all, labels_train_all = load_data()\n",
    "\n",
    "# Concatner les donnes\n",
    "images_train = torch.cat(images_train_all, dim=0).to(device)\n",
    "labels_train = torch.cat(labels_train_all, dim=0).to(device)\n",
    "print('Train data size: ', images_train.shape)\n",
    "\n",
    "# Charger et prparer les donnes de test\n",
    "dst_test = CIFAR100(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "# Prparer les donnes de test\n",
    "images_test = []\n",
    "labels_test = []\n",
    "for i in range(len(dst_test)):\n",
    "    img, lab = dst_test[i]\n",
    "    if lab in set(labels_train.cpu().numpy()):  # Si l'tiquette de test fait partie des classes d'entranement\n",
    "        images_test.append(img)\n",
    "        labels_test.append(lab)\n",
    "\n",
    "images_test = torch.stack(images_test).to(device)\n",
    "labels_test = torch.tensor(labels_test, dtype=torch.long, device=device)\n",
    "print('Test data size: ', images_test.shape)\n",
    "\n",
    "# Crer les TensorDatasets pour l'entranement et les tests\n",
    "train_dataset = TensorDataset(images_train, labels_train)\n",
    "test_dataset = TensorDataset(images_test, labels_test)\n",
    "\n",
    "# Dfinir un modle simple de CNN\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Fonction pour crer le benchmark\n",
    "def cifar100_distilled_benchmark(steps):\n",
    "    return nc_benchmark(\n",
    "        train_dataset=train_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        n_experiences=steps,\n",
    "        task_labels=False\n",
    "    )\n",
    "\n",
    "def main_with_checkpointing(args):\n",
    "    # STEP 1: SET THE RANDOM SEEDS to guarantee reproducibility\n",
    "    RNGManager.set_random_seeds(1234)\n",
    "\n",
    "    # Nothing new here...\n",
    "    device = torch.device(\n",
    "        f\"cuda:{args.cuda}\" if torch.cuda.is_available() and args.cuda >= 0 else \"cpu\"\n",
    "    )\n",
    "    print(\"Using device\", device)\n",
    "\n",
    "    # CL Benchmark Creation (as usual)\n",
    "    benchmark = cifar100_distilled_benchmark(args.steps)  # Pass steps as an argument\n",
    "    model = SimpleCNN(num_classes=100)\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    # Define the VAE generator model and its training strategy\n",
    "    generator = MlpVAE((3, 32, 32), nhid=256, device=device)\n",
    "    optimizer_generator = Adam(generator.parameters(), lr=0.01, weight_decay=0.0001)\n",
    "    \n",
    "    # Define the VAE training strategy\n",
    "    from avalanche.training.templates import SupervisedTemplate\n",
    "    from avalanche.training.plugins import GenerativeReplayPlugin, TrainGeneratorAfterExpPlugin\n",
    "    \n",
    "    class VAETraining(SupervisedTemplate):\n",
    "        # Implementation of VAE training strategy\n",
    "        pass\n",
    "  \n",
    "\n",
    "    vae_training_strategy = VAETraining(\n",
    "        model=generator,\n",
    "        optimizer=optimizer_generator,\n",
    "        criterion=CrossEntropyLoss(),\n",
    "        train_mb_size=64,\n",
    "        train_epochs=30,\n",
    "        eval_mb_size=64,\n",
    "        device=device,\n",
    "        plugins=[GenerativeReplayPlugin(replay_size=200)],\n",
    "    )\n",
    "    \n",
    "\n",
    "    \n",
    "    # Create the Generative Replay strategy\n",
    "    strategy = GenerativeReplay(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        train_mb_size=64,\n",
    "        train_epochs=30,\n",
    "        eval_mb_size=64,\n",
    "        device=device,\n",
    "        generator_strategy=vae_training_strategy,  # The generator strategy\n",
    "        replay_size=200,  # Size of the replay buffer\n",
    "        increasing_replay_size=False,  # Whether to increase the replay buffer size over time\n",
    "        evaluator=EvaluationPlugin(\n",
    "            accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "            loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "            loggers=[InteractiveLogger()],\n",
    "        ),\n",
    "    )\n",
    "\n",
    "\n",
    "    # STEP 2: TRY TO LOAD THE LAST CHECKPOINT\n",
    "    # if the checkpoint exists, load it into the newly created strategy\n",
    "    # the method also loads the experience counter, so we know where to\n",
    "    # resume training\n",
    "    fname = \"./checkpoint/GenerativeReplay.pkl\"  # name of the checkpoint file\n",
    "    os.makedirs(os.path.dirname(fname), exist_ok=True)  # Ensure the checkpoint directory exists\n",
    "    strategy, initial_exp = maybe_load_checkpoint(strategy, fname)\n",
    "\n",
    "    # STEP 3: USE THE \"initial_exp\" to resume training\n",
    "    for train_exp in benchmark.train_stream[initial_exp:]:\n",
    "        strategy.train(train_exp, num_workers=4, persistent_workers=True)\n",
    "        strategy.eval(benchmark.test_stream, num_workers=4)\n",
    "\n",
    "        # STEP 4: SAVE the checkpoint after training on each experience.\n",
    "        save_checkpoint(strategy, fname)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if 'ipykernel_launcher' in sys.argv[0]:\n",
    "        # Running in a Jupyter environment\n",
    "        class Args:\n",
    "            cuda = 0\n",
    "            steps = 10  # Dfinir une valeur par dfaut pour steps\n",
    "        args = Args()\n",
    "    else:\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument(\n",
    "            \"--cuda\",\n",
    "            type=int,\n",
    "            default=0,\n",
    "            help=\"Select zero-indexed cuda device. -1 to use CPU.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--steps\",\n",
    "            type=int,\n",
    "            default=10,\n",
    "            help=\"Number of steps for the benchmark.\",\n",
    "        )\n",
    "        # Parse known arguments and ignore the rest\n",
    "        args, _ = parser.parse_known_args(sys.argv)\n",
    "    main_with_checkpointing(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2820d163-2d9a-424f-82a7-6173288bfa41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Elastic Weight Consolidation (EWC) strategy.\\n\\n    See EWC plugin for details.\\n    This strategy does not use task identities.\\n    '"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EWC :\n",
    "\"\"\"Elastic Weight Consolidation (EWC) strategy.\n",
    "\n",
    "    See EWC plugin for details.\n",
    "    This strategy does not use task identities.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b953c41-e7d0-4a3c-9462-ff325c2b49b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from avalanche.benchmarks.utils import classification_dataset\n",
    "\n",
    "\n",
    "import avalanche.benchmarks.scenarios.dataset_scenario\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.training.supervised import Cumulative\n",
    "from avalanche.evaluation.metrics import accuracy_metrics, loss_metrics\n",
    "from avalanche.logging import InteractiveLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.checkpointing import maybe_load_checkpoint, save_checkpoint\n",
    "from avalanche.training.determinism.rng_manager import RNGManager\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import TensorDataset\n",
    "from avalanche.benchmarks import nc_benchmark\n",
    "\n",
    "\n",
    "from avalanche.training.supervised.strategy_wrappers import Replay\n",
    "\n",
    "\n",
    "\n",
    "# Dfinition des arguments\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.method = 'DSA'\n",
    "        self.dataset = 'CIFAR100'\n",
    "        self.model = 'ConvNet'\n",
    "        self.ipc = 20\n",
    "        self.steps = 10  # Assurez-vous que 'steps' est dfini\n",
    "        self.epoch_eval_train = 300\n",
    "        self.lr_net = 0.01\n",
    "        self.batch_train = 256\n",
    "        self.data_path = r'C:\\Users\\ahmed\\OneDrive\\Documents\\GitHub\\DatasetCondensation\\cl_data'\n",
    "        self.save_path = './result'\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "args = Args()\n",
    "\n",
    "# Initialisation de la variable device\n",
    "device = args.device\n",
    "device = 'cpu'\n",
    "# Charger les donnes\n",
    "def load_data():\n",
    "    if args.method == 'DSA':\n",
    "        fname = os.path.join(args.data_path, 'cl_res_DSA_CIFAR100_ConvNet_20ipc_%dsteps_seed%d.pt' % (args.steps, 0))\n",
    "    elif args.method == 'DM':\n",
    "        fname = os.path.join(args.data_path, 'cl_DM_CIFAR100_ConvNet_20ipc_%dsteps_seed%d.pt' % (args.steps, 0))\n",
    "    else:\n",
    "        raise ValueError(f'Unknown method: {args.method}')\n",
    "\n",
    "    # Vrifier l'existence du fichier\n",
    "    if not os.path.exists(fname):\n",
    "        raise FileNotFoundError(f\"File not found: {fname}\")\n",
    "    \n",
    "    # Charger le fichier\n",
    "    print(f\"Loading file from: {fname}\")\n",
    "    data = torch.load(fname, map_location='cpu')['data']\n",
    "    \n",
    "    images_train_all = [data[step][0] for step in range(args.steps)]\n",
    "    labels_train_all = [data[step][1] for step in range(args.steps)]\n",
    "    \n",
    "    return images_train_all, labels_train_all\n",
    "\n",
    "images_train_all, labels_train_all = load_data()\n",
    "\n",
    "# Concatner les donnes\n",
    "images_train = torch.cat(images_train_all, dim=0).to(device)\n",
    "labels_train = torch.cat(labels_train_all, dim=0).to(device)\n",
    "print('Train data size: ', images_train.shape)\n",
    "\n",
    "# Charger et prparer les donnes de test\n",
    "dst_test = CIFAR100(root='./data', train=False, download=True, transform=ToTensor())\n",
    "\n",
    "# Prparer les donnes de test\n",
    "images_test = []\n",
    "labels_test = []\n",
    "for i in range(len(dst_test)):\n",
    "    img, lab = dst_test[i]\n",
    "    if lab in set(labels_train.cpu().numpy()):  # Si l'tiquette de test fait partie des classes d'entranement\n",
    "        images_test.append(img)\n",
    "        labels_test.append(lab)\n",
    "\n",
    "images_test = torch.stack(images_test).to(device)\n",
    "labels_test = torch.tensor(labels_test, dtype=torch.long, device=device)\n",
    "print('Test data size: ', images_test.shape)\n",
    "\n",
    "# Crer les TensorDatasets pour l'entranement et les tests\n",
    "train_dataset = TensorDataset(images_train, labels_train)\n",
    "test_dataset = TensorDataset(images_test, labels_test)\n",
    "\n",
    "# Dfinir un modle simple de CNN\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Fonction pour crer le benchmark\n",
    "def cifar100_distilled_benchmark(steps):\n",
    "    return nc_benchmark(\n",
    "        train_dataset=train_dataset,\n",
    "        test_dataset=test_dataset,\n",
    "        n_experiences=steps,\n",
    "        task_labels=False\n",
    "    )\n",
    "\n",
    "def main_with_checkpointing(args):\n",
    "    # STEP 1: SET THE RANDOM SEEDS to guarantee reproducibility\n",
    "    RNGManager.set_random_seeds(1234)\n",
    "\n",
    "    # Nothing new here...\n",
    "    device = torch.device(\n",
    "        f\"cuda:{args.cuda}\" if torch.cuda.is_available() and args.cuda >= 0 else \"cpu\"\n",
    "    )\n",
    "    print(\"Using device\", device)\n",
    "\n",
    "    # CL Benchmark Creation (as usual)\n",
    "    benchmark = cifar100_distilled_benchmark(args.steps)  # Pass steps as an argument\n",
    "    model = SimpleCNN(num_classes=100)\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    # Create the evaluation plugin (as usual)\n",
    "    evaluation_plugin = EvaluationPlugin(\n",
    "        accuracy_metrics(experience=True, stream=True), loggers=[InteractiveLogger()]\n",
    "    )\n",
    "\n",
    "    # choose some metrics and evaluation method\n",
    "    interactive_logger = InteractiveLogger()\n",
    "    eval_plugin = EvaluationPlugin(\n",
    "        accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "        loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "        loggers=[interactive_logger],\n",
    "    )\n",
    "\n",
    "    # Create the strategy using Replay\n",
    "    strategy = Replay(\n",
    "        model=model,                # Ensure your model is capable of handling the increased number of classes\n",
    "        optimizer=optimizer,        # Optimizer, e.g., Adam or SGD\n",
    "        criterion=criterion,        # Loss function, e.g., CrossEntropyLoss\n",
    "        mem_size=200,               # Size of the replay buffer\n",
    "        train_mb_size=64,           # Batch size; adjust based on your hardware and model requirements\n",
    "        train_epochs=30,            # Increased number of epochs to ensure adequate training\n",
    "        eval_mb_size=64,            # Evaluation batch size\n",
    "        device=device,              # Device, e.g., 'cuda' or 'cpu'\n",
    "        evaluator=eval_plugin # Evaluation plugin or metric, e.g., accuracy\n",
    "    )\n",
    "\n",
    "\n",
    "    # STEP 2: TRY TO LOAD THE LAST CHECKPOINT\n",
    "    # if the checkpoint exists, load it into the newly created strategy\n",
    "    # the method also loads the experience counter, so we know where to\n",
    "    # resume training\n",
    "    fname = \"./checkpoint/Replay.pkl\"  # name of the checkpoint file\n",
    "    os.makedirs(os.path.dirname(fname), exist_ok=True)  # Ensure the checkpoint directory exists\n",
    "    strategy, initial_exp = maybe_load_checkpoint(strategy, fname)\n",
    "\n",
    "    initial_exp=0\n",
    "    # STEP 3: USE THE \"initial_exp\" to resume training\n",
    "    for train_exp in benchmark.train_stream[initial_exp:]:\n",
    "        strategy.train(train_exp, num_workers=4, persistent_workers=True)\n",
    "        strategy.eval(benchmark.test_stream, num_workers=4)\n",
    "\n",
    "        # STEP 4: SAVE the checkpoint after training on each experience.\n",
    "        save_checkpoint(strategy, fname)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if 'ipykernel_launcher' in sys.argv[0]:\n",
    "        # Running in a Jupyter environment\n",
    "        class Args:\n",
    "            cuda = 0\n",
    "            steps = 10  # Dfinir une valeur par dfaut pour steps\n",
    "        args = Args()\n",
    "    else:\n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument(\n",
    "            \"--cuda\",\n",
    "            type=int,\n",
    "            default=0,\n",
    "            help=\"Select zero-indexed cuda device. -1 to use CPU.\",\n",
    "        )\n",
    "        parser.add_argument(\n",
    "            \"--steps\",\n",
    "            type=int,\n",
    "            default=10,\n",
    "            help=\"Number of steps for the benchmark.\",\n",
    "        )\n",
    "        # Parse known arguments and ignore the rest\n",
    "        args, _ = parser.parse_known_args(sys.argv)\n",
    "    main_with_checkpointing(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
